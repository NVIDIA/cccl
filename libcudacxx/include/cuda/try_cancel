// SPDX-FileCopyrightText: Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

#ifndef _CUDA_TRY_CANCEL
#define _CUDA_TRY_CANCEL

#include <cuda/std/detail/__config>

#if defined(_CCCL_IMPLICIT_SYSTEM_HEADER_GCC)
#  pragma GCC system_header
#elif defined(_CCCL_IMPLICIT_SYSTEM_HEADER_CLANG)
#  pragma clang system_header
#elif defined(_CCCL_IMPLICIT_SYSTEM_HEADER_MSVC)
#  pragma system_header
#endif // no system header

_LIBCUDACXX_BEGIN_NAMESPACE_CUDA

namespace experimental {

namespace __detail {

struct __empty_t {
  __device__ void operator()(dim3);
};

template <int __I>
_CCCL_NODISCARD _CCCL_DEVICE _CCCL_HIDE_FROM_ABI int __cluster_get_dim(__int128 __result) noexcept {
    int __r;
    if constexpr(__I == 0) {
        asm volatile("clusterlaunchcontrol.query_cancel.get_first_ctaid::x.b32.b128 %0, %1;" : "=r"(__r) : "q"(__result) : "memory");
    } else if constexpr(__I == 1) {
        asm volatile("clusterlaunchcontrol.query_cancel.get_first_ctaid::y.b32.b128 %0, %1;" : "=r"(__r) : "q"(__result) : "memory");
    } else if constexpr(__I == 2) {
        asm volatile("clusterlaunchcontrol.query_cancel.get_first_ctaid::z.b32.b128 %0, %1;" : "=r"(__r) : "q"(__result) : "memory");
    } else {
      __r = -1;
      #ifndef NDEBUG
      __trap();
      #endif
    }
    return __r;
}

} // namespace __detail

/// This API for implementing work-stealing, repeatedly attempts to cancel the launch of a thread block
/// from the current grid. On success, it invokes the unary function `__uf` before trying again.
/// On failure, it returns.
///
/// This API does not provide any memory synchronization.
/// This API does not guarantee that any thread will invoke `__uf` with the next block index until all
/// invocatons of `__uf` for the prior block index have returned.
///
/// Preconditions:
/// - All thread block threads shall call this API exactly once.
/// - Exactly one thread block thread shall call this API with `__is_leader` equals `true`.  
template <int __ThreadBlockDim = 3, typename __UnaryFunction = __detail::__empty_t>
    requires std::is_invocable_r_v<void, __UnaryFunction, dim3>
_CCCL_DEVICE _CCCL_HIDE_FROM_ABI void try_cancel_blocks(bool __is_leader, __UnaryFunction __uf) {
  NV_IF_ELSE_TARGET(NV_PROVIDES_SM_100,
    (
    __shared__ uint64_t __barrier; // TODO: use 2 barriers and 2 results avoid last sync threads
    __shared__ __int128 __result;
    bool __phase = false;
    dim3 __block_idx;

    if constexpr (__ThreadBlockDim == 3) {
      __block_idx = dim3(blockIdx.x, blockIdx.y, blockIdx.z);
    } else if constexpr (__ThreadBlockDim == 2) {
      __block_idx = dim3(blockIdx.x, blockIdx.y, 1);
    } else if constexpr (__ThreadBlockDim == 1) {
      __block_idx = dim3(blockIdx.x, 1, 1);      
    } else {
      #ifndef NDEBUG
        __trap();
      #endif
    }

    // Initialize barrier and kick-start try_cancel pipeline:
    if (__is_leader) {
        auto __leader_mask = __activemask();
        asm volatile(
            "{\n\t"
                ".reg .pred p;\n\t"	       
                // elect.sync is a workaround for peeling loop (#nvbug-id)
                "elect.sync _|p, %2;\n\t"
                "@p mbarrier.init.shared::cta.b64 [%1], 1;\n\t"
                "@p clusterlaunchcontrol.try_cancel.async.shared::cta.mbarrier::complete_tx::bytes.b128 [%0], [%1];\n\t"
                "@p mbarrier.arrive.expect_tx.release.cta.shared::cta.b64 _, [%1], 16;\n\t"
            "}"
            :
            : "r"((int)__cvta_generic_to_shared(&__result)), "r"((int)__cvta_generic_to_shared(&__barrier)), "r"(__leader_mask)
            : "memory"
        );
    }
    // Note: mbarrier only accessed by control thread; no inter-thread synchronization required.

    do {
        __uf(__block_idx); // Invoke unary function.

        if (__is_leader) {
	  asm volatile(
                "{\n\t"
                    ".reg .pred p;\n\t"
		    "waitLoop:\n\t\t"
     		        "mbarrier.try_wait.parity.relaxed.cta.shared.b64 p, [%0], %1;\n\t\t"
                        "@!p bra waitLoop;\n\t"
                "}"
                :
		: "r"((int)__cvta_generic_to_shared(&__barrier)), "r"((unsigned)__phase)
		: "memory"
            );
	  __phase = !__phase;
        }
        __syncthreads(); // All threads of prior thread block have "exited".

	// Note: clusterlaunchcontrol.try_cancel arrives at barrier using generic-proxy,
	// and performs cross-proxy fencing required to allows reading '__result' using generic-proxy.
	// Therefore no intra-thread cross-proxy synchronization required to wait on
	// mbarrier or read the result.
        {
            int __success = 0;
            asm volatile(
	        "{\n\t"
                    ".reg .pred p;\n\t"
                    "clusterlaunchcontrol.query_cancel.is_canceled.pred.b128 p, %1;\n\t"
                    "selp.b32 %0, 1, 0, p;\n\t"
                "}\n\t"
                : "=r"(__success)
                : "q"(__result)
            );
            if (__success != 1) {
                // Invalidating mbarrier and synchronizing before exiting not
                // required since each thread block calls this API at most once.
                break;
            }
        }

        // Read new thread block dimensions
        if constexpr (__ThreadBlockDim == 3) {
            int __bx = __detail::__cluster_get_dim<0>(__result);
            int __by = __detail::__cluster_get_dim<1>(__result);
            int __bz = __detail::__cluster_get_dim<2>(__result);
            __block_idx = dim3(__bx, __by, __bz);
        } else if constexpr (__ThreadBlockDim == 2) {
            int __bx = __detail::__cluster_get_dim<0>(__result);
            int __by = __detail::__cluster_get_dim<1>(__result);
            __block_idx = dim3(__bx, __by, 1);
        } else if constexpr (__ThreadBlockDim == 1) {
            int __bx = __detail::__cluster_get_dim<0>(__result);
            __block_idx = dim3(__bx, 1, 1);
        } else {
	  __trap();
	}

        // Wait for all threads to read __result before issuing next async op.
        // generic->generic synchronization
        __syncthreads();
	// TODO: only control-warp requires sync, other warps can arrive
	// TODO: double-buffering results+barrier pairs using phase avoids this sync

        if (__is_leader) {
  	    auto __leader_mask = __activemask();
            asm volatile(
                "{\n\t"
                    ".reg .pred p;\n\t"		
                    // elect.sync is a workaround for peeling loop (#nvbug-id)
                    "elect.sync _|p, %2;\n\t"
                    // Note: the clusterlaunchcontrol.try_cancel operation generic->async writes to 'result' using
		    // async-proxy. Therefore, uni-directional release + acquire generic->async intra-thread synchronization
		    // is required to order the prior loads of the result on the current loop iteration, before the
		    // async-proxy writes of clusterlaunchcontrol.try_cancel of the next iteration.
		    // All cross-proxy fencing is performed in the control-thread only since all threads issuing reads of
		    // result are in the same thread block as the control thread.
                    "@p fence.proxy.async::generic.release.sync_restrict::shared::cta.cluster;\n\t"
                    "@p fence.proxy.async::generic.acquire.sync_restrict::shared::cluster.cluster;"
                    // try to cancel another thread block
                    "@p clusterlaunchcontrol.try_cancel.async.shared::cta.mbarrier::complete_tx::bytes.b128 [%0], [%1];\n\t"
                    "@p mbarrier.arrive.expect_tx.relaxed.cta.shared::cta.b64 _, [%1], 16;\n\t"
                "}"
                :
                : "r"((int)__cvta_generic_to_shared(&__result)), "r"((int)__cvta_generic_to_shared(&__barrier)), "r"(__leader_mask)
                : "memory"
            );
        }
      } while (true);
    ), ( // NV_IF_ELSE_TARGET(NV_PROVIDES_SM_100,
      // SW fall-back for lower compute capabilities.
      // TODO: it may make sense to __trap here instead since lower compute capabilities may want
      // to do something else (grid-stride, atomics, etc.).
      // A higher-level abstraction like for_each should handle that.

      __uf(blockId);
    )
  )  // NV_IF_ELSE_TARGET(NV_PROVIDES_SM_100,
}

} // namespace experimental

_LIBCUDACXX_END_NAMESPACE_CUDA

#endif // _CUDA_TRY_CANCEL
