// -*- C++ -*-
//===--------------------------- atomic -----------------------------------===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef _LIBCUDACXX_ATOMIC
#define _LIBCUDACXX_ATOMIC

/*
    atomic synopsis

namespace std
{

// feature test macro

#define __cpp_lib_atomic_is_always_lock_free // as specified by SG10

 // order and consistency

 enum memory_order: unspecified // enum class in C++20
 {
    relaxed,
    consume, // load-consume
    acquire, // load-acquire
    release, // store-release
    acq_rel, // store-release load-acquire
    seq_cst // store-release load-acquire
 };

 inline constexpr auto memory_order_relaxed = memory_order::relaxed;
 inline constexpr auto memory_order_consume = memory_order::consume;
 inline constexpr auto memory_order_acquire = memory_order::acquire;
 inline constexpr auto memory_order_release = memory_order::release;
 inline constexpr auto memory_order_acq_rel = memory_order::acq_rel;
 inline constexpr auto memory_order_seq_cst = memory_order::seq_cst;

template <class T> T kill_dependency(T y) noexcept;

// lock-free property

#define ATOMIC_BOOL_LOCK_FREE unspecified
#define ATOMIC_CHAR_LOCK_FREE unspecified
#define ATOMIC_CHAR16_T_LOCK_FREE unspecified
#define ATOMIC_CHAR32_T_LOCK_FREE unspecified
#define ATOMIC_WCHAR_T_LOCK_FREE unspecified
#define ATOMIC_SHORT_LOCK_FREE unspecified
#define ATOMIC_INT_LOCK_FREE unspecified
#define ATOMIC_LONG_LOCK_FREE unspecified
#define ATOMIC_LLONG_LOCK_FREE unspecified
#define ATOMIC_POINTER_LOCK_FREE unspecified

// flag type and operations

typedef struct atomic_flag
{
    bool test_and_set(memory_order m = memory_order_seq_cst) volatile noexcept;
    bool test_and_set(memory_order m = memory_order_seq_cst) noexcept;
    void clear(memory_order m = memory_order_seq_cst) volatile noexcept;
    void clear(memory_order m = memory_order_seq_cst) noexcept;
    atomic_flag()  noexcept = default;
    atomic_flag(const atomic_flag&) = delete;
    atomic_flag& operator=(const atomic_flag&) = delete;
    atomic_flag& operator=(const atomic_flag&) volatile = delete;
} atomic_flag;

bool
    atomic_flag_test_and_set(volatile atomic_flag* obj) noexcept;

bool
    atomic_flag_test_and_set(atomic_flag* obj) noexcept;

bool
    atomic_flag_test_and_set_explicit(volatile atomic_flag* obj,
                                      memory_order m) noexcept;

bool
    atomic_flag_test_and_set_explicit(atomic_flag* obj, memory_order m) noexcept;

void
    atomic_flag_clear(volatile atomic_flag* obj) noexcept;

void
    atomic_flag_clear(atomic_flag* obj) noexcept;

void
    atomic_flag_clear_explicit(volatile atomic_flag* obj, memory_order m) noexcept;

void
    atomic_flag_clear_explicit(atomic_flag* obj, memory_order m) noexcept;

#define ATOMIC_FLAG_INIT see below
#define ATOMIC_VAR_INIT(value) see below

template <class T>
struct atomic
{
    static constexpr bool is_always_lock_free;
    bool is_lock_free() const volatile noexcept;
    bool is_lock_free() const noexcept;
    void store(T desr, memory_order m = memory_order_seq_cst) volatile noexcept;
    void store(T desr, memory_order m = memory_order_seq_cst) noexcept;
    T load(memory_order m = memory_order_seq_cst) const volatile noexcept;
    T load(memory_order m = memory_order_seq_cst) const noexcept;
    operator T() const volatile noexcept;
    operator T() const noexcept;
    T exchange(T desr, memory_order m = memory_order_seq_cst) volatile noexcept;
    T exchange(T desr, memory_order m = memory_order_seq_cst) noexcept;
    bool compare_exchange_weak(T& expc, T desr,
                               memory_order s, memory_order f) volatile noexcept;
    bool compare_exchange_weak(T& expc, T desr, memory_order s, memory_order f) noexcept;
    bool compare_exchange_strong(T& expc, T desr,
                                 memory_order s, memory_order f) volatile noexcept;
    bool compare_exchange_strong(T& expc, T desr,
                                 memory_order s, memory_order f) noexcept;
    bool compare_exchange_weak(T& expc, T desr,
                               memory_order m = memory_order_seq_cst) volatile noexcept;
    bool compare_exchange_weak(T& expc, T desr,
                               memory_order m = memory_order_seq_cst) noexcept;
    bool compare_exchange_strong(T& expc, T desr,
                                memory_order m = memory_order_seq_cst) volatile noexcept;
    bool compare_exchange_strong(T& expc, T desr,
                                 memory_order m = memory_order_seq_cst) noexcept;

    atomic() noexcept = default;
    constexpr atomic(T desr) noexcept;
    atomic(const atomic&) = delete;
    atomic& operator=(const atomic&) = delete;
    atomic& operator=(const atomic&) volatile = delete;
    T operator=(T) volatile noexcept;
    T operator=(T) noexcept;
};

template <>
struct atomic<integral>
{
    static constexpr bool is_always_lock_free;
    bool is_lock_free() const volatile noexcept;
    bool is_lock_free() const noexcept;
    void store(integral desr, memory_order m = memory_order_seq_cst) volatile noexcept;
    void store(integral desr, memory_order m = memory_order_seq_cst) noexcept;
    integral load(memory_order m = memory_order_seq_cst) const volatile noexcept;
    integral load(memory_order m = memory_order_seq_cst) const noexcept;
    operator integral() const volatile noexcept;
    operator integral() const noexcept;
    integral exchange(integral desr,
                      memory_order m = memory_order_seq_cst) volatile noexcept;
    integral exchange(integral desr, memory_order m = memory_order_seq_cst) noexcept;
    bool compare_exchange_weak(integral& expc, integral desr,
                               memory_order s, memory_order f) volatile noexcept;
    bool compare_exchange_weak(integral& expc, integral desr,
                               memory_order s, memory_order f) noexcept;
    bool compare_exchange_strong(integral& expc, integral desr,
                                 memory_order s, memory_order f) volatile noexcept;
    bool compare_exchange_strong(integral& expc, integral desr,
                                 memory_order s, memory_order f) noexcept;
    bool compare_exchange_weak(integral& expc, integral desr,
                               memory_order m = memory_order_seq_cst) volatile noexcept;
    bool compare_exchange_weak(integral& expc, integral desr,
                               memory_order m = memory_order_seq_cst) noexcept;
    bool compare_exchange_strong(integral& expc, integral desr,
                                memory_order m = memory_order_seq_cst) volatile noexcept;
    bool compare_exchange_strong(integral& expc, integral desr,
                                 memory_order m = memory_order_seq_cst) noexcept;

    integral
        fetch_add(integral op, memory_order m = memory_order_seq_cst) volatile noexcept;
    integral fetch_add(integral op, memory_order m = memory_order_seq_cst) noexcept;
    integral
        fetch_sub(integral op, memory_order m = memory_order_seq_cst) volatile noexcept;
    integral fetch_sub(integral op, memory_order m = memory_order_seq_cst) noexcept;
    integral
        fetch_and(integral op, memory_order m = memory_order_seq_cst) volatile noexcept;
    integral fetch_and(integral op, memory_order m = memory_order_seq_cst) noexcept;
    integral
        fetch_or(integral op, memory_order m = memory_order_seq_cst) volatile noexcept;
    integral fetch_or(integral op, memory_order m = memory_order_seq_cst) noexcept;
    integral
        fetch_xor(integral op, memory_order m = memory_order_seq_cst) volatile noexcept;
    integral fetch_xor(integral op, memory_order m = memory_order_seq_cst) noexcept;

    atomic() noexcept = default;
    constexpr atomic(integral desr) noexcept;
    atomic(const atomic&) = delete;
    atomic& operator=(const atomic&) = delete;
    atomic& operator=(const atomic&) volatile = delete;
    integral operator=(integral desr) volatile noexcept;
    integral operator=(integral desr) noexcept;

    integral operator++(int) volatile noexcept;
    integral operator++(int) noexcept;
    integral operator--(int) volatile noexcept;
    integral operator--(int) noexcept;
    integral operator++() volatile noexcept;
    integral operator++() noexcept;
    integral operator--() volatile noexcept;
    integral operator--() noexcept;
    integral operator+=(integral op) volatile noexcept;
    integral operator+=(integral op) noexcept;
    integral operator-=(integral op) volatile noexcept;
    integral operator-=(integral op) noexcept;
    integral operator&=(integral op) volatile noexcept;
    integral operator&=(integral op) noexcept;
    integral operator|=(integral op) volatile noexcept;
    integral operator|=(integral op) noexcept;
    integral operator^=(integral op) volatile noexcept;
    integral operator^=(integral op) noexcept;
};

template <class T>
struct atomic<T*>
{
    static constexpr bool is_always_lock_free;
    bool is_lock_free() const volatile noexcept;
    bool is_lock_free() const noexcept;
    void store(T* desr, memory_order m = memory_order_seq_cst) volatile noexcept;
    void store(T* desr, memory_order m = memory_order_seq_cst) noexcept;
    T* load(memory_order m = memory_order_seq_cst) const volatile noexcept;
    T* load(memory_order m = memory_order_seq_cst) const noexcept;
    operator T*() const volatile noexcept;
    operator T*() const noexcept;
    T* exchange(T* desr, memory_order m = memory_order_seq_cst) volatile noexcept;
    T* exchange(T* desr, memory_order m = memory_order_seq_cst) noexcept;
    bool compare_exchange_weak(T*& expc, T* desr,
                               memory_order s, memory_order f) volatile noexcept;
    bool compare_exchange_weak(T*& expc, T* desr,
                               memory_order s, memory_order f) noexcept;
    bool compare_exchange_strong(T*& expc, T* desr,
                                 memory_order s, memory_order f) volatile noexcept;
    bool compare_exchange_strong(T*& expc, T* desr,
                                 memory_order s, memory_order f) noexcept;
    bool compare_exchange_weak(T*& expc, T* desr,
                               memory_order m = memory_order_seq_cst) volatile noexcept;
    bool compare_exchange_weak(T*& expc, T* desr,
                               memory_order m = memory_order_seq_cst) noexcept;
    bool compare_exchange_strong(T*& expc, T* desr,
                                memory_order m = memory_order_seq_cst) volatile noexcept;
    bool compare_exchange_strong(T*& expc, T* desr,
                                 memory_order m = memory_order_seq_cst) noexcept;
    T* fetch_add(ptrdiff_t op, memory_order m = memory_order_seq_cst) volatile noexcept;
    T* fetch_add(ptrdiff_t op, memory_order m = memory_order_seq_cst) noexcept;
    T* fetch_sub(ptrdiff_t op, memory_order m = memory_order_seq_cst) volatile noexcept;
    T* fetch_sub(ptrdiff_t op, memory_order m = memory_order_seq_cst) noexcept;

    atomic() noexcept = default;
    constexpr atomic(T* desr) noexcept;
    atomic(const atomic&) = delete;
    atomic& operator=(const atomic&) = delete;
    atomic& operator=(const atomic&) volatile = delete;

    T* operator=(T*) volatile noexcept;
    T* operator=(T*) noexcept;
    T* operator++(int) volatile noexcept;
    T* operator++(int) noexcept;
    T* operator--(int) volatile noexcept;
    T* operator--(int) noexcept;
    T* operator++() volatile noexcept;
    T* operator++() noexcept;
    T* operator--() volatile noexcept;
    T* operator--() noexcept;
    T* operator+=(ptrdiff_t op) volatile noexcept;
    T* operator+=(ptrdiff_t op) noexcept;
    T* operator-=(ptrdiff_t op) volatile noexcept;
    T* operator-=(ptrdiff_t op) noexcept;
};


template <class T>
    bool
    atomic_is_lock_free(const volatile atomic<T>* obj) noexcept;

template <class T>
    bool
    atomic_is_lock_free(const atomic<T>* obj) noexcept;

template <class T>
    void
    atomic_init(volatile atomic<T>* obj, T desr) noexcept;

template <class T>
    void
    atomic_init(atomic<T>* obj, T desr) noexcept;

template <class T>
    void
    atomic_store(volatile atomic<T>* obj, T desr) noexcept;

template <class T>
    void
    atomic_store(atomic<T>* obj, T desr) noexcept;

template <class T>
    void
    atomic_store_explicit(volatile atomic<T>* obj, T desr, memory_order m) noexcept;

template <class T>
    void
    atomic_store_explicit(atomic<T>* obj, T desr, memory_order m) noexcept;

template <class T>
    T
    atomic_load(const volatile atomic<T>* obj) noexcept;

template <class T>
    T
    atomic_load(const atomic<T>* obj) noexcept;

template <class T>
    T
    atomic_load_explicit(const volatile atomic<T>* obj, memory_order m) noexcept;

template <class T>
    T
    atomic_load_explicit(const atomic<T>* obj, memory_order m) noexcept;

template <class T>
    T
    atomic_exchange(volatile atomic<T>* obj, T desr) noexcept;

template <class T>
    T
    atomic_exchange(atomic<T>* obj, T desr) noexcept;

template <class T>
    T
    atomic_exchange_explicit(volatile atomic<T>* obj, T desr, memory_order m) noexcept;

template <class T>
    T
    atomic_exchange_explicit(atomic<T>* obj, T desr, memory_order m) noexcept;

template <class T>
    bool
    atomic_compare_exchange_weak(volatile atomic<T>* obj, T* expc, T desr) noexcept;

template <class T>
    bool
    atomic_compare_exchange_weak(atomic<T>* obj, T* expc, T desr) noexcept;

template <class T>
    bool
    atomic_compare_exchange_strong(volatile atomic<T>* obj, T* expc, T desr) noexcept;

template <class T>
    bool
    atomic_compare_exchange_strong(atomic<T>* obj, T* expc, T desr) noexcept;

template <class T>
    bool
    atomic_compare_exchange_weak_explicit(volatile atomic<T>* obj, T* expc,
                                          T desr,
                                          memory_order s, memory_order f) noexcept;

template <class T>
    bool
    atomic_compare_exchange_weak_explicit(atomic<T>* obj, T* expc, T desr,
                                          memory_order s, memory_order f) noexcept;

template <class T>
    bool
    atomic_compare_exchange_strong_explicit(volatile atomic<T>* obj,
                                            T* expc, T desr,
                                            memory_order s, memory_order f) noexcept;

template <class T>
    bool
    atomic_compare_exchange_strong_explicit(atomic<T>* obj, T* expc,
                                            T desr,
                                            memory_order s, memory_order f) noexcept;

template <class Integral>
    Integral
    atomic_fetch_add(volatile atomic<Integral>* obj, Integral op) noexcept;

template <class Integral>
    Integral
    atomic_fetch_add(atomic<Integral>* obj, Integral op) noexcept;

template <class Integral>
    Integral
    atomic_fetch_add_explicit(volatile atomic<Integral>* obj, Integral op,
                              memory_order m) noexcept;
template <class Integral>
    Integral
    atomic_fetch_add_explicit(atomic<Integral>* obj, Integral op,
                              memory_order m) noexcept;
template <class Integral>
    Integral
    atomic_fetch_sub(volatile atomic<Integral>* obj, Integral op) noexcept;

template <class Integral>
    Integral
    atomic_fetch_sub(atomic<Integral>* obj, Integral op) noexcept;

template <class Integral>
    Integral
    atomic_fetch_sub_explicit(volatile atomic<Integral>* obj, Integral op,
                              memory_order m) noexcept;
template <class Integral>
    Integral
    atomic_fetch_sub_explicit(atomic<Integral>* obj, Integral op,
                              memory_order m) noexcept;
template <class Integral>
    Integral
    atomic_fetch_and(volatile atomic<Integral>* obj, Integral op) noexcept;

template <class Integral>
    Integral
    atomic_fetch_and(atomic<Integral>* obj, Integral op) noexcept;

template <class Integral>
    Integral
    atomic_fetch_and_explicit(volatile atomic<Integral>* obj, Integral op,
                              memory_order m) noexcept;
template <class Integral>
    Integral
    atomic_fetch_and_explicit(atomic<Integral>* obj, Integral op,
                              memory_order m) noexcept;
template <class Integral>
    Integral
    atomic_fetch_or(volatile atomic<Integral>* obj, Integral op) noexcept;

template <class Integral>
    Integral
    atomic_fetch_or(atomic<Integral>* obj, Integral op) noexcept;

template <class Integral>
    Integral
    atomic_fetch_or_explicit(volatile atomic<Integral>* obj, Integral op,
                             memory_order m) noexcept;
template <class Integral>
    Integral
    atomic_fetch_or_explicit(atomic<Integral>* obj, Integral op,
                             memory_order m) noexcept;
template <class Integral>
    Integral
    atomic_fetch_xor(volatile atomic<Integral>* obj, Integral op) noexcept;

template <class Integral>
    Integral
    atomic_fetch_xor(atomic<Integral>* obj, Integral op) noexcept;

template <class Integral>
    Integral
    atomic_fetch_xor_explicit(volatile atomic<Integral>* obj, Integral op,
                              memory_order m) noexcept;
template <class Integral>
    Integral
    atomic_fetch_xor_explicit(atomic<Integral>* obj, Integral op,
                              memory_order m) noexcept;

template <class T>
    T*
    atomic_fetch_add(volatile atomic<T*>* obj, ptrdiff_t op) noexcept;

template <class T>
    T*
    atomic_fetch_add(atomic<T*>* obj, ptrdiff_t op) noexcept;

template <class T>
    T*
    atomic_fetch_add_explicit(volatile atomic<T*>* obj, ptrdiff_t op,
                              memory_order m) noexcept;
template <class T>
    T*
    atomic_fetch_add_explicit(atomic<T*>* obj, ptrdiff_t op, memory_order m) noexcept;

template <class T>
    T*
    atomic_fetch_sub(volatile atomic<T*>* obj, ptrdiff_t op) noexcept;

template <class T>
    T*
    atomic_fetch_sub(atomic<T*>* obj, ptrdiff_t op) noexcept;

template <class T>
    T*
    atomic_fetch_sub_explicit(volatile atomic<T*>* obj, ptrdiff_t op,
                              memory_order m) noexcept;
template <class T>
    T*
    atomic_fetch_sub_explicit(atomic<T*>* obj, ptrdiff_t op, memory_order m) noexcept;

// Atomics for standard typedef types

typedef atomic<bool>               atomic_bool;
typedef atomic<char>               atomic_char;
typedef atomic<signed char>        atomic_schar;
typedef atomic<unsigned char>      atomic_uchar;
typedef atomic<short>              atomic_short;
typedef atomic<unsigned short>     atomic_ushort;
typedef atomic<int>                atomic_int;
typedef atomic<unsigned int>       atomic_uint;
typedef atomic<long>               atomic_long;
typedef atomic<unsigned long>      atomic_ulong;
typedef atomic<long long>          atomic_llong;
typedef atomic<unsigned long long> atomic_ullong;
typedef atomic<char16_t>           atomic_char16_t;
typedef atomic<char32_t>           atomic_char32_t;
typedef atomic<wchar_t>            atomic_wchar_t;

typedef atomic<int_least8_t>   atomic_int_least8_t;
typedef atomic<uint_least8_t>  atomic_uint_least8_t;
typedef atomic<int_least16_t>  atomic_int_least16_t;
typedef atomic<uint_least16_t> atomic_uint_least16_t;
typedef atomic<int_least32_t>  atomic_int_least32_t;
typedef atomic<uint_least32_t> atomic_uint_least32_t;
typedef atomic<int_least64_t>  atomic_int_least64_t;
typedef atomic<uint_least64_t> atomic_uint_least64_t;

typedef atomic<int_fast8_t>   atomic_int_fast8_t;
typedef atomic<uint_fast8_t>  atomic_uint_fast8_t;
typedef atomic<int_fast16_t>  atomic_int_fast16_t;
typedef atomic<uint_fast16_t> atomic_uint_fast16_t;
typedef atomic<int_fast32_t>  atomic_int_fast32_t;
typedef atomic<uint_fast32_t> atomic_uint_fast32_t;
typedef atomic<int_fast64_t>  atomic_int_fast64_t;
typedef atomic<uint_fast64_t> atomic_uint_fast64_t;

typedef atomic<int8_t>   atomic_int8_t;
typedef atomic<uint8_t>  atomic_uint8_t;
typedef atomic<int16_t>  atomic_int16_t;
typedef atomic<uint16_t> atomic_uint16_t;
typedef atomic<int32_t>  atomic_int32_t;
typedef atomic<uint32_t> atomic_uint32_t;
typedef atomic<int64_t>  atomic_int64_t;
typedef atomic<uint64_t> atomic_uint64_t;

typedef atomic<intptr_t>  atomic_intptr_t;
typedef atomic<uintptr_t> atomic_uintptr_t;
typedef atomic<size_t>    atomic_size_t;
typedef atomic<ptrdiff_t> atomic_ptrdiff_t;
typedef atomic<intmax_t>  atomic_intmax_t;
typedef atomic<uintmax_t> atomic_uintmax_t;

// fences

void atomic_thread_fence(memory_order m) noexcept;
void atomic_signal_fence(memory_order m) noexcept;

}  // std

*/

#include <cuda/std/detail/__config>

#if defined(_CCCL_IMPLICIT_SYSTEM_HEADER_GCC)
#  pragma GCC system_header
#elif defined(_CCCL_IMPLICIT_SYSTEM_HEADER_CLANG)
#  pragma clang system_header
#elif defined(_CCCL_IMPLICIT_SYSTEM_HEADER_MSVC)
#  pragma system_header
#endif // no system header

#include <cuda/std/detail/libcxx/include/__assert> // all public C++ headers provide the assertion handler
#include <cuda/std/detail/libcxx/include/__debug>
#include <cuda/std/detail/libcxx/include/__threading_support>
#include <cuda/std/__type_traits/conditional.h>
#include <cuda/std/__type_traits/enable_if.h>
#include <cuda/std/__type_traits/is_assignable.h>
#include <cuda/std/__type_traits/is_floating_point.h>
#include <cuda/std/__type_traits/is_integral.h>
#include <cuda/std/__type_traits/is_same.h>
#include <cuda/std/__type_traits/is_trivially_copyable.h>
#include <cuda/std/__type_traits/underlying_type.h>
#include <cuda/std/__utility/forward.h>
#include <cuda/std/detail/libcxx/include/cstring>
#include <cuda/std/cstddef>
#include <cuda/std/cstdint>
#include <cuda/std/type_traits>
#include <cuda/std/version>

#include <cuda/std/detail/libcxx/include/__pragma_push>

#ifdef _LIBCUDACXX_HAS_NO_THREADS
# error <atomic> is not supported on this single threaded system
#endif
#ifdef _LIBCUDACXX_HAS_NO_ATOMIC_HEADER
# error <atomic> is not implemented
#endif
#ifdef _LIBCUDACXX_UNSUPPORTED_THREAD_API
# error "<atomic> is not supported on this system"
#endif
#ifdef kill_dependency
# error C++ standard library is incompatible with <stdatomic.h>
#endif

#include <cuda/std/__atomic/order.h>
#include <cuda/std/__atomic/scopes.h>

#include <cuda/std/__atomic/storage/common.h>
#include <cuda/std/__atomic/storage/base.h>
#include <cuda/std/__atomic/storage/locked.h>
#include <cuda/std/__atomic/storage/reference.h>
#include <cuda/std/__atomic/storage/small.h>

#include <cuda/std/__atomic/wait/polling.h>
#include <cuda/std/__atomic/wait/notify_wait.h>

_LIBCUDACXX_BEGIN_NAMESPACE_STD


template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp kill_dependency(_Tp __y) noexcept
{
    return __y;
}

template <typename _Tp>
struct __atomic_impl_traits {
    static  constexpr bool __atomic_requires_lock = __atomic_is_always_lock_free<_Tp>::__value;
    static  constexpr bool __atomic_requires_small = sizeof(_Tp) < 4;
    static  constexpr bool __atomic_supports_reference = sizeof(_Tp) >= 4 && sizeof(_Tp) <= 8;

    using __atomic_storage_t = typename __conditional_t<__atomic_requires_small,
                                                __atomic_small_storage<_Tp>,
                                                __conditional_t<__atomic_requires_lock,
                                                    __atomic_locked_storage<_Tp>,
                                                    __atomic_storage<_Tp>
                                                    >>;

    using __atomic_ref_storage_t = typename __atomic_ref_storage<_Tp>;
};

template <class _Tp, typename _Storage>
struct __atomic_base_storage {
    mutable _Storage __a_;

    __atomic_base_storage() = default;
    __atomic_base_storage(const __atomic_base_storage&) = default;
    __atomic_base_storage(__atomic_base_storage&&) = default;

    __atomic_base_storage& operator=(const __atomic_base_storage&) = default;
    __atomic_base_storage& operator=(__atomic_base_storage&&) = default;

    _LIBCUDACXX_INLINE_VISIBILITY constexpr
    __atomic_base_storage(_Storage&& __a) noexcept : __a_(_CUDA_VSTD::forward<_Storage>(__a)) {}
};

template <class _Tp, bool _Cq, typename _Storage>
struct __atomic_base_core : public __atomic_base_storage<_Tp, _Storage>{
    __atomic_base_core() = default;
    __atomic_base_core(const __atomic_base_core&) = delete;
    __atomic_base_core(__atomic_base_core&&) = delete;

    __atomic_base_core& operator=(const __atomic_base_core&) = delete;
    __atomic_base_core& operator=(__atomic_base_core&&) = delete;

    _LIBCUDACXX_INLINE_VISIBILITY constexpr
    __atomic_base_core(_Storage&& __a) noexcept : __atomic_base_storage<_Tp, _Storage>(_CUDA_VSTD::forward<_Storage>(__a)) {}

#if defined(_LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE)
    static constexpr bool is_always_lock_free = _LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE(sizeof(_Tp), 0);
#endif // defined(_LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE)

    _LIBCUDACXX_INLINE_VISIBILITY
    bool is_lock_free() const volatile noexcept
        {return _LIBCUDACXX_ATOMIC_IS_LOCK_FREE(sizeof(_Tp));}
    _LIBCUDACXX_INLINE_VISIBILITY
    bool is_lock_free() const noexcept
        {return static_cast<__atomic_base_core const volatile*>(this)->is_lock_free();}
    _LIBCUDACXX_INLINE_VISIBILITY

    void store(_Tp __d, memory_order __m = memory_order_seq_cst) volatile noexcept
      _LIBCUDACXX_CHECK_STORE_MEMORY_ORDER(__m)
        {__atomic_store_dispatch(this->__a_, __d, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    void store(_Tp __d, memory_order __m = memory_order_seq_cst) noexcept
      _LIBCUDACXX_CHECK_STORE_MEMORY_ORDER(__m)
        {__atomic_store_dispatch(this->__a_, __d, __m);}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp load(memory_order __m = memory_order_seq_cst) const volatile noexcept
      _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m)
        {return __atomic_load_dispatch(this->__a_, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp load(memory_order __m = memory_order_seq_cst) const noexcept
      _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m)
        {return __atomic_load_dispatch(this->__a_, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    operator _Tp() const volatile noexcept {return load();}
    _LIBCUDACXX_INLINE_VISIBILITY
    operator _Tp() const noexcept          {return load();}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp exchange(_Tp __d, memory_order __m = memory_order_seq_cst) volatile noexcept
        {return __atomic_exchange_dispatch(this->__a_, __d, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp exchange(_Tp __d, memory_order __m = memory_order_seq_cst) noexcept
        {return __atomic_exchange_dispatch(this->__a_, __d, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    bool compare_exchange_weak(_Tp& __e, _Tp __d,
                               memory_order __s, memory_order __f) volatile noexcept
      _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)
        {return __atomic_compare_exchange_weak_dispatch(this->__a_, &__e, __d, __s, __f);}
    _LIBCUDACXX_INLINE_VISIBILITY
    bool compare_exchange_weak(_Tp& __e, _Tp __d,
                               memory_order __s, memory_order __f) noexcept
      _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)
        {return __atomic_compare_exchange_weak_dispatch(this->__a_, &__e, __d, __s, __f);}
    _LIBCUDACXX_INLINE_VISIBILITY
    bool compare_exchange_strong(_Tp& __e, _Tp __d,
                                 memory_order __s, memory_order __f) volatile noexcept
      _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)
        {return __atomic_compare_exchange_strong_dispatch(this->__a_, &__e, __d, __s, __f);}
    _LIBCUDACXX_INLINE_VISIBILITY
    bool compare_exchange_strong(_Tp& __e, _Tp __d,
                                 memory_order __s, memory_order __f) noexcept
      _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)
        {return __atomic_compare_exchange_strong_dispatch(this->__a_, &__e, __d, __s, __f);}
    _LIBCUDACXX_INLINE_VISIBILITY
    bool compare_exchange_weak(_Tp& __e, _Tp __d,
                              memory_order __m = memory_order_seq_cst) volatile noexcept {
        if (memory_order_acq_rel == __m)
            return __atomic_compare_exchange_weak_dispatch(this->__a_, &__e, __d, __m, memory_order_acquire);
        else if (memory_order_release == __m)
            return __atomic_compare_exchange_weak_dispatch(this->__a_, &__e, __d, __m, memory_order_relaxed);
        else
            return __atomic_compare_exchange_weak_dispatch(this->__a_, &__e, __d, __m, __m);
    }
    _LIBCUDACXX_INLINE_VISIBILITY
    bool compare_exchange_weak(_Tp& __e, _Tp __d,
                               memory_order __m = memory_order_seq_cst) noexcept {
        if(memory_order_acq_rel == __m)
            return __atomic_compare_exchange_weak_dispatch(this->__a_, &__e, __d, __m, memory_order_acquire);
        else if(memory_order_release == __m)
            return __atomic_compare_exchange_weak_dispatch(this->__a_, &__e, __d, __m, memory_order_relaxed);
        else
            return __atomic_compare_exchange_weak_dispatch(this->__a_, &__e, __d, __m, __m);
    }
    _LIBCUDACXX_INLINE_VISIBILITY
    bool compare_exchange_strong(_Tp& __e, _Tp __d,
                              memory_order __m = memory_order_seq_cst) volatile noexcept {
        if (memory_order_acq_rel == __m)
            return __atomic_compare_exchange_strong_dispatch(this->__a_, &__e, __d, __m, memory_order_acquire);
        else if (memory_order_release == __m)
            return __atomic_compare_exchange_strong_dispatch(this->__a_, &__e, __d, __m, memory_order_relaxed);
        else
            return __atomic_compare_exchange_strong_dispatch(this->__a_, &__e, __d, __m, __m);
    }
    _LIBCUDACXX_INLINE_VISIBILITY
    bool compare_exchange_strong(_Tp& __e, _Tp __d,
                                 memory_order __m = memory_order_seq_cst) noexcept {
        if (memory_order_acq_rel == __m)
            return __atomic_compare_exchange_strong_dispatch(this->__a_, &__e, __d, __m, memory_order_acquire);
        else if (memory_order_release == __m)
            return __atomic_compare_exchange_strong_dispatch(this->__a_, &__e, __d, __m, memory_order_relaxed);
        else
            return __atomic_compare_exchange_strong_dispatch(this->__a_, &__e, __d, __m, __m);
    }

    _LIBCUDACXX_INLINE_VISIBILITY void wait(_Tp __v, memory_order __m = memory_order_seq_cst) const volatile noexcept
        {__atomic_wait_dispatch(this->__a_, __v, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY void wait(_Tp __v, memory_order __m = memory_order_seq_cst) const noexcept
        {__atomic_wait_dispatch(this->__a_, __v, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY void notify_one() volatile noexcept
        {__atomic_notify_one_dispatch(this->__a_);}
    _LIBCUDACXX_INLINE_VISIBILITY void notify_one() noexcept
        {__atomic_notify_one_dispatch(this->__a_);}
    _LIBCUDACXX_INLINE_VISIBILITY void notify_all() volatile noexcept
        {__atomic_notify_all_dispatch(this->__a_);}
    _LIBCUDACXX_INLINE_VISIBILITY void notify_all() noexcept
        {__atomic_notify_all_dispatch(this->__a_);}
};

template <class _Tp, typename _Storage>
struct __atomic_base_core<_Tp, true, _Storage> : public __atomic_base_storage<_Tp, _Storage>{
    __atomic_base_core() = default;
    __atomic_base_core(const __atomic_base_core&) = default;
    __atomic_base_core(__atomic_base_core&&) = default;

    __atomic_base_core& operator=(const __atomic_base_core&) = default;
    __atomic_base_core& operator=(__atomic_base_core&&) = default;

    _LIBCUDACXX_INLINE_VISIBILITY constexpr
    __atomic_base_core(_Storage&& __a) noexcept : __atomic_base_storage<_Tp, _Storage>(_CUDA_VSTD::forward<_Storage>(__a)) {}

#if defined(_LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE)
    static constexpr bool is_always_lock_free = _LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE(sizeof(_Tp), 0);
#endif // defined(_LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE)

    _LIBCUDACXX_INLINE_VISIBILITY
    bool is_lock_free() const volatile noexcept
        {return _LIBCUDACXX_ATOMIC_IS_LOCK_FREE(sizeof(_Tp));}
    _LIBCUDACXX_INLINE_VISIBILITY
    bool is_lock_free() const noexcept
        {return static_cast<__atomic_base_core const volatile*>(this)->is_lock_free();}
    _LIBCUDACXX_INLINE_VISIBILITY

    void store(_Tp __d, memory_order __m = memory_order_seq_cst) const volatile noexcept
      _LIBCUDACXX_CHECK_STORE_MEMORY_ORDER(__m)
        {__atomic_store_dispatch(this->__a_, __d, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    void store(_Tp __d, memory_order __m = memory_order_seq_cst) const noexcept
      _LIBCUDACXX_CHECK_STORE_MEMORY_ORDER(__m)
        {__atomic_store_dispatch(this->__a_, __d, __m);}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp load(memory_order __m = memory_order_seq_cst) const volatile noexcept
      _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m)
        {return __atomic_load_dispatch(this->__a_, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp load(memory_order __m = memory_order_seq_cst) const noexcept
      _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m)
        {return __atomic_load_dispatch(this->__a_, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    operator _Tp() const volatile noexcept {return load();}
    _LIBCUDACXX_INLINE_VISIBILITY
    operator _Tp() const noexcept          {return load();}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp exchange(_Tp __d, memory_order __m = memory_order_seq_cst) const volatile noexcept
        {return __atomic_exchange_dispatch(this->__a_, __d, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp exchange(_Tp __d, memory_order __m = memory_order_seq_cst) const noexcept
        {return __atomic_exchange_dispatch(this->__a_, __d, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    bool compare_exchange_weak(_Tp& __e, _Tp __d,
                               memory_order __s, memory_order __f) const volatile noexcept
      _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)
        {return __atomic_compare_exchange_weak_dispatch(this->__a_, &__e, __d, __s, __f);}
    _LIBCUDACXX_INLINE_VISIBILITY
    bool compare_exchange_weak(_Tp& __e, _Tp __d,
                               memory_order __s, memory_order __f) const noexcept
      _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)
        {return __atomic_compare_exchange_weak_dispatch(this->__a_, &__e, __d, __s, __f);}
    _LIBCUDACXX_INLINE_VISIBILITY
    bool compare_exchange_strong(_Tp& __e, _Tp __d,
                                 memory_order __s, memory_order __f) const volatile noexcept
      _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)
        {return __atomic_compare_exchange_strong_dispatch(this->__a_, &__e, __d, __s, __f);}
    _LIBCUDACXX_INLINE_VISIBILITY
    bool compare_exchange_strong(_Tp& __e, _Tp __d,
                                 memory_order __s, memory_order __f) const noexcept
      _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)
        {return __atomic_compare_exchange_strong_dispatch(this->__a_, &__e, __d, __s, __f);}
    _LIBCUDACXX_INLINE_VISIBILITY
    bool compare_exchange_weak(_Tp& __e, _Tp __d,
                              memory_order __m = memory_order_seq_cst) const volatile noexcept {
        if (memory_order_acq_rel == __m)
            return __atomic_compare_exchange_weak_dispatch(this->__a_, &__e, __d, __m, memory_order_acquire);
        else if (memory_order_release == __m)
            return __atomic_compare_exchange_weak_dispatch(this->__a_, &__e, __d, __m, memory_order_relaxed);
        else
            return __atomic_compare_exchange_weak_dispatch(this->__a_, &__e, __d, __m, __m);
    }
    _LIBCUDACXX_INLINE_VISIBILITY
    bool compare_exchange_weak(_Tp& __e, _Tp __d,
                               memory_order __m = memory_order_seq_cst) const noexcept {
        if(memory_order_acq_rel == __m)
            return __atomic_compare_exchange_weak_dispatch(this->__a_, &__e, __d, __m, memory_order_acquire);
        else if(memory_order_release == __m)
            return __atomic_compare_exchange_weak_dispatch(this->__a_, &__e, __d, __m, memory_order_relaxed);
        else
            return __atomic_compare_exchange_weak_dispatch(this->__a_, &__e, __d, __m, __m);
    }
    _LIBCUDACXX_INLINE_VISIBILITY
    bool compare_exchange_strong(_Tp& __e, _Tp __d,
                              memory_order __m = memory_order_seq_cst) const volatile noexcept {
        if (memory_order_acq_rel == __m)
            return __atomic_compare_exchange_strong_dispatch(this->__a_, &__e, __d, __m, memory_order_acquire);
        else if (memory_order_release == __m)
            return __atomic_compare_exchange_strong_dispatch(this->__a_, &__e, __d, __m, memory_order_relaxed);
        else
            return __atomic_compare_exchange_strong_dispatch(this->__a_, &__e, __d, __m, __m);
    }
    _LIBCUDACXX_INLINE_VISIBILITY
    bool compare_exchange_strong(_Tp& __e, _Tp __d,
                                 memory_order __m = memory_order_seq_cst) const noexcept {
        if (memory_order_acq_rel == __m)
            return __atomic_compare_exchange_strong_dispatch(this->__a_, &__e, __d, __m, memory_order_acquire);
        else if (memory_order_release == __m)
            return __atomic_compare_exchange_strong_dispatch(this->__a_, &__e, __d, __m, memory_order_relaxed);
        else
            return __atomic_compare_exchange_strong_dispatch(this->__a_, &__e, __d, __m, __m);
    }

    _LIBCUDACXX_INLINE_VISIBILITY void wait(_Tp __v, memory_order __m = memory_order_seq_cst) const volatile noexcept
        {__atomic_wait_dispatch(this->__a_, __v, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY void wait(_Tp __v, memory_order __m = memory_order_seq_cst) const noexcept
        {__atomic_wait_dispatch(this->__a_, __v, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY void notify_one() const volatile noexcept
        {__atomic_notify_one_dispatch(this->__a_);}
    _LIBCUDACXX_INLINE_VISIBILITY void notify_one() const noexcept
        {__atomic_notify_one_dispatch(this->__a_);}
    _LIBCUDACXX_INLINE_VISIBILITY void notify_all() const volatile noexcept
        {__atomic_notify_all_dispatch(this->__a_);}
    _LIBCUDACXX_INLINE_VISIBILITY void notify_all() const noexcept
        {__atomic_notify_all_dispatch(this->__a_);}
};

template <class _Tp, bool _Cq, typename _Storage>
struct __atomic_base_arithmetic : public __atomic_base_core<_Tp, _Cq, _Storage> {
    __atomic_base_arithmetic() = default;
    __atomic_base_arithmetic(const __atomic_base_arithmetic&) = delete;
    __atomic_base_arithmetic(__atomic_base_arithmetic&&) = delete;

    __atomic_base_arithmetic& operator=(const __atomic_base_arithmetic&) = delete;
    __atomic_base_arithmetic& operator=(__atomic_base_arithmetic&&) = delete;

    _LIBCUDACXX_INLINE_VISIBILITY constexpr
    __atomic_base_arithmetic(_Storage&& __a) noexcept : __atomic_base_core<_Tp, _Cq, _Storage>(_CUDA_VSTD::forward<_Storage>(__a)) {}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_add(_Tp __op, memory_order __m = memory_order_seq_cst) volatile noexcept
        {return __atomic_fetch_add_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_add(_Tp __op, memory_order __m = memory_order_seq_cst) noexcept
        {return __atomic_fetch_add_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_sub(_Tp __op, memory_order __m = memory_order_seq_cst) volatile noexcept
        {return __atomic_fetch_sub_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_sub(_Tp __op, memory_order __m = memory_order_seq_cst) noexcept
        {return __atomic_fetch_sub_dispatch(this->__a_, __op, __m);}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator++(int) volatile noexcept      {return fetch_add(_Tp(1));}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator++(int) noexcept               {return fetch_add(_Tp(1));}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator--(int) volatile noexcept      {return fetch_sub(_Tp(1));}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator--(int) noexcept               {return fetch_sub(_Tp(1));}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator++() volatile noexcept         {return fetch_add(_Tp(1)) + _Tp(1);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator++() noexcept                  {return fetch_add(_Tp(1)) + _Tp(1);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator--() volatile noexcept         {return fetch_sub(_Tp(1)) - _Tp(1);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator--() noexcept                  {return fetch_sub(_Tp(1)) - _Tp(1);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator+=(_Tp __op) volatile noexcept {return fetch_add(__op) + __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator+=(_Tp __op) noexcept          {return fetch_add(__op) + __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator-=(_Tp __op) volatile noexcept {return fetch_sub(__op) - __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator-=(_Tp __op) noexcept          {return fetch_sub(__op) - __op;}
};

template <class _Tp, typename _Storage>
struct __atomic_base_arithmetic<_Tp, true, _Storage> : public __atomic_base_core<_Tp, true, _Storage> {
    __atomic_base_arithmetic() = default;
    __atomic_base_arithmetic(const __atomic_base_arithmetic&) = default;
    __atomic_base_arithmetic(__atomic_base_arithmetic&&) = default;

    __atomic_base_arithmetic& operator=(const __atomic_base_arithmetic&) = default;
    __atomic_base_arithmetic& operator=(__atomic_base_arithmetic&&) = default;

    _LIBCUDACXX_INLINE_VISIBILITY constexpr
    __atomic_base_arithmetic(_Storage&& __a) noexcept : __atomic_base_core<_Tp, true, _Storage>(_CUDA_VSTD::forward<_Storage>(__a)) {}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_add(_Tp __op, memory_order __m = memory_order_seq_cst) const volatile noexcept
        {return __atomic_fetch_add_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_add(_Tp __op, memory_order __m = memory_order_seq_cst) const noexcept
        {return __atomic_fetch_add_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_sub(_Tp __op, memory_order __m = memory_order_seq_cst) const volatile noexcept
        {return __atomic_fetch_sub_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_sub(_Tp __op, memory_order __m = memory_order_seq_cst) const noexcept
        {return __atomic_fetch_sub_dispatch(this->__a_, __op, __m);}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator++(int) const volatile noexcept      {return fetch_add(_Tp(1));}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator++(int) const noexcept               {return fetch_add(_Tp(1));}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator--(int) const volatile noexcept      {return fetch_sub(_Tp(1));}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator--(int) const noexcept               {return fetch_sub(_Tp(1));}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator++() const volatile noexcept         {return fetch_add(_Tp(1)) + _Tp(1);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator++() const noexcept                  {return fetch_add(_Tp(1)) + _Tp(1);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator--() const volatile noexcept         {return fetch_sub(_Tp(1)) - _Tp(1);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator--() const noexcept                  {return fetch_sub(_Tp(1)) - _Tp(1);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator+=(_Tp __op) const volatile noexcept {return fetch_add(__op) + __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator+=(_Tp __op) const noexcept          {return fetch_add(__op) + __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator-=(_Tp __op) const volatile noexcept {return fetch_sub(__op) - __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator-=(_Tp __op) const noexcept          {return fetch_sub(__op) - __op;}
};

template <class _Tp, bool _Cq, typename _Storage>
struct __atomic_base_bitwise : public __atomic_base_arithmetic<_Tp, _Cq, _Storage> {
    __atomic_base_bitwise() = default;
    __atomic_base_bitwise(const __atomic_base_bitwise&) = delete;
    __atomic_base_bitwise(__atomic_base_bitwise&&) = delete;

    __atomic_base_bitwise& operator=(const __atomic_base_bitwise&) = delete;
    __atomic_base_bitwise& operator=(__atomic_base_bitwise&&) = delete;

    _LIBCUDACXX_INLINE_VISIBILITY constexpr
    __atomic_base_bitwise(_Storage&& __a) noexcept : __atomic_base_arithmetic<_Tp, _Cq, _Storage>(_CUDA_VSTD::forward<_Storage>(__a)) {}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_and(_Tp __op, memory_order __m = memory_order_seq_cst) volatile noexcept
        {return __atomic_fetch_and_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_and(_Tp __op, memory_order __m = memory_order_seq_cst) noexcept
        {return __atomic_fetch_and_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_or(_Tp __op, memory_order __m = memory_order_seq_cst) volatile noexcept
        {return __atomic_fetch_or_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_or(_Tp __op, memory_order __m = memory_order_seq_cst) noexcept
        {return __atomic_fetch_or_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_xor(_Tp __op, memory_order __m = memory_order_seq_cst) volatile noexcept
        {return __atomic_fetch_xor_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_xor(_Tp __op, memory_order __m = memory_order_seq_cst) noexcept
        {return __atomic_fetch_xor_dispatch(this->__a_, __op, __m);}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator&=(_Tp __op) volatile noexcept {return fetch_and(__op) & __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator&=(_Tp __op) noexcept          {return fetch_and(__op) & __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator|=(_Tp __op) volatile noexcept {return fetch_or(__op) | __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator|=(_Tp __op) noexcept          {return fetch_or(__op) | __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator^=(_Tp __op) volatile noexcept {return fetch_xor(__op) ^ __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator^=(_Tp __op) noexcept          {return fetch_xor(__op) ^ __op;}
};

template <class _Tp, typename _Storage>
struct __atomic_base_bitwise<_Tp, true, _Storage> : public __atomic_base_arithmetic<_Tp, true, _Storage> {
    __atomic_base_bitwise() = default;
    __atomic_base_bitwise(const __atomic_base_bitwise&) = default;
    __atomic_base_bitwise(__atomic_base_bitwise&&) = default;

    __atomic_base_bitwise& operator=(const __atomic_base_bitwise&) = default;
    __atomic_base_bitwise& operator=(__atomic_base_bitwise&&) = default;

    _LIBCUDACXX_INLINE_VISIBILITY constexpr
    __atomic_base_bitwise(_Storage&& __a) noexcept : __atomic_base_arithmetic<_Tp, true, _Storage>(_CUDA_VSTD::forward<_Storage>(__a)) {}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_and(_Tp __op, memory_order __m = memory_order_seq_cst) const volatile noexcept
        {return __atomic_fetch_and_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_and(_Tp __op, memory_order __m = memory_order_seq_cst) const noexcept
        {return __atomic_fetch_and_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_or(_Tp __op, memory_order __m = memory_order_seq_cst) const volatile noexcept
        {return __atomic_fetch_or_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_or(_Tp __op, memory_order __m = memory_order_seq_cst) const noexcept
        {return __atomic_fetch_or_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_xor(_Tp __op, memory_order __m = memory_order_seq_cst) const volatile noexcept
        {return __atomic_fetch_xor_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp fetch_xor(_Tp __op, memory_order __m = memory_order_seq_cst) const noexcept
        {return __atomic_fetch_xor_dispatch(this->__a_, __op, __m);}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator&=(_Tp __op) const volatile noexcept {return fetch_and(__op) & __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator&=(_Tp __op) const noexcept          {return fetch_and(__op) & __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator|=(_Tp __op) const volatile noexcept {return fetch_or(__op) | __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator|=(_Tp __op) const noexcept          {return fetch_or(__op) | __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator^=(_Tp __op) const volatile noexcept {return fetch_xor(__op) ^ __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator^=(_Tp __op) const noexcept          {return fetch_xor(__op) ^ __op;}
};

template <typename _Tp, bool _Cq, typename _Storage>
using __atomic_select_base = __conditional_t<is_floating_point<_Tp>::value,
                                             __atomic_base_arithmetic<_Tp, _Cq, _Storage>,
                                             __conditional_t<is_integral<_Tp>::value,
                                                __atomic_base_bitwise<_Tp, _Cq, _Storage>,
                                                __atomic_base_core<_Tp, _Cq, _Storage> >>;

template <typename _Tp, typename _Base = __atomic_select_base<_Tp, false, typename __atomic_impl_traits<_Tp>::__atomic_storage_t>>
struct __atomic_base : public _Base {
    __atomic_base() = default;
    __atomic_base(const __atomic_base&) = delete;
    __atomic_base(__atomic_base&&) = delete;

    __atomic_base& operator=(const __atomic_base&) = delete;
    __atomic_base& operator=(__atomic_base&&) = delete;

    _LIBCUDACXX_INLINE_VISIBILITY constexpr
    __atomic_base(const _Tp& __a) noexcept :
        _Base(__atomic_impl_traits<_Tp>::__atomic_storage_t(__a)) {}
};

template <typename _Tp, typename _Base = __atomic_select_base<_Tp, true, typename __atomic_impl_traits<_Tp>::__atomic_ref_storage_t>>
struct __atomic_base_ref : public _Base {
    __atomic_base_ref() = default;
    __atomic_base_ref(const __atomic_base_ref&) = default;
    __atomic_base_ref(__atomic_base_ref&&) = default;

    __atomic_base_ref& operator=(const __atomic_base_ref&) = default;
    __atomic_base_ref& operator=(__atomic_base_ref&&) = default;

    _LIBCUDACXX_INLINE_VISIBILITY constexpr
    __atomic_base_ref(_Tp& __a) noexcept :
        _Base(__atomic_impl_traits<_Tp>::__atomic_ref_storage_t(__a)) {}
};

#if defined(_LIBCUDACXX_ATOMIC_ALWAYS_LOCK_FREE)
template <class _Tp, bool _Cq, typename _Storage>
constexpr bool __atomic_base_core<_Tp, _Cq, _Storage>::is_always_lock_free;
#endif

// atomic<T>
template <class _Tp>
struct atomic
    : public __atomic_base<_Tp>
{
    typedef __atomic_base<_Tp> __base;
    using value_type = _Tp;

    atomic() noexcept = default;
    _LIBCUDACXX_INLINE_VISIBILITY
    constexpr atomic(_Tp __d) noexcept : __base(__d) {}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator=(_Tp __d) volatile noexcept
        {__base::store(__d); return __d;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator=(_Tp __d) noexcept
        {__base::store(__d); return __d;}
};

// atomic<T*>

template <class _Tp>
struct atomic<_Tp*>
    : public __atomic_base<_Tp*>
{
    typedef __atomic_base<_Tp*> __base;
    using value_type = _Tp*;

    atomic() noexcept = default;
    _LIBCUDACXX_INLINE_VISIBILITY
    constexpr atomic(_Tp* __d) noexcept : __base(__d) {}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator=(_Tp* __d) volatile noexcept
        {__base::store(__d); return __d;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator=(_Tp* __d) noexcept
        {__base::store(__d); return __d;}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* fetch_add(ptrdiff_t __op, memory_order __m = memory_order_seq_cst)
                                                                        volatile noexcept
        {return __atomic_fetch_add_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* fetch_add(ptrdiff_t __op, memory_order __m = memory_order_seq_cst)
                                                                        noexcept
        {return __atomic_fetch_add_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* fetch_sub(ptrdiff_t __op, memory_order __m = memory_order_seq_cst)
                                                                        volatile noexcept
        {return __atomic_fetch_sub_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* fetch_sub(ptrdiff_t __op, memory_order __m = memory_order_seq_cst)
                                                                        noexcept
        {return __atomic_fetch_sub_dispatch(this->__a_, __op, __m);}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator++(int) volatile noexcept            {return fetch_add(1);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator++(int) noexcept                     {return fetch_add(1);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator--(int) volatile noexcept            {return fetch_sub(1);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator--(int) noexcept                     {return fetch_sub(1);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator++() volatile noexcept               {return fetch_add(1) + 1;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator++() noexcept                        {return fetch_add(1) + 1;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator--() volatile noexcept               {return fetch_sub(1) - 1;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator--() noexcept                        {return fetch_sub(1) - 1;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator+=(ptrdiff_t __op) volatile noexcept {return fetch_add(__op) + __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator+=(ptrdiff_t __op) noexcept          {return fetch_add(__op) + __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator-=(ptrdiff_t __op) volatile noexcept {return fetch_sub(__op) - __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator-=(ptrdiff_t __op) noexcept          {return fetch_sub(__op) - __op;}
};

// atomic_ref<T>

template <class _Tp>
 struct atomic_ref
    : public __atomic_base_ref<_Tp>
{
    typedef __atomic_base_ref<_Tp> __base;
    using value_type = _Tp;

    static constexpr size_t required_alignment = sizeof(_Tp);

    static constexpr bool is_always_lock_free = sizeof(_Tp) <= 8;

    _LIBCUDACXX_INLINE_VISIBILITY
    explicit atomic_ref(_Tp& __ref) : __base(__ref) {}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp operator=(_Tp __v) const volatile noexcept {__base::store(__v); return __v;}
};

// atomic_ref<T*>

template <class _Tp>
 struct atomic_ref<_Tp*>
    : public __atomic_base_ref<_Tp*>
{
    typedef __atomic_base_ref<_Tp*> __base;
    using value_type = _Tp*;

    static constexpr size_t required_alignment = sizeof(_Tp*);

    static constexpr bool is_always_lock_free = sizeof(_Tp*) <= 8;

    _LIBCUDACXX_INLINE_VISIBILITY
    explicit atomic_ref(_Tp*& __ref) : __base(__ref) {}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator=(_Tp* __v) const noexcept {__base::store(__v); return __v;}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* fetch_add(ptrdiff_t __op, memory_order __m = memory_order_seq_cst)
                                                                        const noexcept
        {return __atomic_fetch_add_dispatch(this->__a_, __op, __m);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* fetch_sub(ptrdiff_t __op, memory_order __m = memory_order_seq_cst)
                                                                        const noexcept
        {return __atomic_fetch_sub_dispatch(this->__a_, __op, __m);}

    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator++(int) const noexcept                     {return fetch_add(1);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator--(int) const noexcept                     {return fetch_sub(1);}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator++() const noexcept                        {return fetch_add(1) + 1;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator--() const noexcept                        {return fetch_sub(1) - 1;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator+=(ptrdiff_t __op) const noexcept          {return fetch_add(__op) + __op;}
    _LIBCUDACXX_INLINE_VISIBILITY
    _Tp* operator-=(ptrdiff_t __op) const noexcept          {return fetch_sub(__op) - __op;}
};

// atomic_is_lock_free

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_is_lock_free(const volatile atomic<_Tp>* __o) noexcept
{
    return __o->is_lock_free();
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_is_lock_free(const atomic<_Tp>* __o) noexcept
{
    return __o->is_lock_free();
}

// atomic_init

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
void
atomic_init(volatile atomic<_Tp>* __o, _Tp __d) noexcept
{
    __atomic_init_dispatch(__o->__a_, __d);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
void
atomic_init(atomic<_Tp>* __o, _Tp __d) noexcept
{
    __atomic_init_dispatch(__o->__a_, __d);
}

// atomic_store

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
void
atomic_store(volatile atomic<_Tp>* __o, _Tp __d) noexcept
{
    __o->store(__d);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
void
atomic_store(atomic<_Tp>* __o, _Tp __d) noexcept
{
    __o->store(__d);
}

// atomic_store_explicit

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
void
atomic_store_explicit(volatile atomic<_Tp>* __o, _Tp __d, memory_order __m) noexcept
  _LIBCUDACXX_CHECK_STORE_MEMORY_ORDER(__m)
{
    __o->store(__d, __m);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
void
atomic_store_explicit(atomic<_Tp>* __o, _Tp __d, memory_order __m) noexcept
  _LIBCUDACXX_CHECK_STORE_MEMORY_ORDER(__m)
{
    __o->store(__d, __m);
}

// atomic_load

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp
atomic_load(const volatile atomic<_Tp>* __o) noexcept
{
    return __o->load();
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp
atomic_load(const atomic<_Tp>* __o) noexcept
{
    return __o->load();
}

// atomic_load_explicit

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp
atomic_load_explicit(const volatile atomic<_Tp>* __o, memory_order __m) noexcept
  _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m)
{
    return __o->load(__m);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp
atomic_load_explicit(const atomic<_Tp>* __o, memory_order __m) noexcept
  _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m)
{
    return __o->load(__m);
}

// atomic_exchange

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp
atomic_exchange(volatile atomic<_Tp>* __o, _Tp __d) noexcept
{
    return __o->exchange(__d);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp
atomic_exchange(atomic<_Tp>* __o, _Tp __d) noexcept
{
    return __o->exchange(__d);
}

// atomic_exchange_explicit

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp
atomic_exchange_explicit(volatile atomic<_Tp>* __o, _Tp __d, memory_order __m) noexcept
{
    return __o->exchange(__d, __m);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp
atomic_exchange_explicit(atomic<_Tp>* __o, _Tp __d, memory_order __m) noexcept
{
    return __o->exchange(__d, __m);
}

// atomic_compare_exchange_weak

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_compare_exchange_weak(volatile atomic<_Tp>* __o, _Tp* __e, _Tp __d) noexcept
{
    return __o->compare_exchange_weak(*__e, __d);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_compare_exchange_weak(atomic<_Tp>* __o, _Tp* __e, _Tp __d) noexcept
{
    return __o->compare_exchange_weak(*__e, __d);
}

// atomic_compare_exchange_strong

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_compare_exchange_strong(volatile atomic<_Tp>* __o, _Tp* __e, _Tp __d) noexcept
{
    return __o->compare_exchange_strong(*__e, __d);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_compare_exchange_strong(atomic<_Tp>* __o, _Tp* __e, _Tp __d) noexcept
{
    return __o->compare_exchange_strong(*__e, __d);
}

// atomic_compare_exchange_weak_explicit

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_compare_exchange_weak_explicit(volatile atomic<_Tp>* __o, _Tp* __e,
                                      _Tp __d,
                                      memory_order __s, memory_order __f) noexcept
  _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)
{
    return __o->compare_exchange_weak(*__e, __d, __s, __f);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_compare_exchange_weak_explicit(atomic<_Tp>* __o, _Tp* __e, _Tp __d,
                                      memory_order __s, memory_order __f) noexcept
  _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)
{
    return __o->compare_exchange_weak(*__e, __d, __s, __f);
}

// atomic_compare_exchange_strong_explicit

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_compare_exchange_strong_explicit(volatile atomic<_Tp>* __o,
                                        _Tp* __e, _Tp __d,
                                        memory_order __s, memory_order __f) noexcept
  _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)
{
    return __o->compare_exchange_strong(*__e, __d, __s, __f);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_compare_exchange_strong_explicit(atomic<_Tp>* __o, _Tp* __e,
                                        _Tp __d,
                                        memory_order __s, memory_order __f) noexcept
  _LIBCUDACXX_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)
{
    return __o->compare_exchange_strong(*__e, __d, __s, __f);
}

// atomic_wait

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
void atomic_wait(const volatile atomic<_Tp>* __o,
                    typename atomic<_Tp>::value_type __v) noexcept
{
    return __o->wait(__v);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
void atomic_wait(const atomic<_Tp>* __o,
                    typename atomic<_Tp>::value_type __v) noexcept
{
    return __o->wait(__v);
}

// atomic_wait_explicit

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
void atomic_wait_explicit(const volatile atomic<_Tp>* __o,
                            typename atomic<_Tp>::value_type __v,
                            memory_order __m) noexcept
  _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m)
{
    return __o->wait(__v, __m);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
void atomic_wait_explicit(const atomic<_Tp>* __o,
                            typename atomic<_Tp>::value_type __v,
                            memory_order __m) noexcept
  _LIBCUDACXX_CHECK_LOAD_MEMORY_ORDER(__m)
{
    return __o->wait(__v, __m);
}

// atomic_notify_one

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
void atomic_notify_one(volatile atomic<_Tp>* __o) noexcept
{
    __o->notify_one();
}
template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
void atomic_notify_one(atomic<_Tp>* __o) noexcept
{
    __o->notify_one();
}

// atomic_notify_one

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
void atomic_notify_all(volatile atomic<_Tp>* __o) noexcept
{
    __o->notify_all();
}
template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
void atomic_notify_all(atomic<_Tp>* __o) noexcept
{
    __o->notify_all();
}

// atomic_fetch_add

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    (is_integral<_Tp>::value && !is_same<_Tp, bool>::value) || is_floating_point<_Tp>::value,
    _Tp
>
atomic_fetch_add(volatile atomic<_Tp>* __o, _Tp __op) noexcept
{
    return __o->fetch_add(__op);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    (is_integral<_Tp>::value && !is_same<_Tp, bool>::value) || is_floating_point<_Tp>::value,
    _Tp
>
atomic_fetch_add(atomic<_Tp>* __o, _Tp __op) noexcept
{
    return __o->fetch_add(__op);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp*
atomic_fetch_add(volatile atomic<_Tp*>* __o, ptrdiff_t __op) noexcept
{
    return __o->fetch_add(__op);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp*
atomic_fetch_add(atomic<_Tp*>* __o, ptrdiff_t __op) noexcept
{
    return __o->fetch_add(__op);
}

// atomic_fetch_add_explicit

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    (is_integral<_Tp>::value && !is_same<_Tp, bool>::value) || is_floating_point<_Tp>::value,
    _Tp
>
atomic_fetch_add_explicit(volatile atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept
{
    return __o->fetch_add(__op, __m);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    (is_integral<_Tp>::value && !is_same<_Tp, bool>::value) || is_floating_point<_Tp>::value,
    _Tp
>
atomic_fetch_add_explicit(atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept
{
    return __o->fetch_add(__op, __m);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp*
atomic_fetch_add_explicit(volatile atomic<_Tp*>* __o, ptrdiff_t __op,
                          memory_order __m) noexcept
{
    return __o->fetch_add(__op, __m);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp*
atomic_fetch_add_explicit(atomic<_Tp*>* __o, ptrdiff_t __op, memory_order __m) noexcept
{
    return __o->fetch_add(__op, __m);
}

// atomic_fetch_sub

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    (is_integral<_Tp>::value && !is_same<_Tp, bool>::value) || is_floating_point<_Tp>::value,
    _Tp
>
atomic_fetch_sub(volatile atomic<_Tp>* __o, _Tp __op) noexcept
{
    return __o->fetch_sub(__op);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    (is_integral<_Tp>::value && !is_same<_Tp, bool>::value) || is_floating_point<_Tp>::value,
    _Tp
>
atomic_fetch_sub(atomic<_Tp>* __o, _Tp __op) noexcept
{
    return __o->fetch_sub(__op);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp*
atomic_fetch_sub(volatile atomic<_Tp*>* __o, ptrdiff_t __op) noexcept
{
    return __o->fetch_sub(__op);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp*
atomic_fetch_sub(atomic<_Tp*>* __o, ptrdiff_t __op) noexcept
{
    return __o->fetch_sub(__op);
}

// atomic_fetch_sub_explicit

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    (is_integral<_Tp>::value && !is_same<_Tp, bool>::value) || is_floating_point<_Tp>::value,
    _Tp
>
atomic_fetch_sub_explicit(volatile atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept
{
    return __o->fetch_sub(__op, __m);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    (is_integral<_Tp>::value && !is_same<_Tp, bool>::value) || is_floating_point<_Tp>::value,
    _Tp
>
atomic_fetch_sub_explicit(atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept
{
    return __o->fetch_sub(__op, __m);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp*
atomic_fetch_sub_explicit(volatile atomic<_Tp*>* __o, ptrdiff_t __op,
                          memory_order __m) noexcept
{
    return __o->fetch_sub(__op, __m);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
_Tp*
atomic_fetch_sub_explicit(atomic<_Tp*>* __o, ptrdiff_t __op, memory_order __m) noexcept
{
    return __o->fetch_sub(__op, __m);
}

// atomic_fetch_and

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,
    _Tp
>
atomic_fetch_and(volatile atomic<_Tp>* __o, _Tp __op) noexcept
{
    return __o->fetch_and(__op);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,
    _Tp
>
atomic_fetch_and(atomic<_Tp>* __o, _Tp __op) noexcept
{
    return __o->fetch_and(__op);
}

// atomic_fetch_and_explicit

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,
    _Tp
>
atomic_fetch_and_explicit(volatile atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept
{
    return __o->fetch_and(__op, __m);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,
    _Tp
>
atomic_fetch_and_explicit(atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept
{
    return __o->fetch_and(__op, __m);
}

// atomic_fetch_or

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,
    _Tp
>
atomic_fetch_or(volatile atomic<_Tp>* __o, _Tp __op) noexcept
{
    return __o->fetch_or(__op);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,
    _Tp
>
atomic_fetch_or(atomic<_Tp>* __o, _Tp __op) noexcept
{
    return __o->fetch_or(__op);
}

// atomic_fetch_or_explicit

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,
    _Tp
>
atomic_fetch_or_explicit(volatile atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept
{
    return __o->fetch_or(__op, __m);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,
    _Tp
>
atomic_fetch_or_explicit(atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept
{
    return __o->fetch_or(__op, __m);
}

// atomic_fetch_xor

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,
    _Tp
>
atomic_fetch_xor(volatile atomic<_Tp>* __o, _Tp __op) noexcept
{
    return __o->fetch_xor(__op);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,
    _Tp
>
atomic_fetch_xor(atomic<_Tp>* __o, _Tp __op) noexcept
{
    return __o->fetch_xor(__op);
}

// atomic_fetch_xor_explicit

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,
    _Tp
>
atomic_fetch_xor_explicit(volatile atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept
{
    return __o->fetch_xor(__op, __m);
}

template <class _Tp>
_LIBCUDACXX_INLINE_VISIBILITY
__enable_if_t
<
    is_integral<_Tp>::value && !is_same<_Tp, bool>::value,
    _Tp
>
atomic_fetch_xor_explicit(atomic<_Tp>* __o, _Tp __op, memory_order __m) noexcept
{
    return __o->fetch_xor(__op, __m);
}

// flag type and operations

typedef struct atomic_flag
{
    __atomic_impl_traits<_LIBCUDACXX_ATOMIC_FLAG_TYPE>::__atomic_storage_t __a_;

    _LIBCUDACXX_INLINE_VISIBILITY
    bool test(memory_order __m = memory_order_seq_cst) const volatile noexcept
        {return _LIBCUDACXX_ATOMIC_FLAG_TYPE(true)==__atomic_load_dispatch(__a_, __m, __thread_scope_system_tag{}, __atomic_tag_t<decltype(__a_)>{});}
    _LIBCUDACXX_INLINE_VISIBILITY
    bool test(memory_order __m = memory_order_seq_cst) const noexcept
        {return _LIBCUDACXX_ATOMIC_FLAG_TYPE(true)==__atomic_load_dispatch(__a_, __m, __thread_scope_system_tag{}, __atomic_tag_t<decltype(__a_)>{});}

    _LIBCUDACXX_INLINE_VISIBILITY
    bool test_and_set(memory_order __m = memory_order_seq_cst) volatile noexcept
        {return __atomic_exchange_dispatch(__a_, _LIBCUDACXX_ATOMIC_FLAG_TYPE(true), __m, __thread_scope_system_tag{}, __atomic_tag_t<decltype(__a_)>{});}
    _LIBCUDACXX_INLINE_VISIBILITY
    bool test_and_set(memory_order __m = memory_order_seq_cst) noexcept
        {return __atomic_exchange_dispatch(__a_, _LIBCUDACXX_ATOMIC_FLAG_TYPE(true), __m, __thread_scope_system_tag{}, __atomic_tag_t<decltype(__a_)>{});}
    _LIBCUDACXX_INLINE_VISIBILITY
    void clear(memory_order __m = memory_order_seq_cst) volatile noexcept
        {__atomic_store_dispatch(__a_, _LIBCUDACXX_ATOMIC_FLAG_TYPE(false), __m, __thread_scope_system_tag{}, __atomic_tag_t<decltype(__a_)>{});}
    _LIBCUDACXX_INLINE_VISIBILITY
    void clear(memory_order __m = memory_order_seq_cst) noexcept
        {__atomic_store_dispatch(__a_, _LIBCUDACXX_ATOMIC_FLAG_TYPE(false), __m, __thread_scope_system_tag{}, __atomic_tag_t<decltype(__a_)>{});}

#if !defined(__CUDA_MINIMUM_ARCH__) || __CUDA_MINIMUM_ARCH__ >= 700
    _LIBCUDACXX_INLINE_VISIBILITY
    void wait(bool __v, memory_order __m = memory_order_seq_cst) const volatile noexcept
        {__atomic_wait(&__a_, _LIBCUDACXX_ATOMIC_FLAG_TYPE(__v), __m, __thread_scope_system_tag{});}
    _LIBCUDACXX_INLINE_VISIBILITY
    void wait(bool __v, memory_order __m = memory_order_seq_cst) const noexcept
        {__atomic_wait(&__a_, _LIBCUDACXX_ATOMIC_FLAG_TYPE(__v), __m, __thread_scope_system_tag{});}
    _LIBCUDACXX_INLINE_VISIBILITY
    void notify_one() volatile noexcept
        {__atomic_notify_one(&__a_, __thread_scope_system_tag{});}
    _LIBCUDACXX_INLINE_VISIBILITY
    void notify_one() noexcept
        {__atomic_notify_one(&__a_, __thread_scope_system_tag{});}
    _LIBCUDACXX_INLINE_VISIBILITY
    void notify_all() volatile noexcept
        {__atomic_notify_all(&__a_, __thread_scope_system_tag{});}
    _LIBCUDACXX_INLINE_VISIBILITY
    void notify_all() noexcept
        {__atomic_notify_all(&__a_, __thread_scope_system_tag{});}
#endif

    atomic_flag() noexcept = default;

    _LIBCUDACXX_INLINE_VISIBILITY constexpr
    atomic_flag(bool __b) noexcept : __a_(__b) {} // EXTENSION

    atomic_flag(const atomic_flag&) = delete;
    atomic_flag& operator=(const atomic_flag&) = delete;
    atomic_flag& operator=(const atomic_flag&) volatile = delete;
} atomic_flag;


inline _LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_flag_test(const volatile atomic_flag* __o) noexcept
{
    return __o->test();
}

inline _LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_flag_test(const atomic_flag* __o) noexcept
{
    return __o->test();
}

inline _LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_flag_test_explicit(const volatile atomic_flag* __o, memory_order __m) noexcept
{
    return __o->test(__m);
}

inline _LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_flag_test_explicit(const atomic_flag* __o, memory_order __m) noexcept
{
    return __o->test(__m);
}

inline _LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_flag_test_and_set(volatile atomic_flag* __o) noexcept
{
    return __o->test_and_set();
}

inline _LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_flag_test_and_set(atomic_flag* __o) noexcept
{
    return __o->test_and_set();
}

inline _LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_flag_test_and_set_explicit(volatile atomic_flag* __o, memory_order __m) noexcept
{
    return __o->test_and_set(__m);
}

inline _LIBCUDACXX_INLINE_VISIBILITY
bool
atomic_flag_test_and_set_explicit(atomic_flag* __o, memory_order __m) noexcept
{
    return __o->test_and_set(__m);
}

inline _LIBCUDACXX_INLINE_VISIBILITY
void
atomic_flag_clear(volatile atomic_flag* __o) noexcept
{
    __o->clear();
}

inline _LIBCUDACXX_INLINE_VISIBILITY
void
atomic_flag_clear(atomic_flag* __o) noexcept
{
    __o->clear();
}

inline _LIBCUDACXX_INLINE_VISIBILITY
void
atomic_flag_clear_explicit(volatile atomic_flag* __o, memory_order __m) noexcept
{
    __o->clear(__m);
}

inline _LIBCUDACXX_INLINE_VISIBILITY
void
atomic_flag_clear_explicit(atomic_flag* __o, memory_order __m) noexcept
{
    __o->clear(__m);
}

#if !defined(__CUDA_MINIMUM_ARCH__) || __CUDA_MINIMUM_ARCH__ >= 700

inline _LIBCUDACXX_INLINE_VISIBILITY
void
atomic_flag_wait(const volatile atomic_flag* __o, bool __v) noexcept
{
    __o->wait(__v);
}

inline _LIBCUDACXX_INLINE_VISIBILITY
void
atomic_flag_wait(const atomic_flag* __o, bool __v) noexcept
{
    __o->wait(__v);
}

inline _LIBCUDACXX_INLINE_VISIBILITY
void
atomic_flag_wait_explicit(const volatile atomic_flag* __o,
                          bool __v, memory_order __m) noexcept
{
    __o->wait(__v, __m);
}

inline _LIBCUDACXX_INLINE_VISIBILITY
void
atomic_flag_wait_explicit(const atomic_flag* __o,
                          bool __v, memory_order __m) noexcept
{
    __o->wait(__v, __m);
}

inline _LIBCUDACXX_INLINE_VISIBILITY
void
atomic_flag_notify_one(volatile atomic_flag* __o) noexcept
{
    __o->notify_one();
}

inline _LIBCUDACXX_INLINE_VISIBILITY
void
atomic_flag_notify_one(atomic_flag* __o) noexcept
{
    __o->notify_one();
}

inline _LIBCUDACXX_INLINE_VISIBILITY
void
atomic_flag_notify_all(volatile atomic_flag* __o) noexcept
{
    __o->notify_all();
}

inline _LIBCUDACXX_INLINE_VISIBILITY
void
atomic_flag_notify_all(atomic_flag* __o) noexcept
{
    __o->notify_all();
}

#endif

// fences

inline _LIBCUDACXX_INLINE_VISIBILITY
void
atomic_thread_fence(memory_order __m) noexcept
{
    __atomic_thread_fence_dispatch(__m);
}

inline _LIBCUDACXX_INLINE_VISIBILITY
void
atomic_signal_fence(memory_order __m) noexcept
{
    __atomic_signal_fence_dispatch(__m);
}

// Atomics for standard typedef types

typedef atomic<bool>               atomic_bool;
typedef atomic<char>               atomic_char;
typedef atomic<signed char>        atomic_schar;
typedef atomic<unsigned char>      atomic_uchar;
typedef atomic<short>              atomic_short;
typedef atomic<unsigned short>     atomic_ushort;
typedef atomic<int>                atomic_int;
typedef atomic<unsigned int>       atomic_uint;
typedef atomic<long>               atomic_long;
typedef atomic<unsigned long>      atomic_ulong;
typedef atomic<long long>          atomic_llong;
typedef atomic<unsigned long long> atomic_ullong;
typedef atomic<char16_t>           atomic_char16_t;
typedef atomic<char32_t>           atomic_char32_t;
typedef atomic<wchar_t>            atomic_wchar_t;

typedef atomic<int_least8_t>   atomic_int_least8_t;
typedef atomic<uint_least8_t>  atomic_uint_least8_t;
typedef atomic<int_least16_t>  atomic_int_least16_t;
typedef atomic<uint_least16_t> atomic_uint_least16_t;
typedef atomic<int_least32_t>  atomic_int_least32_t;
typedef atomic<uint_least32_t> atomic_uint_least32_t;
typedef atomic<int_least64_t>  atomic_int_least64_t;
typedef atomic<uint_least64_t> atomic_uint_least64_t;

typedef atomic<int_fast8_t>   atomic_int_fast8_t;
typedef atomic<uint_fast8_t>  atomic_uint_fast8_t;
typedef atomic<int_fast16_t>  atomic_int_fast16_t;
typedef atomic<uint_fast16_t> atomic_uint_fast16_t;
typedef atomic<int_fast32_t>  atomic_int_fast32_t;
typedef atomic<uint_fast32_t> atomic_uint_fast32_t;
typedef atomic<int_fast64_t>  atomic_int_fast64_t;
typedef atomic<uint_fast64_t> atomic_uint_fast64_t;

typedef atomic< int8_t>  atomic_int8_t;
typedef atomic<uint8_t>  atomic_uint8_t;
typedef atomic< int16_t> atomic_int16_t;
typedef atomic<uint16_t> atomic_uint16_t;
typedef atomic< int32_t> atomic_int32_t;
typedef atomic<uint32_t> atomic_uint32_t;
typedef atomic< int64_t> atomic_int64_t;
typedef atomic<uint64_t> atomic_uint64_t;

typedef atomic<intptr_t>  atomic_intptr_t;
typedef atomic<uintptr_t> atomic_uintptr_t;
typedef atomic<size_t>    atomic_size_t;
typedef atomic<ptrdiff_t> atomic_ptrdiff_t;
typedef atomic<intmax_t>  atomic_intmax_t;
typedef atomic<uintmax_t> atomic_uintmax_t;

static_assert(ATOMIC_INT_LOCK_FREE, "This library assumes atomic<int> is lock-free.");

typedef atomic<int>       atomic_signed_lock_free;
typedef atomic<unsigned>  atomic_unsigned_lock_free;

#define ATOMIC_FLAG_INIT {false}
#define ATOMIC_VAR_INIT(__v) {__v}

_LIBCUDACXX_END_NAMESPACE_STD

#include <cuda/std/__cuda/atomic.h>
#include <cuda/std/detail/libcxx/include/__pragma_pop>

#endif  // _LIBCUDACXX_ATOMIC
