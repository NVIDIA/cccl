//===----------------------------------------------------------------------===//
//
// Part of CUDASTF in CUDA C++ Core Libraries,
// under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// SPDX-FileCopyrightText: Copyright (c) 2022-2024 NVIDIA CORPORATION & AFFILIATES.
//
//===----------------------------------------------------------------------===//

/**
 * @file
 *
 * @brief Experiment with local context nesting
 *
 */

#include "cuda/experimental/__stf/utility/stackable_ctx.cuh"
#include <cuda/experimental/stf.cuh>

using namespace cuda::experimental::stf;

int X0(int i) {
    return 17 * i + 45;
}

int main() {
    stackable_ctx ctx;

    int array[1024];
    for (size_t i = 0; i < 1024; i++) {
        array[i] = 1 + i * i;
    }

    auto lA = ctx.logical_data(array).set_symbol("A");

    // repeat : {tmp = a, a++; tmp*=2; a+=tmp}
    for (size_t iter = 0; iter < 10; iter++) {
        ctx.push_graph();

        lA.push(access_mode::rw);

        auto tmp = ctx.logical_data(lA.shape()).set_symbol("tmp");

        ctx.parallel_for(tmp.shape(), tmp.write(), lA.read())->*[] __device__(size_t i, auto tmp, auto a) {
            tmp(i) = a(i);
        };

        ctx.parallel_for(lA.shape(), lA.rw())->*[] __device__(size_t i, auto a) { a(i) += 1; };

        ctx.parallel_for(tmp.shape(), tmp.rw())->*[] __device__(size_t i, auto tmp) { tmp(i) *= 2; };

        ctx.parallel_for(lA.shape(), tmp.read(), lA.rw())->*[] __device__(size_t i, auto tmp, auto a) {
            a(i) += tmp(i);
        };

        lA.pop();

        ctx.pop();
    }

    ctx.finalize();
}
