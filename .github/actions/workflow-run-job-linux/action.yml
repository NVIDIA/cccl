name: "Run Linux Job"
description: "Run a job on a Linux runner."

inputs:
  id:
    description: "A unique identifier."
    required: true
  command:
    description: "The command to run."
    required: true
  image:
    description: "The Docker image to use."
    required: true
  runner:
    description: "The GHA runs-on value."
    required: true
  cuda:
    description: "The CUDA version to use when selecting a devcontainer."
    required: true
  host:
    description: "The host compiler to use when selecting a devcontainer."
    required: true
  # This token must have the "read:enterprise" scope
  dist-token:
    description: "The token used to authenticate with the sccache-dist build cluster."
  producer_id:
    description: "The producer job's ID, if downloading a wheelhouse from a producer."
    required: false

runs:
  using: "composite"
  steps:
    - name: Install dependencies
      shell: sh
      env:
        COMMAND: "${{inputs.command}}"
      run: |
        # Install script dependencies
        alias retry="${{github.workspace}}/ci/util/retry.sh 5 30"
        retry sudo apt update
        retry sudo apt install -y --no-install-recommends tree git
        if echo "$COMMAND" | grep -Eq '"./ci/.*_python.sh"'; then
          echo "is_python=true" >> "$GITHUB_ENV"
        fi
    - name: Add NVCC problem matcher
      shell: bash --noprofile --norc -euo pipefail {0}
      run: |
        echo "::add-matcher::${{github.workspace}}/.github/problem-matchers/problem-matcher.json"
    - name: Get AWS credentials for sccache bucket
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: arn:aws:iam::279114543810:role/gha-oidc-NVIDIA
        aws-region: us-east-2
        role-duration-seconds: 43200 # 12 hours
    - if: ${{ env.is_python == 'true' }}
      name: Download wheelhouse artifact if it exists
      uses: actions/download-artifact@v4
      with:
        name: wheelhouse-${{ inputs.producer_id || inputs.id }}
        path: wheelhouse/
      continue-on-error: true
    - name: Run command # Do not change this step's name, it is checked in parse-job-times.py
      shell: bash --noprofile --norc -euo pipefail {0}
      env:
        CI: true
        RUNNER: "${{inputs.runner}}"
        IMAGE: "${{inputs.image}}"
        # Dereferencing the command from an env var instead of a GHA input avoids issues with escaping
        # semicolons and other special characters (e.g. `-arch "60;70;80"`).
        COMMAND: "${{inputs.command}}"
        DIST_TOKEN: "${{inputs.dist-token}}"
        AWS_ACCESS_KEY_ID: "${{env.AWS_ACCESS_KEY_ID}}"
        AWS_SESSION_TOKEN: "${{env.AWS_SESSION_TOKEN}}"
        AWS_SECRET_ACCESS_KEY: "${{env.AWS_SECRET_ACCESS_KEY}}"
      run: |
        cat <<"EOF" > ci.sh
        #! /usr/bin/env bash
        set -euo pipefail
        ulimit -n $(ulimit -Hn)
        sccache --stop-server >/dev/null 2>&1
        echo -e "\e[1;34mRunning as '$(whoami)' user in $(pwd):\e[0m"
        # Print current dist status to verify we're connected
        echo -e "::group::️⚙️ \e[1;34mBuild cluster status:\e[0m"
        ./ci/sccache_dist_status.sh | sed 's/\"//g' | column -t -s,
        echo -e "::endgroup::"
        echo -e "\e[1;34m${COMMAND}\e[0m"
        exit_code=0
        eval "${COMMAND}" || exit_code=$?
        if [[ "$exit_code" -ne 0 ]]; then
          echo -e "::group::️❗ \e[1;31mInstructions to Reproduce CI Failure Locally\e[0m"
          echo "::error:: To replicate this failure locally, follow the steps below:"
          echo "1. Clone the repository, and navigate to the correct branch and commit:"
          echo "   git clone --branch $GITHUB_REF_NAME --single-branch https://github.com/$GITHUB_REPOSITORY.git && cd $(echo $GITHUB_REPOSITORY | cut -d'/' -f2) && git checkout $GITHUB_SHA"
          echo ""
          echo "2. Run the failed command inside the same Docker container used by this CI job:"
          echo "   .devcontainer/launch.sh -d -c ${{inputs.cuda}} -H ${{inputs.host}} -- ${COMMAND}"
          echo ""
          echo "For additional information, see:"
          echo "   - DevContainer Documentation: https://github.com/NVIDIA/cccl/blob/main/.devcontainer/README.md"
          echo "   - Continuous Integration (CI) Overview: https://github.com/NVIDIA/cccl/blob/main/ci-overview.md"
          echo -e "::endgroup::"
          exit $exit_code
        fi
        EOF

        chmod +x ci.sh

        # The devcontainer will mount this path to the home directory:
        readonly aws_dir="${{github.workspace}}/.aws"
        readonly cfg_dir="${{github.workspace}}/.config"
        mkdir -p "${aws_dir}" "${cfg_dir}/sccache";

        cat <<EOF > "${aws_dir}/config"
        [default]
        bucket=rapids-sccache-devs
        region=us-east-2
        EOF

        cat <<EOF > "${aws_dir}/credentials"
        [default]
        aws_access_key_id=$AWS_ACCESS_KEY_ID
        aws_session_token=$AWS_SESSION_TOKEN
        aws_secret_access_key=$AWS_SECRET_ACCESS_KEY
        EOF

        # Configure the sccache client
        cat <<EOF > "${cfg_dir}/sccache/config"
        server_startup_timeout_ms = $((5 * 60 * 1000))
        [cache.disk]
        size = 0
        [cache.disk.preprocessor_cache_mode]
        use_preprocessor_cache_mode = false
        EOF

        chmod 0600 "${aws_dir}/credentials"
        chmod 0664 "${aws_dir}/config"
        chmod 0664 "${cfg_dir}/sccache/config"

        # Download new sccache binary
        mkdir -p "${{runner.temp}}/bin"
        curl -fsSL \
          "https://github.com/rapidsai/sccache/releases/download/v0.10.0-rapids.35/sccache-v0.10.0-rapids.35-$(uname -m)-unknown-linux-musl.tar.gz" \
        | tar -C "${{runner.temp}}/bin" -zf - --wildcards --strip-components=1 -x '*/sccache'

        declare -a extra_launch_args=(
          # Write debug logs to a file we can upload
          --env "SCCACHE_SERVER_LOG=sccache=debug"
          --env "SCCACHE_ERROR_LOG=/home/coder/cccl/sccache.log"
          # Cache in a separate S3 bucket prefix
          --env "SCCACHE_S3_KEY_PREFIX=cccl-test-sccache-dist"
          # Mount in new sccache binary
          --volume "${{runner.temp}}/bin/sccache:/usr/bin/sccache:ro"
        )

        OS="$(uname -s)"
        CPUS="$(nproc --all)"
        ARCH="$(dpkg --print-architecture)"

        # Use the build cluster
        if test -n "${DIST_TOKEN+x}"; then

          # Configure sccache client to talk to the build cluster
          cat <<EOF >> "${cfg_dir}/sccache/config"
        [dist]
        # Infinitely retry all retryable dist-compilation errors
        max_retries = inf
        # Never fallback to building locally, fail instead
        fallback_to_local_compile = false
        # Build cluster URL
        scheduler_url = "https://${ARCH}.${OS,,}.sccache.rapids.nvidia.com"

        # Build cluster auth
        [dist.auth]
        type = "token"
        token = "$DIST_TOKEN"

        # Build cluster network config
        [dist.net]
        connect_timeout = 30
        request_timeout = 4000

        [dist.net.keepalive]
        enabled = true
        interval = 20
        timeout = 600
        EOF

          if grep -q '"./ci/build_' <<< "$COMMAND"; then
            extra_launch_args+=(
              # Repopulate the cache
              # --env "SCCACHE_RECACHE=1"
              # Do not cache build products
              # --env "SCCACHE_NO_CACHE=1"
            )
          fi

          # Over-subscribe -j to keep the build cluster busy if _not_ ClangCUDA.
          # ClangCUDA can use the build cluster for C++ files, but _not_ CUDA,
          # and we'll OOM if we try to compile too many at once.
          if ! grep -q '\-cuda "clang' <<< "$COMMAND"; then
            if ! grep -q '_python.sh' <<< "$COMMAND"; then
              extra_launch_args+=(
                --env "PARALLEL_LEVEL=$((CPUS < 32 ? 32 : CPUS))"
                --ulimit "nofile=$(ulimit -Hn):$(ulimit -Hn)"
              )
            else
              extra_launch_args+=(
                --env "PARALLEL_LEVEL=${CPUS}"
              )
            fi
          fi

          if ! grep -q '11.1' <<< "${{inputs.cuda}}"; then
            # Compile device objects in parallel
            extra_launch_args+=(
              --env "NVCC_APPEND_FLAGS=-t=100"
            )
          fi
        fi

        # Explicitly pass which GPU to use if on a GPU runner
        if [[ "${RUNNER}" = *"-gpu-"* ]]; then
          extra_launch_args+=(--gpus "device=${NVIDIA_VISIBLE_DEVICES}")
        fi

        # If the image contains "cudaXX.Yext"...
        if [[ "${IMAGE}" =~ cuda[0-9.]+ext ]]; then
          extra_launch_args+=(--cuda-ext)
        fi

        # Launch this container using the host's docker daemon
        set -x

        ${{github.workspace}}/.devcontainer/launch.sh \
          --docker \
          --cuda ${{inputs.cuda}} \
          --host ${{inputs.host}} \
          "${extra_launch_args[@]}" \
          --env "CI=$CI" \
          --env "AWS_ROLE_ARN=" \
          --env "COMMAND=$COMMAND" \
          --env "SCCACHE_IDLE_TIMEOUT=0" \
          --env "GITHUB_ENV=$GITHUB_ENV" \
          --env "GITHUB_SHA=$GITHUB_SHA" \
          --env "GITHUB_PATH=$GITHUB_PATH" \
          --env "GITHUB_OUTPUT=$GITHUB_OUTPUT" \
          --env "GITHUB_ACTIONS=$GITHUB_ACTIONS" \
          --env "GITHUB_REF_NAME=$GITHUB_REF_NAME" \
          --env "GITHUB_WORKSPACE=$GITHUB_WORKSPACE" \
          --env "GITHUB_REPOSITORY=$GITHUB_REPOSITORY" \
          --env "GITHUB_STEP_SUMMARY=$GITHUB_STEP_SUMMARY" \
          --env "HOST_WORKSPACE=${{github.workspace}}" \
          --env "NVIDIA_VISIBLE_DEVICES=$NVIDIA_VISIBLE_DEVICES" \
          --volume "${{github.workspace}}/ci.sh:/ci.sh" \
          -- /ci.sh

    - if: ${{ always() }}
      name: Create job artifact dir
      shell: bash --noprofile --norc -euo pipefail {0}
      run: |
        result_dir="jobs/${{inputs.id}}"
        mkdir -p "$result_dir"
        echo "result_dir=$result_dir" >> "$GITHUB_ENV"

    - if: ${{ success() }}
      name: Record job success
      shell: bash --noprofile --norc -euo pipefail {0}
      run: |
        touch "$result_dir/success"

    - if: ${{ always() }}
      name: Prepare job artifacts
      shell: bash --noprofile --norc -euo pipefail {0}
      run: |
        echo "Prepare job artifacts"

        # chmod all temp contents 777 so the runner can delete them
        find "${{runner.temp}}/" -exec chmod 0777 {} \;

        # Finds a matching file in the repo directory and copies it to the results directory.
        find_and_copy() {
          find . -maxdepth 4 -type f -name "$1" -print0 | xargs -0 -I% cp -v % "$result_dir/"
        }

        # Copy any artifacts we want to preserve out of the container
        find_and_copy "sccache.log" || :
        find_and_copy ".ninja_log" || :
        find_and_copy "build.ninja" || :
        find_and_copy "rules.ninja" || :
        find_and_copy "sccache_stats.json" || :

        echo "::group::Job artifacts"
        tree "$result_dir"
        echo "::endgroup::"

    - if: ${{ always() }}
      name: Upload job artifacts
      uses: actions/upload-artifact@v4
      with:
        name: jobs-${{inputs.id}}
        path: jobs
        compression-level: 0
        include-hidden-files: true

    - if: ${{ env.is_python == 'true' }}
      name: Upload wheelhouse
      uses: actions/upload-artifact@v4
      with:
        name: wheelhouse-${{inputs.id}}
        path: wheelhouse/
        compression-level: 0
