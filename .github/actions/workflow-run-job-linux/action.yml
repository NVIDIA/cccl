name: "Run Linux Job"
description: "Run a job on a Linux runner."

inputs:
  id:
    description: "A unique identifier."
    required: true
  command:
    description: "The command to run."
    required: true
  image:
    description: "The Docker image to use."
    required: true
  runner:
    description: "The GHA runs-on value."
    required: true
  cuda:
    description: "The CUDA version to use when selecting a devcontainer."
    required: true
  host:
    description: "The host compiler to use when selecting a devcontainer."
    required: true
  # This token must have the "read:enterprise" scope
  dist-token:
    description: "The token used to authenticate with the sccache-dist build cluster."

runs:
  using: "composite"
  steps:
    - name: Define and log job details
      shell: bash --noprofile --norc -euo pipefail {0}
      env:
        # Dereferencing the command from an env var instead of a GHA input avoids issues with escaping
        # semicolons and other special characters (e.g. `-arch "60;70;80"`).
        COMMAND: "${{inputs.command}}"
        GH_TOKEN: ${{ github.token }}
      run: |
        echo -e "\e[1;34mMock with: ci/util/create_mock_job_env.sh $GITHUB_RUN_ID ${{inputs.id}}\e[0m"

        # Use the job def info to setup the Github cache key:
        job_hash=$(${GITHUB_WORKSPACE}/ci/util/workflow/get_stable_job_hash.sh ${{ inputs.id }} | cut -c1-8)
        job_project=$(${GITHUB_WORKSPACE}/ci/util/workflow/get_job_project.sh ${{ inputs.id }})

        echo "::group::Ô∏èüîç Job Inputs"
        echo "Job command: ${COMMAND}" # Intentionally not passing to GITUB_ENV, arg handling is fragile.

        # Define the job input parameters as JOB_* environment variables that are visible in all steps:
        echo "JOB_ID=${{inputs.id}}"               | tee -a $GITHUB_ENV
        echo "JOB_RUNNER=${{inputs.runner}}"       | tee -a $GITHUB_ENV
        echo "JOB_IMAGE=${{inputs.image}}"         | tee -a $GITHUB_ENV
        echo "JOB_CUDA=${{inputs.cuda}}"           | tee -a $GITHUB_ENV
        echo "JOB_HOST=${{inputs.host}}"           | tee -a $GITHUB_ENV

        # Used to locate an appropriate existing cache:
        echo "TOOLCHAIN_SLUG=${{inputs.cuda}}-${{inputs.host}}" | tee -a "${GITHUB_ENV}"
        echo "PROJECT_SLUG=$job_project" | tee -a "${GITHUB_ENV}"
        echo "JOB_SLUG=$job_hash" | tee -a "${GITHUB_ENV}"

        # The new cache won't be uploaded unless the cache key is unique:
        echo "UNIQUE_SLUG=${{github.run_id}}-${{github.run_attempt}}-$RANDOM" | tee -a "${GITHUB_ENV}"
        echo "::endgroup::"

    - name: Setup sccache preprocessor cache
      id: sccache-preprocessor-cache
      uses: actions/cache@v4
      with:
        path: .cache/sccache/preprocessor
        key: sccache-preprocessor-cache-${{runner.os}}-${{env.TOOLCHAIN_SLUG}}-${{env.PROJECT_SLUG}}-${{env.JOB_SLUG}}-${{env.UNIQUE_SLUG}}
        restore-keys: |
          sccache-preprocessor-cache-${{runner.os}}-${{env.TOOLCHAIN_SLUG}}-${{env.PROJECT_SLUG}}-${{env.JOB_SLUG}}
          sccache-preprocessor-cache-${{runner.os}}-${{env.TOOLCHAIN_SLUG}}-${{env.PROJECT_SLUG}}
    - name: Setup sccache-dist client toolchains cache
      id: sccache-dist-toolchains-cache
      uses: actions/cache@v4
      with:
        path: .cache/sccache-dist-client
        key: sccache-toolchains-cache--${{runner.os}}-${{env.TOOLCHAIN_SLUG}}-${{env.PROJECT_SLUG}}-${{env.JOB_SLUG}}-${{env.UNIQUE_SLUG}}
        restore-keys: |
          sccache-toolchains-cache-${{runner.os}}-${{env.TOOLCHAIN_SLUG}}-${{env.PROJECT_SLUG}}-${{env.JOB_SLUG}}
          sccache-toolchains-cache-${{runner.os}}-${{env.TOOLCHAIN_SLUG}}-${{env.PROJECT_SLUG}}

    - name: Add NVCC problem matcher
      continue-on-error: true
      shell: bash --noprofile --norc -euo pipefail {0}
      run: |
        echo "::add-matcher::${{github.workspace}}/.github/problem-matchers/problem-matcher.json"

    - name: Get AWS credentials for sccache bucket
      if: ${{ github.repository == 'NVIDIA/cccl' }}
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: arn:aws:iam::279114543810:role/gha-oidc-NVIDIA
        aws-region: us-east-2
        role-duration-seconds: 43200 # 12 hours

    - name: Print CI override matrix job def
      env:
        GH_TOKEN: ${{ github.token }}
      continue-on-error: true
      shell: bash --noprofile --norc -euo pipefail {0}
      run: |
        # Get the origin matrix job definition embedded in thw workflow artifact:
        matrix_job=$(ci/util/workflow/get_job_def.sh | jq -c '.origin.matrix_job')

        # Delete the cxx_family and cudacxx_family fields
        matrix_job=$(echo "$matrix_job" | jq 'del(.cxx_family, .cudacxx_family, .job_name)')

        # Convert to a single line of YAML, with unquoted keys:
        matrix_job=$(
          echo "- $matrix_job" |
            yq -o=yaml |
            sed -E 's/"([[:alnum:]_]+)"([[:space:]]*):/\1\2:/g' |
            tr '"' "'"
        )

        echo -e "\e[1;34mOverride matrix entry:\e[0m"
        echo -e "\e[1;34m    $matrix_job\e[0m"

    - name: Run command # Do not change this step's name, it is checked in parse-job-times.py
      id: run
      shell: bash --noprofile --norc -euo pipefail {0}
      env:
        GH_TOKEN: ${{ github.token }}
        CI: true
        # Dereferencing the command from an env var instead of a GHA input avoids issues with escaping
        # semicolons and other special characters (e.g. `-arch "60;70;80"`).
        COMMAND: "${{inputs.command}}"
        AWS_ACCESS_KEY_ID: "${{env.AWS_ACCESS_KEY_ID}}"
        AWS_SESSION_TOKEN: "${{env.AWS_SESSION_TOKEN}}"
        AWS_SECRET_ACCESS_KEY: "${{env.AWS_SECRET_ACCESS_KEY}}"
        # Build cluster auth
        SCCACHE_DIST_TOKEN: "${{inputs.dist-token}}"
        SCCACHE_DIST_AUTH_TOKEN_VAR: "SCCACHE_DIST_TOKEN"
        # Retry intermittent failures
        SCCACHE_DIST_MAX_RETRIES: "inf"
        # Never fallback to building locally, fail instead
        SCCACHE_DIST_FALLBACK_TO_LOCAL_COMPILE: "false"
        SCCACHE_IDLE_TIMEOUT: 0
      run: |
        cat <<'EOF' > ci.sh
        #! /usr/bin/env bash

        set -euo pipefail
        ulimit -n $(ulimit -Hn)

        # Need this for git to trust the docker-mounted workspace:
        git config --global --add safe.directory "$(pwd)"

        echo -e "\e[1;34mRunning as '$(whoami)' user in $(pwd):\e[0m"
        echo -e "\e[1;34mMock with: ci/util/create_mock_job_env.sh $GITHUB_RUN_ID $JOB_ID\e[0m"
        echo -e "\e[1;34m${COMMAND}\e[0m"
        exit_code=0
        eval "${COMMAND}" || exit_code=$?
        if [[ "$exit_code" -ne 0 ]]; then
          echo -e "::group::Ô∏è‚ùó \e[1;31mInstructions to Reproduce CI Failure Locally\e[0m"
          echo "::error:: To replicate this failure locally, follow the steps below:"
          echo "1. Clone the repository, and navigate to the correct branch and commit:"
          echo "   git clone --branch $GITHUB_REF_NAME --single-branch https://github.com/$GITHUB_REPOSITORY.git && cd $(echo $GITHUB_REPOSITORY | cut -d'/' -f2) && git checkout $GITHUB_SHA"
          echo ""
          echo "2. Run the failed command inside the same Docker container used by this CI job:"
          echo "   .devcontainer/launch.sh -d -c ${{inputs.cuda}} -H ${{inputs.host}} -- ${COMMAND}"
          echo ""
          echo "For additional information, see:"
          echo "   - DevContainer Documentation: https://github.com/NVIDIA/cccl/blob/main/.devcontainer/README.md"
          echo "   - Continuous Integration (CI) Overview: https://github.com/NVIDIA/cccl/blob/main/ci-overview.md"
          echo -e "::endgroup::"
        fi

        ci/upload_job_result_artifacts.sh "${{inputs.id}}" $exit_code

        exit $exit_code
        EOF

        chmod +x ci.sh

        # The devcontainer will mount this path to the home directory:
        readonly aws_dir="${{github.workspace}}/.aws"
        readonly cfg_dir="${{github.workspace}}/.config"
        mkdir -p "${aws_dir}" "${cfg_dir}/sccache";

        cat <<EOF > "${aws_dir}/config"
        [default]
        bucket=rapids-sccache-devs
        region=us-east-2
        EOF

        cat <<EOF > "${aws_dir}/credentials"
        [default]
        aws_access_key_id=$AWS_ACCESS_KEY_ID
        aws_session_token=$AWS_SESSION_TOKEN
        aws_secret_access_key=$AWS_SECRET_ACCESS_KEY
        EOF

        chmod 0600 "${aws_dir}/credentials"
        chmod 0664 "${aws_dir}/config"

        declare -a extra_launch_args=()

        # if grep -q '"./ci/test_cub.sh' <<< "$COMMAND"; then
        #   extra_launch_args+=(
        #     # Disable sccache for CUB tests
        #     --env "DISABLE_SCCACHE=1"
        #   )
        # fi

        # if grep -q '"./ci/build_' <<< "$COMMAND"; then
        #   extra_launch_args+=(
        #     # Repopulate the cache
        #     --env "SCCACHE_RECACHE=1"
        #     # Do not cache build products
        #     # --env "SCCACHE_NO_CACHE=1"
        #   )
        # fi

        # Over-subscribe -j to keep the build cluster busy if _not_ ClangCUDA.
        # ClangCUDA can use the build cluster for C++ files, but _not_ CUDA,
        # and we'll OOM if we try to compile too many at once.
        if ! grep -q '\-cuda "clang' <<< "$COMMAND"; then
          if ! grep -q '_python.sh' <<< "$COMMAND"; then
            extra_launch_args+=(
              --env "PARALLEL_LEVEL=$(ulimit -Hn)"
            )
          else
            extra_launch_args+=(
              --env "PARALLEL_LEVEL=$(nproc --all)"
            )
          fi
        fi

        # Clear the ARN when using the pre-minted aws creds on the main repo.
        if [[ "$GITHUB_REPOSITORY" == "NVIDIA/cccl" ]]; then
          extra_launch_args+=(--env "AWS_ROLE_ARN=")
        fi

        # Explicitly pass which GPU to use if on a GPU runner
        if [[ "${JOB_RUNNER}" = *"-gpu-"* ]]; then
          extra_launch_args+=(--gpus "device=${NVIDIA_VISIBLE_DEVICES}")
        fi

        # If the image contains "cudaXX.Yext"...
        if [[ "${JOB_IMAGE}" =~ cuda[0-9.]+ext ]]; then
          extra_launch_args+=(--cuda-ext)
        fi

        # Initialize artifact paths, vars, etc shared with the devcontainer:
        source ci/util/artifacts/common.sh
        source ci/util/workflow/common.sh

        exit_code=0

        # Launch this container using the host's docker daemon
        ( # Subshell to contain the set -x log
          set -x
          .devcontainer/launch.sh \
            --docker \
            --cuda $JOB_CUDA \
            --host $JOB_HOST \
            "${extra_launch_args[@]}" \
            --env "CI=true" \
            --env "COMMAND=$COMMAND" \
            --env "GH_TOKEN=$GH_TOKEN" \
            --env "GITHUB_ACTIONS=$GITHUB_ACTIONS" \
            --env "GITHUB_REF_NAME=$GITHUB_REF_NAME" \
            --env "GITHUB_RUN_ID=$GITHUB_RUN_ID" \
            --env "GITHUB_SHA=$GITHUB_SHA" \
            --env "GITHUB_REPOSITORY=$GITHUB_REPOSITORY" \
            --env "HOST_WORKSPACE=${{github.workspace}}" \
            --env "JOB_ID=$JOB_ID" \
            --env "NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-}" \
            --env "RUNNER_TEMP=$RUNNER_TEMP" \
            --volume "${ARTIFACT_ARCHIVES}:${ARTIFACT_ARCHIVES}" \
            --volume "${ARTIFACT_UPLOAD_STAGE}:${ARTIFACT_UPLOAD_STAGE}" \
            --volume "${WORKFLOW_DIR}:${WORKFLOW_DIR}" \
            -- ./ci.sh
        ) || exit_code=$?

        # Dump artifact matrix for upload step:
        printf "ARTIFACTS=%s\n" "$(ci/util/artifacts/upload/print_matrix.sh)" >> "${GITHUB_OUTPUT}"

        exit $exit_code

    - name: Upload job artifacts
      if: ${{ always() && steps.run.outputs.ARTIFACTS != '' && fromJson(steps.run.outputs.ARTIFACTS)[0] != null }}
      uses: ./.github/actions/upload-artifacts
      with:
        artifacts: ${{ steps.run.outputs.ARTIFACTS }}
