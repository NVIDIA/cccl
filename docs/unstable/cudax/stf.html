

<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>CUDASTF &#8212; CUDA Core Compute Libraries</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/nvidia-sphinx-theme.css?v=933278ad" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />



    <script src="../_static/documentation_options.js?v=bbe6ed3a"></script>
    <script src="../_static/doctools.js?v=fd6eb6e6"></script>
    <script src="../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=5ceeb459"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'cudax/stf';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://NVIDIA.github.io/cccl/nv-versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'unstable';
        DOCUMENTATION_OPTIONS.show_version_warning_banner =
            false;
        </script>

    <link rel="canonical" href="https://NVIDIA.github.io/cccl/cudax/stf.html" />
    <link rel="icon" href="../_static/favicon.png"/>

    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Implementation of the matrix class" href="stf/custom_data_interface.html" />
    <link rel="prev" title="cuda::experimental::stf::graphed_interface_of&lt; void_interface &gt;" href="api/structcuda_1_1experimental_1_1stf_1_1graphed__interface__of_3_01void__interface_01_4.html" />


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="unstable" />


  </head>

  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>


  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="CUDA Core Compute Libraries - Home"/>
    <img src="../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="CUDA Core Compute Libraries - Home"/>
  
  
    <p class="title logo__title">CUDA Core Compute Libraries</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">


<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/NVIDIA/cccl" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        



  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="CUDA Core Compute Libraries - Home"/>
    <img src="../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="CUDA Core Compute Libraries - Home"/>
  
  
    <p class="title logo__title">CUDA Core Compute Libraries</p>
  
</a>


  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">


<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/NVIDIA/cccl" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">



<nav class="bd-docs-nav bd-links"
     aria-label="Table of Contents">
  <p class="bd-links__title" role="heading" aria-level="1">Table of Contents</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../cpp.html">CUDA C++ Core Libraries</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../libcudacxx/index.html">libcu++</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../libcudacxx/index.html">Overview</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../libcudacxx/setup.html">Setup</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/setup/requirements.html">Requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/setup/getting.html">Getting libcu++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/setup/building_and_testing.html">Building &amp; Testing libcu++</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../libcudacxx/releases.html">Releases</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/releases/changelog.html">Changelog</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/releases/versioning.html">Versioning</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../libcudacxx/standard_api.html">Standard API</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/standard_api/algorithms_library.html">Algorithms Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/standard_api/c_library.html">C Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/standard_api/concepts_library.html">Concepts Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/standard_api/container_library.html">Container Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/standard_api/execution_library.html">Execution Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/standard_api/numerics_library.html">Numerics Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/standard_api/ranges_library.html">Ranges Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/standard_api/synchronization_library.html">Synchronization Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/standard_api/time_library.html">Time Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/standard_api/type_support.html">Type Support Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/standard_api/utility_library.html">Utility Library</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../libcudacxx/extended_api.html">Extended API</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/bit.html">Bit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/execution_model.html">Execution model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/exceptions.html">Exception Handling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/memory_model.html">Memory model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/thread_groups.html">Thread Groups</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/synchronization_primitives.html">Synchronization Primitives</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/asynchronous_operations.html">Asynchronous Operations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/memory_access_properties.html">Memory access properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/functional.html">Functional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/iterators.html">Fancy Iterators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/type_traits.html">Type traits</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/vector_tuple_protocol.html">Vector Tuple Protocol</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/numeric.html">Numeric</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/random.html">Random</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/memory.html">Memory</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/memory_resource.html">Memory Resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/math.html">Math</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/mdspan.html">Mdspan</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/tma.html">Tensor Memory Accelerator (TMA)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/warp.html">Warp</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/utility.html">Utility</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/extended_api/work_stealing.html">Work stealing</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../libcudacxx/runtime.html">Runtime</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/runtime/cudart_interactions.html">CUDA Runtime interactions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/runtime/stream.html">Streams</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/runtime/event.html">Events</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/runtime/algorithm.html">Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/runtime/device.html">Devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/runtime/hierarchy.html">Hierarchy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/runtime/launch.html">Launch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/runtime/buffer.html">Buffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/runtime/memory_pools.html">Memory Pools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/runtime/legacy_resources.html">Legacy resources</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../libcudacxx/ptx_api.html">PTX API</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/ptx/examples.html">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/ptx/instructions.html">PTX Instructions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/ptx/pragmas.html">PTX Pragmas</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="../libcudacxx/api/index.html">API reference</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../cub/index.html">CUB</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../cub/index.html">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cub/test_overview.html">CUB Tests</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cub/benchmarking.html">CUB Benchmarks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cub/tuning.html">CUB Tunings</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../cub/developer_overview.html">CUB Developer Overview</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../cub/developer/thread_level.html">Thread-level</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cub/developer/warp_level.html">Warp-level</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cub/developer/block_scope.html">Block-scope</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cub/developer/device_scope.html">Device-scope</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cub/developer/nvtx.html">NVTX</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../cub/releases.html">CUB Releases</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../cub/releases/changelog.html">CUB 2.1.0</a></li>


















































</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../cub/api.html">API documentation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../cub/api_docs/thread_level.html">Thread-level Primitives</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cub/api_docs/warp_wide.html">Warp-Wide “Collective” Primitives</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cub/api_docs/block_wide.html">Block-Wide “Collective” Primitives</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cub/api_docs/device_wide.html">Device-Wide Primitives</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="../cub/api/index.html">API reference</a></li>
</ul>
</details></li>










<li class="toctree-l2 has-children"><a class="reference internal" href="../thrust/index.html">Thrust</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../thrust/index.html">Overview</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../thrust/developer_overview.html">Thrust Developer Overview</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../thrust/developer/cmake_options.html">Developer CMake Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="../thrust/developer/systems.html">Thrust systems</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../thrust/releases.html">Releases</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../thrust/releases/changelog.html">Changelog</a></li>
<li class="toctree-l4"><a class="reference internal" href="../thrust/releases/versioning.html">Versioning</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="../thrust/release_process.html">Release Process</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../thrust/api.html">API documentation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../thrust/api_docs/algorithms.html">Algorithms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../thrust/api_docs/containers.html">Containers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../thrust/api_docs/function_objects.html">Function Objects</a></li>
<li class="toctree-l4"><a class="reference internal" href="../thrust/api_docs/iterators.html">Iterators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../thrust/api_docs/memory_management.html">Memory Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../thrust/api_docs/numerics.html">Numerics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../thrust/api_docs/parallel_execution_policies.html">Parallel Execution Policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../thrust/api_docs/random.html">Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../thrust/api_docs/system.html">System</a></li>
<li class="toctree-l4"><a class="reference internal" href="../thrust/api_docs/utility.html">Utility</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="../thrust/api/index.html">API reference</a></li>
</ul>
</details></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="index.html">CUDA Experimental</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="index.html">Overview</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="container.html">Containers library</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="api/classcuda_1_1experimental_1_1uninitialized__buffer.html">cuda::experimental::uninitialized_buffer</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="memory_resource.html">Memory Resources</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/api/classcuda_1_1mr_1_1basic__any__resource.html">cuda::mr::basic_any_resource</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/api/structcuda_1_1memory__pool__properties.html">cuda::memory_pool_properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/api/structcuda_1_1device__memory__pool.html">cuda::device_memory_pool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/api/structcuda_1_1pinned__memory__pool.html">cuda::pinned_memory_pool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/api/structcuda_1_1managed__memory__pool.html">cuda::managed_memory_pool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/api/classcuda_1_1mr_1_1legacy__pinned__memory__resource.html">cuda::mr::legacy_pinned_memory_resource</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/api/classcuda_1_1mr_1_1legacy__managed__memory__resource.html">cuda::mr::legacy_managed_memory_resource</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libcudacxx/api/structcuda_1_1mr_1_1shared__resource.html">cuda::mr::shared_resource</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="graph.html">Graphs library</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="api/structcuda_1_1experimental_1_1graph.html">cuda::experimental::graph</a></li>
<li class="toctree-l4"><a class="reference internal" href="api/structcuda_1_1experimental_1_1graph__builder.html">cuda::experimental::graph_builder</a></li>
<li class="toctree-l4"><a class="reference internal" href="api/structcuda_1_1experimental_1_1graph__builder__ref.html">cuda::experimental::graph_builder_ref</a></li>
<li class="toctree-l4"><a class="reference internal" href="api/structcuda_1_1experimental_1_1graph__node__ref.html">cuda::experimental::graph_node_ref</a></li>
<li class="toctree-l4"><a class="reference internal" href="api/structcuda_1_1experimental_1_1stf_1_1graphed__interface__of.html">cuda::experimental::stf::graphed_interface_of</a></li>
<li class="toctree-l4"><a class="reference internal" href="api/structcuda_1_1experimental_1_1stf_1_1graphed__interface__of_3_01mdspan_3_01T_00_01P_8_8_8_01_4_01_4.html">cuda::experimental::stf::graphed_interface_of&lt; mdspan&lt; T, P… &gt; &gt;</a></li>
<li class="toctree-l4"><a class="reference internal" href="api/structcuda_1_1experimental_1_1stf_1_1graphed__interface__of_3_01scalar__view_3_01T_01_4_01_4.html">cuda::experimental::stf::graphed_interface_of&lt; scalar_view&lt; T &gt; &gt;</a></li>
<li class="toctree-l4"><a class="reference internal" href="api/structcuda_1_1experimental_1_1stf_1_1graphed__interface__of_3_01void__interface_01_4.html">cuda::experimental::stf::graphed_interface_of&lt; void_interface &gt;</a></li>
</ul>
</details></li>
<li class="toctree-l3 current active has-children"><a class="current reference internal" href="#">CUDASTF</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="stf/custom_data_interface.html">Implementation of the <code class="docutils literal notranslate"><span class="pre">matrix</span></code> class</a></li>





<li class="toctree-l4"><a class="reference internal" href="stf/lower_level_api.html">Lower-level API</a></li>

</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="api/index.html">API reference</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../cccl/tma.html">Tensor Memory Accelerator (TMA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cccl/3.0_migration_guide.html">CCCL 2.x ‐ CCCL 3.0 migration guide</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../cccl/development/index.html">CCCL Development Guide</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../cccl/development/macro.html">CCCL Internal Macros</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cccl/development/testing.html">CCCL Testing Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cccl/development/build_and_bisect_tools.html">Build and Bisect Utilities</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../cccl/development/visibility.html">Symbol Visibility</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../cccl/development/visibility/host_stub_visibility.html">Host Stub Visibility Issue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cccl/development/visibility/device_kernel_visibility.html">Device Kernel Visibility Issue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cccl/development/visibility/different_architectures.html">Linking TUs compiled with different architectures</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../cccl/contributing.html">Contributing to the CUDA Core Compute Libraries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../cccl/contributing/code_of_conduct.html">Code of Conduct</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../cccl/license.html">License</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../python/index.html">CCCL Python Libraries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../python/setup.html">Setup and Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python/compute.html"><code class="docutils literal notranslate"><span class="pre">cuda.compute</span></code>: Parallel Computing Primitives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python/coop.html"><code class="docutils literal notranslate"><span class="pre">cuda.coop</span></code>: Cooperative Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python/resources.html">Resources</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../python/api_reference.html">API Reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../python/compute_api.html"><code class="docutils literal notranslate"><span class="pre">cuda.compute</span></code> API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../python/coop_api.html"><code class="docutils literal notranslate"><span class="pre">cuda.coop</span></code> API Reference</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../cpp.html" class="nav-link">CUDA C++ Core Libraries</a></li>
    
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">CUDA Experimental</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">CUDASTF</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="cudastf">
<span id="stf"></span><h1><a class="toc-backref" href="#id15" role="doc-backlink">CUDASTF</a><a class="headerlink" href="#cudastf" title="Link to this heading">#</a></h1>
<nav class="contents" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#cudastf" id="id15">CUDASTF</a></p>
<ul>
<li><p><a class="reference internal" href="#the-sequential-task-flow-stf-programming-model" id="id16">The Sequential Task Flow (STF) programming model</a></p></li>
<li><p><a class="reference internal" href="#getting-started-with-cudastf" id="id17">Getting started with CUDASTF</a></p></li>
<li><p><a class="reference internal" href="#backends-and-contexts" id="id18">Backends and contexts</a></p></li>
<li><p><a class="reference internal" href="#logical-data" id="id19">Logical data</a></p></li>
<li><p><a class="reference internal" href="#tasks" id="id20">Tasks</a></p></li>
<li><p><a class="reference internal" href="#synchronization" id="id21">Synchronization</a></p></li>
<li><p><a class="reference internal" href="#places" id="id22">Places</a></p></li>
<li><p><a class="reference internal" href="#parallel-for-construct" id="id23"><code class="docutils literal notranslate"><span class="pre">parallel_for</span></code> construct</a></p></li>
<li><p><a class="reference internal" href="#launch-construct" id="id24"><code class="docutils literal notranslate"><span class="pre">launch</span></code> construct</a></p></li>
<li><p><a class="reference internal" href="#cuda-kernel-construct" id="id25"><code class="docutils literal notranslate"><span class="pre">cuda_kernel</span></code> construct</a></p></li>
<li><p><a class="reference internal" href="#cuda-kernel-chain-construct" id="id26"><code class="docutils literal notranslate"><span class="pre">cuda_kernel_chain</span></code> construct</a></p></li>
<li><p><a class="reference internal" href="#c-types-of-logical-data-and-tasks" id="id27">C++ Types of logical data and tasks</a></p></li>
<li><p><a class="reference internal" href="#modular-use-of-cudastf" id="id28">Modular use of CUDASTF</a></p></li>
<li><p><a class="reference internal" href="#debugging" id="id29">Debugging</a></p></li>
<li><p><a class="reference internal" href="#tools" id="id30">Tools</a></p></li>
<li><p><a class="reference internal" href="#cudastf-reference-card" id="id31">CUDASTF Reference Card</a></p></li>
</ul>
</li>
</ul>
</nav>
<p>CUDASTF is an implementation of the Sequential Task Flow model for CUDA.</p>
<p>The availability of parallelism within modern hardware has dramatically
increased, with large nodes now featuring multiple accelerators. As a
result, maximizing concurrency at the application level in a scalable
manner has become a crucial priority. To effectively hide latencies, it
is essential to achieve the highest level of asynchrony possible.</p>
<p>CUDASTF introduces a tasking model that automates data transfers while
enforcing implicit data-driven dependencies.</p>
<p>Implemented as a header-only C++ library, CUDASTF builds on top of CUDA
APIs to simplify the development of multi-GPU applications.</p>
<p>CUDASTF enables the creation of highly concurrent parallel applications,
leveraging both the CUDA stream API and the CUDA graph API for efficient
task orchestration and data management. The same client code can run
optimally on single- and multi-GPU systems.</p>
<section id="the-sequential-task-flow-stf-programming-model">
<h2><a class="toc-backref" href="#id16" role="doc-backlink">The Sequential Task Flow (STF) programming model</a><a class="headerlink" href="#the-sequential-task-flow-stf-programming-model" title="Link to this heading">#</a></h2>
<p>The CUDASTF programming model involves defining logical data and
submitting tasks that operate on this data. CUDASTF automatically
deduces the dependencies between different tasks and orchestrates both
computation and data movement to ensure efficient execution with as much
concurrency as possible.</p>
<p>CUDASTF employs the <a class="reference external" href="https://inria.hal.science/hal-01618526">Sequential Task
Flow</a> (STF) programming
model, which enables the extraction of concurrency from a sequence of
tasks annotated with appropriate data accesses and their respective
modes (read-only, write-only, or read/write).</p>
<p>For instance, two tasks modifying the same data will be serialized (in
order to maintain read-after-write and write-after-write coherency),
whereas two tasks reading the same data without modification can be
executed concurrently (read-after-read is coherent in any order). A task
must wait until all preceding modifications have been completed before
reading a piece of data (read-after-write). Similarly, a task that needs
to modify data can only do so once all preceding reads have finished
(write-after-read).</p>
<p>Applying these simple rules to a complex algorithm (initially expressed
serially as a sequence of tasks) results in a directed acyclic graph
(DAG) of tasks, which enables CUDASTF to devise concurrent execution for
the given algorithm.</p>
<p>By providing data use annotations to CUDASTF, programmers benefit from
both automated parallelization and transparent data management. Through
a specialized cache coherency protocol, CUDASTF automates data
allocation and transfers. As a result, programmers can focus on
developing efficient task-based algorithms instead of grappling with
asynchrony and asynchronous data management.</p>
<p>To illustrate how a sequence of tasks can be transformed into a parallel
application using annotated data accesses, consider the following
example involving three logical data pieces denoted as <code class="docutils literal notranslate"><span class="pre">X</span></code>, <code class="docutils literal notranslate"><span class="pre">Y</span></code>,
and <code class="docutils literal notranslate"><span class="pre">Z</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">T1</span><span class="p">[</span><span class="n">X</span><span class="p">(</span><span class="n">rw</span><span class="p">)],</span> <span class="n">T2</span><span class="p">[</span><span class="n">X</span><span class="p">(</span><span class="n">read</span><span class="p">),</span> <span class="n">Y</span><span class="p">(</span><span class="n">rw</span><span class="p">)],</span> <span class="n">T3</span><span class="p">[</span><span class="n">X</span><span class="p">(</span><span class="n">read</span><span class="p">),</span> <span class="n">Z</span><span class="p">(</span><span class="n">rw</span><span class="p">)],</span> <span class="n">T4</span><span class="p">[</span><span class="n">Y</span><span class="p">(</span><span class="n">read</span><span class="p">),</span> <span class="n">Z</span><span class="p">(</span><span class="n">rw</span><span class="p">)]</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">T2</span></code> and <code class="docutils literal notranslate"><span class="pre">T3</span></code> read <code class="docutils literal notranslate"><span class="pre">X</span></code>, which is modified by <code class="docutils literal notranslate"><span class="pre">T1</span></code>, creating a
read-after-write dependency between <code class="docutils literal notranslate"><span class="pre">T1</span></code> and <code class="docutils literal notranslate"><span class="pre">T2</span></code>, as well as
between <code class="docutils literal notranslate"><span class="pre">T1</span></code> and <code class="docutils literal notranslate"><span class="pre">T3</span></code>. Since <code class="docutils literal notranslate"><span class="pre">T2</span></code> and <code class="docutils literal notranslate"><span class="pre">T3</span></code> only perform
concurrent read accesses, they can execute concurrently. <code class="docutils literal notranslate"><span class="pre">T4</span></code> reads
<code class="docutils literal notranslate"><span class="pre">Y</span></code> and <code class="docutils literal notranslate"><span class="pre">Z</span></code>, which were modified by <code class="docutils literal notranslate"><span class="pre">T2</span></code> and <code class="docutils literal notranslate"><span class="pre">T3</span></code>, respectively,
resulting in write-after-read dependencies between <code class="docutils literal notranslate"><span class="pre">T2</span></code> and <code class="docutils literal notranslate"><span class="pre">T4</span></code>,
and between <code class="docutils literal notranslate"><span class="pre">T3</span></code> and <code class="docutils literal notranslate"><span class="pre">T4</span></code>. The resulting dependency graph is shown
below.</p>
<img alt="../_images/graph_01.png" src="../_images/graph_01.png" />
</section>
<section id="getting-started-with-cudastf">
<h2><a class="toc-backref" href="#id17" role="doc-backlink">Getting started with CUDASTF</a><a class="headerlink" href="#getting-started-with-cudastf" title="Link to this heading">#</a></h2>
<section id="getting-cudastf">
<h3>Getting CUDASTF<a class="headerlink" href="#getting-cudastf" title="Link to this heading">#</a></h3>
<p>CUDASTF is part of the CUDA Experimental library of the CCCL project. It is not distributed with the CUDA Toolkit like the rest of CCCL. It is only available on the <a class="reference external" href="https://github.com/NVIDIA/cccl">CCCL GitHub repository</a>.</p>
</section>
<section id="using-cudastf">
<h3>Using CUDASTF<a class="headerlink" href="#using-cudastf" title="Link to this heading">#</a></h3>
<p>CUDASTF is a header-only C++ library which only require to include its
main header. CUDASTF API is part of the <code class="docutils literal notranslate"><span class="pre">cuda::experimental::stf</span></code> C++
namespace, and we will assume for brevity that we are using this
workspace in the rest of this document.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cuda/experimental/stf.cuh&gt;</span>

<span class="k">using</span><span class="w"> </span><span class="n">cuda</span><span class="o">::</span><span class="n">experimental</span><span class="o">::</span><span class="n">stf</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="compiling">
<h3>Compiling<a class="headerlink" href="#compiling" title="Link to this heading">#</a></h3>
<p>CUDASTF requires a compiler conforming to the C++17 standard or later.
Although there is no need to link against CUDASTF itself, the library
internally utilizes the CUDA library.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compilation flags</span>
nvcc<span class="w"> </span>-std<span class="o">=</span>c++17<span class="w"> </span>--expt-relaxed-constexpr<span class="w"> </span>--extended-lambda<span class="w"> </span>-I<span class="k">$(</span>cudastf_path<span class="k">)</span>
<span class="c1"># Linking flags</span>
nvcc<span class="w"> </span>-lcuda
</pre></div>
</div>
<p>It is also possible to use CUDASTF without <code class="docutils literal notranslate"><span class="pre">nvcc</span></code>. This is for example
useful when calling existing CUDA libraries such as CUBLAS which do not
require authoring custom kernels. Note that CUDASTF APIs intended to
automatically generate CUDA kernels such as <code class="docutils literal notranslate"><span class="pre">parallel_for</span></code> or
<code class="docutils literal notranslate"><span class="pre">launch</span></code> are disabled when compiling without nvcc.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compilation flags</span>
g++<span class="w"> </span>-I<span class="k">$(</span>cudastf_path<span class="k">)</span>
<span class="c1"># Linking flags</span>
g++<span class="w"> </span>-lcuda<span class="w"> </span>-lcudart
</pre></div>
</div>
</section>
<section id="using-cudastf-within-a-cmake-project">
<h3>Using CUDASTF within a CMake project<a class="headerlink" href="#using-cudastf-within-a-cmake-project" title="Link to this heading">#</a></h3>
<p>As part of the CCCL project, CUDASTF uses CMake for its build and installation
infrastructure, and is the recommended way of building applications that use
CUDASTF.</p>
<p>This is facilitated by the CMake Package Manager as illustrated in this simple example which is available <a class="reference external" href="https://github.com/NVIDIA/cccl/tree/main/examples/cudax_stf">here</a>, and which is described in the next paragraph.</p>
</section>
<section id="a-simple-example">
<h3>A simple example<a class="headerlink" href="#a-simple-example" title="Link to this heading">#</a></h3>
<p>The following example illustrates the use of CUDASTF to implement the
well-known AXPY kernel, which computes <code class="docutils literal notranslate"><span class="pre">Y</span> <span class="pre">=</span> <span class="pre">Y</span> <span class="pre">+</span> <span class="pre">alpha</span> <span class="pre">*</span> <span class="pre">X</span></code> where <code class="docutils literal notranslate"><span class="pre">X</span></code>
and <code class="docutils literal notranslate"><span class="pre">Y</span></code> are two vectors, and <code class="docutils literal notranslate"><span class="pre">alpha</span></code> is a scalar_view value.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cuda/experimental/stf.cuh&gt;</span>

<span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">cuda</span><span class="o">::</span><span class="nn">experimental</span><span class="o">::</span><span class="nn">stf</span><span class="p">;</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">axpy</span><span class="p">(</span><span class="n">T</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">slice</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">slice</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">tid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">nthreads</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tid</span><span class="p">;</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">nthreads</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">y</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">(</span><span class="n">ind</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="n">main</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="o">**</span><span class="w"> </span><span class="n">argv</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">context</span><span class="w"> </span><span class="n">ctx</span><span class="p">;</span>

<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">16</span><span class="p">;</span>
<span class="w">    </span><span class="kt">double</span><span class="w"> </span><span class="n">X</span><span class="p">[</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="n">Y</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">ind</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">X</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sin</span><span class="p">((</span><span class="kt">double</span><span class="p">)</span><span class="n">ind</span><span class="p">);</span>
<span class="w">        </span><span class="n">Y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">col</span><span class="p">((</span><span class="kt">double</span><span class="p">)</span><span class="n">ind</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">lY</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">Y</span><span class="p">);</span>

<span class="w">    </span><span class="kt">double</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">3.14</span><span class="p">;</span>

<span class="w">    </span><span class="cm">/* Compute Y = Y + alpha X */</span>
<span class="w">    </span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sX</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sY</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">axpy</span><span class="o">&lt;&lt;&lt;</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">sX</span><span class="p">,</span><span class="w"> </span><span class="n">sY</span><span class="p">);</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="n">ctx</span><span class="p">.</span><span class="n">finalize</span><span class="p">();</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The code is organized into several steps, which will be described in
more detail in the following sections:</p>
<ol class="arabic simple">
<li><p>include CUDASTF headers</p></li>
<li><p>declare a CUDASTF context</p></li>
<li><p>create logical data</p></li>
<li><p>submit and wait for the completion of pending work</p></li>
</ol>
<p>More examples can be found in the <code class="docutils literal notranslate"><span class="pre">examples</span></code> directory in the sources.</p>
</section>
<section id="compiling-examples">
<h3>Compiling examples<a class="headerlink" href="#compiling-examples" title="Link to this heading">#</a></h3>
<p>The following commands compile STF examples from the root of the CCCL sources.
Please note the <code class="docutils literal notranslate"><span class="pre">-j</span></code> option, which specifies how many processes should be used to
compile the examples. Not specifying it will launch as many processes as there
are processors on the machine, which might lead to an excessive resource
consumption and system instability.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>build
<span class="nb">cd</span><span class="w"> </span>build
cmake<span class="w"> </span>..<span class="w"> </span>--preset<span class="w"> </span>cudax
<span class="nb">cd</span><span class="w"> </span>cudax
ninja<span class="w"> </span>cudax.examples.stf<span class="w"> </span>-j4
</pre></div>
</div>
<p>To launch examples, simply run binaries under the <cite>bin/</cite>
subdirectory in the current directory. For instance, to launch the <cite>01-axpy</cite>
example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./bin/cudax.cpp17.example.stf.01-axpy
</pre></div>
</div>
</section>
</section>
<section id="backends-and-contexts">
<h2><a class="toc-backref" href="#id18" role="doc-backlink">Backends and contexts</a><a class="headerlink" href="#backends-and-contexts" title="Link to this heading">#</a></h2>
<p>The code snippet below includes the required CUDASTF header. It then
creates a context object, which is an entry point for every API calls,
and which stores the state of the CUDASTF library and to keep track of
all resources and all dependencies. This context must eventually be
destroyed using the <code class="docutils literal notranslate"><span class="pre">finalize()</span></code> method.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">context</span><span class="w"> </span><span class="n">ctx</span><span class="p">;</span>
</pre></div>
</div>
<p>There are currently three context backends available in CUDASTF, with a
common API but possibly different implementations, and a few specific
extensions. The <code class="docutils literal notranslate"><span class="pre">context</span></code> class, which is a generic context
implementation should be preferred to write generic code. Using a
specific context type might reduce compilation time, but provide less
flexibility.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">stream_ctx</span></code> class defines a context type that relies on CUDA
streams and CUDA events to implement synchronizations. Tasks are
launched eagerly. This is the context type used by default in the
generic <code class="docutils literal notranslate"><span class="pre">context</span></code> type.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">graph_ctx</span></code> class is a context type that implements task
parallelism by the means of CUDA graphs. Tasks (and all related
operations) are put into CUDA graphs. Note that the lambda function
attached describing a task is captured immediately (during the
<code class="docutils literal notranslate"><span class="pre">ctx.task</span></code> API call) even if the execution is deferred. The underlying
CUDA graph is launched when a synchronization with the host is needed,
or when the context is finalized. Other circumstances such as task
fences might flush all pending operations and result into a graph
launch. Subsequent operations would be put in a new CUDA graph.
Selecting this backend is an easy way to adopt CUDA graphs, and can be
beneficial in terms of performance with a repeated task patterns. Unlike
other context types, it is not allowed for a task to synchronize with
the CUDA stream (e.g. with <code class="docutils literal notranslate"><span class="pre">cudaStreamSynchronize</span></code>) within a task.</p>
<p>Using either <code class="docutils literal notranslate"><span class="pre">context</span></code>, <code class="docutils literal notranslate"><span class="pre">stream_ctx</span></code>, or <code class="docutils literal notranslate"><span class="pre">graph_ctx</span></code> should result
in the same behaviour, even if the underlying implementation differs.
One may switch from a type to another one by adapting how we initialize
the context object, or by selecting an appropriate type to decide
statically:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// assigns a graph_ctx() to a generic context</span>
<span class="n">context</span><span class="w"> </span><span class="n">ctx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">graph_ctx</span><span class="p">();</span>

<span class="c1">// statically select a context based on CUDA streams and CUDA events</span>
<span class="n">stream_ctx</span><span class="w"> </span><span class="n">ctx</span><span class="p">;</span>

<span class="c1">// statically select a context based on CUDA streams and CUDA events</span>
<span class="n">graph_ctx</span><span class="w"> </span><span class="n">ctx</span><span class="p">;</span>
</pre></div>
</div>
<p>For the most part, these types can be used interchangeably. The key
difference is that <code class="docutils literal notranslate"><span class="pre">stream_ctx</span></code> and <code class="docutils literal notranslate"><span class="pre">graph_ctx</span></code> are statically bound to
use either the CUDA stream or graph APIs, while <code class="docutils literal notranslate"><span class="pre">context</span></code> defers this
decision to runtime, allowing dynamic selection of the appropriate backend.
This flexibility does not introduce significant runtime overhead or
compilation time differences, but it may be necessary when the user needs
to select the context type dynamically (see Algorithms).</p>
<section id="tasks-in-the-stream-backend">
<h3>Tasks in the Stream backend<a class="headerlink" href="#tasks-in-the-stream-backend" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">stream_ctx</span></code> backend utilizes CUDA streams and events to provide
synchronization. Each <code class="docutils literal notranslate"><span class="pre">stream_task</span></code> in the <code class="docutils literal notranslate"><span class="pre">stream_ctx</span></code> backend
represents a task that is associated with an input CUDA stream.
Asynchronous work can be submitted in the body of the task using this
input stream. Once the <code class="docutils literal notranslate"><span class="pre">stream_task</span></code> completes, all work submitted
within the task’s body is assumed to be synchronized with the associated
stream.</p>
<p>Users can query the stream associated to a <code class="docutils literal notranslate"><span class="pre">stream_task</span></code> using its
<code class="docutils literal notranslate"><span class="pre">get_stream()</span></code> method.</p>
</section>
<section id="tasks-in-the-graph-backend">
<h3>Tasks in the Graph backend<a class="headerlink" href="#tasks-in-the-graph-backend" title="Link to this heading">#</a></h3>
<p>In the <code class="docutils literal notranslate"><span class="pre">graph_ctx</span></code> environment, a CUDA graph is either created
internally or passed in by the user during construction. If the user
supplies the CUDA graph, CUDASTF can automatically insert CUDA graph
nodes to enable subsequent tasks to be submitted as child graphs of the
user-supplied graph.</p>
<p>Creating a <code class="docutils literal notranslate"><span class="pre">graph_task</span></code> object results in creating a child graph in the
aforementioned graph associated to the <code class="docutils literal notranslate"><span class="pre">graph_ctx</span></code> object. The child
graph implements the body of the task, and CUDASTF automatically inserts
the appropriate dependencies to ensure this child graph is executed only
after all of its dependencies are fulfilled. CUDASTF may also add other
nodes in the supporting CUDA graph, such as those needed for data
transfers or data allocations.</p>
<p>Users can retrieve the graph associated to a <code class="docutils literal notranslate"><span class="pre">graph_task</span></code> by using its
<code class="docutils literal notranslate"><span class="pre">get_graph()</span></code> method.</p>
</section>
</section>
<section id="logical-data">
<h2><a class="toc-backref" href="#id19" role="doc-backlink">Logical data</a><a class="headerlink" href="#logical-data" title="Link to this heading">#</a></h2>
<p>In traditional computing, “data”, such as a matrix describing a neural
network layer, typically refers to a location in memory with a defined
address. However, in mixed CPU/GPU systems, the same conceptual data may
exist simultaneously in multiple locations and have multiple addresses
(typically the CPU-tied RAM plus one or more copies in the
high-bandwidth memory used by GPUs). CUDASTF refers to such conceptual
data as <em>logical data</em>, an abstract handle for data that may get
transparently transferred to or replicated over the different places
used by CUDASTF tasks. When user code creates a logical data object from
a user-provided object (e.g. an array of <code class="docutils literal notranslate"><span class="pre">double</span></code>), they transfer the
ownership of the original data to CUDASTF. As a result, any access to
the original data should be performed through the logical data
interface, as CUDASTF may transfer the logical data to a CUDA device
where it can be modified, rendering the original data invalid. By doing
this, user code is relieved of all memory allocation chores and of
keeping track of which physical location holds the correct data at
different stages of computation.</p>
<p>A logical data is created by calling the <code class="docutils literal notranslate"><span class="pre">ctx.logical_data</span></code> member
function. The resulting object will be used to specify data accesses
within tasks.</p>
<p>In the following example, a stack array <code class="docutils literal notranslate"><span class="pre">X</span></code> is used to define a new
logical data object <code class="docutils literal notranslate"><span class="pre">lX</span></code>, which should be subsequently used instead of
<code class="docutils literal notranslate"><span class="pre">X</span></code>:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">X</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
</pre></div>
</div>
<p>Each logical data object internally maintains various <em>data instances</em>,
which are replicas of the logical data at different <em>data places</em>. For
instance, there could be an instance in host memory, as well as
instances in the embedded memory of CUDA device 0 and CUDA device 1.
CUDASTF ensures that tasks have access to <em>valid</em> data instances where
they execute and may dynamically create new instances or destroy
existing ones.</p>
<p>In the example above, <code class="docutils literal notranslate"><span class="pre">X</span></code> is initially on the host (on the CPU stack).
If a task is subsequently launched on device <code class="docutils literal notranslate"><span class="pre">0</span></code> that modifies data
through <code class="docutils literal notranslate"><span class="pre">lX</span></code>, a new data instance will be created in memory associated
with device <code class="docutils literal notranslate"><span class="pre">0</span></code>. In addition making that allocation, CUDASTF ensures
that a data transfer is issued asynchronously from the host to the
device, so that the task is given a <em>valid</em> replica of <code class="docutils literal notranslate"><span class="pre">X</span></code>. Given that
the task modifies data through <code class="docutils literal notranslate"><span class="pre">lX</span></code>, the instance associated to the
host will also be invalidated, so CUDASTF will later copy data back to
the host if another task needs to access <code class="docutils literal notranslate"><span class="pre">X</span></code> from the CPU.</p>
<section id="data-interfaces">
<h3>Data interfaces<a class="headerlink" href="#data-interfaces" title="Link to this heading">#</a></h3>
<p>CUDASTF implements a generic interface to manipulate different types of
data formats across the machine.</p>
<p>Every type of data format is described using three separate types :</p>
<ul class="simple">
<li><p>its shape, which stores parameters which will be common to all instance.  For
a fixed-sized vector, the shape would for example contain the length of the
vector.</p></li>
<li><p>a per-instance type that describes a specific data instance. For a
fixed-sized vector, this type would for example contain the address of the
vector.</p></li>
<li><p>a data interface class which implements operations such as allocating a data
instance based on its shape, or copying an instance into another instance.</p></li>
</ul>
</section>
<section id="defining-custom-data-interfaces-advanced">
<h3>Defining custom data interfaces (advanced)<a class="headerlink" href="#defining-custom-data-interfaces-advanced" title="Link to this heading">#</a></h3>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="stf/custom_data_interface.html">Implementation of the <code class="docutils literal notranslate"><span class="pre">matrix</span></code> class</a></li>
<li class="toctree-l1"><a class="reference internal" href="stf/custom_data_interface.html#defining-the-shape-of-a-matrix">Defining the shape of a matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="stf/custom_data_interface.html#hash-of-a-matrix">Hash of a matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="stf/custom_data_interface.html#defining-a-data-interface">Defining a data interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="stf/custom_data_interface.html#associating-a-data-interface-with-the-cuda-stream-backend">Associating a data interface with the CUDA stream backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="stf/custom_data_interface.html#example-of-code-using-the-matrix-data-interface">Example of code using the <code class="docutils literal notranslate"><span class="pre">matrix</span></code> data interface</a></li>
</ul>
</div>
<p>CUDASTF API is designed to be extensible, so that advanced users may
define their own data interfaces. This can be useful when manipulating
data formats which are not regular multidimensional arrays, or to
provide a direct access to a domain-specific or an application-specific
data format.</p>
<p>A complete example is given <a class="reference internal" href="stf/custom_data_interface.html#stf-custom-data-interface"><span class="std std-ref">here</span></a> to
illustrate how to implement a custom data interface.</p>
</section>
<section id="write-back-policy">
<h3>Write-back policy<a class="headerlink" href="#write-back-policy" title="Link to this heading">#</a></h3>
<p>When a logical data object is destroyed, the original data instance is
updated (unless the logical data was created without a reference value,
e.g. from a shape). The result is only guaranteed to be available on the
corresponding data place when after the <code class="docutils literal notranslate"><span class="pre">finalize()</span></code> method was called
on the context. Likewise, when calling <code class="docutils literal notranslate"><span class="pre">finalize()</span></code> a write-back
mechanism is automatically issued on all logical data associated to the
context if they were not already destroyed.</p>
<p>Write back is enabled by default, but it is possible to disable it for a
specific logical data by calling this method on a logical data :
<code class="docutils literal notranslate"><span class="pre">set_write_back(bool</span> <span class="pre">flag)</span></code>. Enabling write-back on a logical data
which was defined from a shape and has no reference data instance will
result in an error.</p>
</section>
<section id="slices">
<h3>Slices<a class="headerlink" href="#slices" title="Link to this heading">#</a></h3>
<p>To facilitate the use of potentially non-contiguous multi-dimensional
arrays, we have introduced a C++ data structure class called <code class="docutils literal notranslate"><span class="pre">slice</span></code>.
A slice is a partial specialization of C++’s
<code class="docutils literal notranslate"><span class="pre">std::mdspan</span></code> (or <code class="docutils literal notranslate"><span class="pre">std::experimental::mdspan</span></code> depending on the C++ revision).</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="o">&gt;</span>
<span class="k">using</span><span class="w"> </span><span class="n">slice</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mdspan</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span><span class="w"> </span><span class="n">dextents</span><span class="o">&lt;</span><span class="kt">size_t</span><span class="p">,</span><span class="w"> </span><span class="n">dimensions</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="n">layout_stride</span><span class="o">&gt;</span><span class="p">;</span>
</pre></div>
</div>
<p>When creating a <code class="docutils literal notranslate"><span class="pre">logical_data</span></code> from a C++ array, CUDASTF automatically
describes it as a slice instantiated with the scalar_view element type and
the dimensionality of the array. Here is an example with an 1D array of
<code class="docutils literal notranslate"><span class="pre">double</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="mi">128</span><span class="p">];</span>
<span class="n">context</span><span class="w"> </span><span class="n">ctx</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lA</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">A</span><span class="p">);</span>
</pre></div>
</div>
<p>Internally, all instances of <code class="docutils literal notranslate"><span class="pre">A</span></code> are described as <code class="docutils literal notranslate"><span class="pre">slice&lt;double,</span> <span class="pre">1&gt;</span></code>
where <code class="docutils literal notranslate"><span class="pre">double</span></code> is the scalar_view element type, and <code class="docutils literal notranslate"><span class="pre">1</span></code> is the
dimensionality of the array. The default dimension corresponds to <code class="docutils literal notranslate"><span class="pre">1</span></code>,
so <code class="docutils literal notranslate"><span class="pre">slice&lt;double&gt;</span></code> is equivalent with <code class="docutils literal notranslate"><span class="pre">slice&lt;double,</span> <span class="pre">1&gt;</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">mdspan</span></code> facility provides a <a class="reference external" href="https://en.cppreference.com/w/cpp/container/mdspan">variety of
methods</a> also
available to its alias <code class="docutils literal notranslate"><span class="pre">slice</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">*data_handle()</span></code> gives the address of the first element</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">operator()</span></code> so that <code class="docutils literal notranslate"><span class="pre">A(i)</span></code> is the <code class="docutils literal notranslate"><span class="pre">i</span></code>-th element of a slice of
dimension <code class="docutils literal notranslate"><span class="pre">1</span></code>, and <code class="docutils literal notranslate"><span class="pre">A(i,</span> <span class="pre">j)</span></code> is the element at coordinates
<code class="docutils literal notranslate"><span class="pre">(i,</span> <span class="pre">j)</span></code> in a 2D slice.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">size_t</span> <span class="pre">size()</span></code> returns the total number of elements in the slice</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">size_t</span> <span class="pre">extent(size_t</span> <span class="pre">dim)</span></code> returns the size of a slice in a given
dimension (run-time version)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">size_t</span> <span class="pre">stride(size_t</span> <span class="pre">dim)</span></code> returns the distance in memory between
two elements in a given dimension, expressed as a number of elements
(run-time version)</p></li>
</ul>
<p>Slices can be passed by value, copied, or moved. Copying a slice does
not copy the underlying data. Slices can be passed as arguments to CUDA
kernel. Example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">axpy</span><span class="p">(</span><span class="n">T</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">slice</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">slice</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">tid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">nthreads</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tid</span><span class="p">;</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">nthreads</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">y</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">(</span><span class="n">ind</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<section id="defining-multidimensional-slices">
<h4>Defining multidimensional slices<a class="headerlink" href="#defining-multidimensional-slices" title="Link to this heading">#</a></h4>
<p>Slices can be used on data with multiple dimensions, and possibly
non-contiguous data.</p>
<p>For example, to define a 2D slice, we can use the <code class="docutils literal notranslate"><span class="pre">make_slice</span></code> method
which takes a base pointer, a tuple with all dimensions, and then the
<em>strides</em>. The number of stride values is equal to the number of
dimensions minus one. The i-th stride defines the number of elements in
memory between two successive elements along dimension i.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="mi">5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">2</span><span class="p">];</span>

<span class="c1">// contiguous 2D slice</span>
<span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="o">&gt;</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_slice</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="p">},</span><span class="w"> </span><span class="mi">5</span><span class="p">);</span>

<span class="c1">// non-contiguous 2D slice</span>
<span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="o">&gt;</span><span class="w"> </span><span class="n">s2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_slice</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="p">},</span><span class="w"> </span><span class="mi">5</span><span class="p">);</span>
</pre></div>
</div>
<p>In the second example, <code class="docutils literal notranslate"><span class="pre">s2</span></code> defines a non-contiguous 2D slice because
the stride is greater than the extent in the first dimension. We will
here <em>skip</em> an element between between <code class="docutils literal notranslate"><span class="pre">s2(3,</span> <span class="pre">0)</span></code> (which is <code class="docutils literal notranslate"><span class="pre">A[3]</span></code>)
and <code class="docutils literal notranslate"><span class="pre">s2(0,</span> <span class="pre">1)</span></code> (which is <code class="docutils literal notranslate"><span class="pre">A[5]</span></code>)</p>
<p>Similarly with 3D data, we need to define 2 strides and 3 extent values
:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="mi">5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">40</span><span class="p">];</span>

<span class="c1">// contiguous 3D slice</span>
<span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="o">&gt;</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_slice</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">40</span><span class="w"> </span><span class="p">},</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">3</span><span class="p">);</span>

<span class="c1">// non-contiguous 3D slice</span>
<span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="o">&gt;</span><span class="w"> </span><span class="n">s2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_slice</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">40</span><span class="w"> </span><span class="p">},</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">3</span><span class="p">);</span>

<span class="c1">// non-contiguous 3D slice</span>
<span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="o">&gt;</span><span class="w"> </span><span class="n">s3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_slice</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">40</span><span class="w"> </span><span class="p">},</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">3</span><span class="p">);</span>
</pre></div>
</div>
<p>Such slices can also be used to create logical data :</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="mi">32</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">32</span><span class="p">];</span>

<span class="c1">// Contiguous 2D slice</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">make_slice</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="w"> </span><span class="p">},</span><span class="w"> </span><span class="mi">32</span><span class="p">));</span>

<span class="c1">// Non-contiguous 2D slice</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lX2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">make_slice</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="mi">24</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="w"> </span><span class="p">},</span><span class="w"> </span><span class="mi">32</span><span class="p">));</span>
</pre></div>
</div>
</section>
</section>
<section id="defining-logical-data-from-a-shape">
<h3>Defining logical data from a shape<a class="headerlink" href="#defining-logical-data-from-a-shape" title="Link to this heading">#</a></h3>
<p>Data interfaces supports data which are only described as a shape. For
example, a user may want to define a vector of 10 integers, and later
fill it with a task. In this case, there is no need to have a <em>reference
instance</em> associated to that logical data because CUDASTF will
automatically allocate an instance on its first usage.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">shape_of</span><span class="o">&lt;</span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="mi">10</span><span class="p">));</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">write</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">X</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">cudaMemsetAsync</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">data_handle</span><span class="p">(),</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">X</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>
<span class="p">};</span>
</pre></div>
</div>
<p>In this example, <code class="docutils literal notranslate"><span class="pre">lX</span></code> is defined using a shape only, and there is no
physical backing needed to create it. Note that since there exists no
valid <em>data instance</em> of <code class="docutils literal notranslate"><span class="pre">lX</span></code>, the first task needs to make a
write-only access (using the <code class="docutils literal notranslate"><span class="pre">write()</span></code> member of <code class="docutils literal notranslate"><span class="pre">lX</span></code>). A write-only
access will indeed allocate <code class="docutils literal notranslate"><span class="pre">lX</span></code> at the appropriate location, but it
will not try to load a valid copy of it prior to executing the task.</p>
<p>Using other access modes such as <code class="docutils literal notranslate"><span class="pre">read()</span></code>, <code class="docutils literal notranslate"><span class="pre">relaxed()</span></code> or <code class="docutils literal notranslate"><span class="pre">rw()</span></code>
that attempt to provide a valid instance will result in an error.  The
<code class="docutils literal notranslate"><span class="pre">reduce()</span></code> access mode can be used only if the reduction is not accumulating
its result with an existing value, so we can for example use
<code class="docutils literal notranslate"><span class="pre">reduce(reducer::sum&lt;double&gt;{})</span></code> but not <code class="docutils literal notranslate"><span class="pre">reduce(reducer::sum&lt;double&gt;{},</span> <span class="pre">no_init{})</span></code>
on a logical data which has valid data instance.</p>
<p>Similarly, it is possible to define a logical data from a slice shapes
with multiple dimensions.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">lX_2D</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">shape_of</span><span class="o">&lt;</span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">24</span><span class="p">));</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lX_3D</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">shape_of</span><span class="o">&lt;</span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">24</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">));</span>
</pre></div>
</div>
</section>
</section>
<section id="tasks">
<h2><a class="toc-backref" href="#id20" role="doc-backlink">Tasks</a><a class="headerlink" href="#tasks" title="Link to this heading">#</a></h2>
<p>A task is created by calling the <code class="docutils literal notranslate"><span class="pre">ctx.task</span></code> member function. It takes
an optional argument that specifies the execution location of the task.
If none is provided, the current CUDA device will be used, which is
equivalent to passing <code class="docutils literal notranslate"><span class="pre">exec_place::current_device()</span></code>. Data accesses
are specified using a list of data dependencies. Each dependency is
described by calling the <code class="docutils literal notranslate"><span class="pre">read()</span></code>, <code class="docutils literal notranslate"><span class="pre">rw()</span></code>, <code class="docutils literal notranslate"><span class="pre">write()</span></code> or <code class="docutils literal notranslate"><span class="pre">reduce()</span></code>
method of the logical data object.</p>
<p>In the example below, <code class="docutils literal notranslate"><span class="pre">X</span></code> is accessed in read-only mode and <code class="docutils literal notranslate"><span class="pre">Y</span></code>
needs to be updated so it uses a read-write access mode.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">axpy</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">tid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">nthreads</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tid</span><span class="p">;</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">nthreads</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
<span class="p">...</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">slice</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">sX</span><span class="p">,</span><span class="w"> </span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">sY</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">axpy</span><span class="o">&lt;&lt;&lt;</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">sX</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span><span class="w"> </span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">sX</span><span class="p">.</span><span class="n">data_handle</span><span class="p">(),</span><span class="w"> </span><span class="n">sY</span><span class="p">.</span><span class="n">data_handle</span><span class="p">());</span>
<span class="p">};</span>
</pre></div>
</div>
<p>The object returned by the call <code class="docutils literal notranslate"><span class="pre">ctx.task()</span></code> overloads
<code class="docutils literal notranslate"><span class="pre">operator-&gt;*()</span></code> to accept a lambda function on the right-hand side.
This makes it easy for user code to pass the task’s body to the context
with a syntax akin to a control flow statement. The first argument of
the lambda function is a <code class="docutils literal notranslate"><span class="pre">cudaStream_t</span></code> that can be used to submit
work asynchronously on the selected device within the body of the task.
For each logical data, CUDASTF passes a <em>data instance</em> to the lambda
function. These <em>data instances</em> provide access to a local copy of the
logical data, which is coherent with respect to the CUDA stream passed
to the task.</p>
<p>For example, data instances associated to 1D arrays of <code class="docutils literal notranslate"><span class="pre">double</span></code> are
typed as <code class="docutils literal notranslate"><span class="pre">slice&lt;double&gt;</span></code> if the data is in write or read-write mode,
and <code class="docutils literal notranslate"><span class="pre">slice&lt;const</span> <span class="pre">double&gt;</span></code> if the data is in read-only mode. The
<code class="docutils literal notranslate"><span class="pre">.data_handle()</span></code> method of this type returns the base address of the
underlying array, and the <code class="docutils literal notranslate"><span class="pre">.size()</span></code> method returns the total number of
elements. For multi-dimensional arrays, <code class="docutils literal notranslate"><span class="pre">.extent(d)</span></code> returns the size
along dimension <code class="docutils literal notranslate"><span class="pre">d</span></code>. (For a 1D array, <code class="docutils literal notranslate"><span class="pre">.size()</span></code> is therefore
equivalent to <code class="docutils literal notranslate"><span class="pre">.extent(0)</span></code>.)</p>
<p>Better yet, the CUDA kernel could manipulate slices directly instead of
resorting to unsafe pointers as parameters:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">axpy</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">slice</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">tid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">nthreads</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tid</span><span class="p">;</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">nthreads</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">y</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">(</span><span class="n">ind</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
<span class="p">...</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">slice</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">sX</span><span class="p">,</span><span class="w"> </span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">sY</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">axpy</span><span class="o">&lt;&lt;&lt;</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">sX</span><span class="p">,</span><span class="w"> </span><span class="n">sY</span><span class="p">);</span>
<span class="p">};</span>
</pre></div>
</div>
<p>Task submission can be further simplified to rely on type deduction with
<code class="docutils literal notranslate"><span class="pre">auto</span></code>, which also makes code more generic:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sX</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sY</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">axpy</span><span class="o">&lt;&lt;&lt;</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">sX</span><span class="p">,</span><span class="w"> </span><span class="n">sY</span><span class="p">);</span>
<span class="p">};</span>
</pre></div>
</div>
<p><em>It is important to note that the body of the task construct is executed
directly at the submission of the task, and not when the task is
actually ready for execution. As a result, the body of the task here
submits a CUDA kernel in the stream, but it is not the CUDA kernel
itself.</em> For example, attempting to use slices <code class="docutils literal notranslate"><span class="pre">sX</span></code> and <code class="docutils literal notranslate"><span class="pre">sY</span></code> in the
example above immediately in the lambda function would be incorrect; the
right way is to pass them to a kernel synchronized with the stream
<code class="docutils literal notranslate"><span class="pre">s</span></code>. CUDA execution semantics will ensure that by the time the kernel
runs, <code class="docutils literal notranslate"><span class="pre">sX</span></code> and <code class="docutils literal notranslate"><span class="pre">sY</span></code> will be valid.</p>
<section id="example-of-creating-and-using-multiple-tasks">
<h3>Example of creating and using multiple tasks<a class="headerlink" href="#example-of-creating-and-using-multiple-tasks" title="Link to this heading">#</a></h3>
<p>Often, complex algorithms involve multiple processing stages, each with
its own inputs and outputs. In CUDASTF it suffices to express computing
stages in a sequential manner along with their data dependencies.
CUDASTF will ensure optimal parallel execution without requiring the
user code to explicitly define a dependency graph. Consider the
following example consisting of four tasks, of which three run on GPUs:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lY</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">Y</span><span class="p">);</span>

<span class="c1">// Task 1</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">read</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sX</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sY</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">K1</span><span class="o">&lt;&lt;&lt;</span><span class="p">...,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">sX</span><span class="p">,</span><span class="w"> </span><span class="n">sY</span><span class="p">);</span>
<span class="w">    </span><span class="n">K2</span><span class="o">&lt;&lt;&lt;</span><span class="p">...,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">sX</span><span class="p">,</span><span class="w"> </span><span class="n">sY</span><span class="p">);</span><span class="o">:</span>
<span class="p">};</span>

<span class="c1">// Task 2</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sX</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">K3</span><span class="o">&lt;&lt;&lt;</span><span class="p">...,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">sX</span><span class="p">);</span>
<span class="p">};</span>

<span class="c1">// Task 3</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sY</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">K4</span><span class="o">&lt;&lt;&lt;</span><span class="p">...,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">sY</span><span class="p">);</span>
<span class="p">};</span>

<span class="c1">// Task 4</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">host_launch</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">read</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="k">auto</span><span class="w"> </span><span class="n">sX</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sY</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">callback</span><span class="p">(</span><span class="n">sX</span><span class="p">,</span><span class="w"> </span><span class="n">sY</span><span class="p">);</span>
<span class="p">};</span>
</pre></div>
</div>
<p>Tasks <code class="docutils literal notranslate"><span class="pre">T2</span></code> and <code class="docutils literal notranslate"><span class="pre">T3</span></code> depend on <code class="docutils literal notranslate"><span class="pre">T1</span></code> because they respectively
modify <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">Y</span></code>, which were accessed in read-only mode by <code class="docutils literal notranslate"><span class="pre">T1</span></code>.
Task <code class="docutils literal notranslate"><span class="pre">T4</span></code>, executed on the host, reads both <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">Y</span></code>, and
therefore needs to wait for the completion of <code class="docutils literal notranslate"><span class="pre">T2</span></code> and <code class="docutils literal notranslate"><span class="pre">T3</span></code>. Note
that Task <code class="docutils literal notranslate"><span class="pre">T1</span></code> submits multiple CUDA kernels in the same CUDA stream.
This illustrates how a task in CUDASTF encapsulates a piece of work that
is asynchronous with respect to CUDA stream semantics.</p>
<p>The resulting task graph under the STF programming model is shown below.</p>
<img alt="../_images/task-sequence-user.png" src="../_images/task-sequence-user.png" />
<p>In full detail, the resulting graph of asynchronous operations includes
additional data allocations of <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">Y</span></code> on the current device, as
well as copies to and from the device. These automated steps highlight
how CUDASTF alleviates much of the complexity associated with using
multiple processing units, allowing programmers to focus on algorithmic
matters instead.</p>
<img alt="../_images/task-sequence.png" src="../_images/task-sequence.png" />
</section>
<section id="lower-level-task-api">
<h3>Lower-level task API<a class="headerlink" href="#lower-level-task-api" title="Link to this heading">#</a></h3>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="stf/lower_level_api.html">Lower-level API</a></li>
<li class="toctree-l1"><a class="reference internal" href="stf/lower_level_api.html#compatibility-with-cuda-graphs">Compatibility with CUDA graphs</a></li>
</ul>
</div>
<p>A lower-level API that does not rely on lambda functions is also
available, and is described <a class="reference internal" href="stf/lower_level_api.html"><span class="doc">here</span></a>.</p>
</section>
</section>
<section id="synchronization">
<h2><a class="toc-backref" href="#id21" role="doc-backlink">Synchronization</a><a class="headerlink" href="#synchronization" title="Link to this heading">#</a></h2>
<p>It is important to note that each task body (passed to the context via
<code class="docutils literal notranslate"><span class="pre">operator-&gt;*()</span></code>) is executed immediately and is used to <em>submit work
asynchronously</em> with respect to the synchronization semantic of the CUDA
stream. CUDASTF ensures that any operation enqueued in the stream
attached to the task within task body may access the specified data in a
coherently, with respect to the requested access modes.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">submit</span><span class="p">();</span>
<span class="c1">// Unrelated CPU-based code might go here...</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">finalize</span><span class="p">();</span>
</pre></div>
</div>
<p>Due to the asynchronous nature of task parallelism, it is necessary to
ensure that all operations are properly scheduled and executed. As
CUDASTF transparently handles data management (allocations, transfers,
…), there can be outstanding asynchronous operations that were not
submitted explicitly by the user. Therefore it is not sufficient to use
native CUDA synchronization operations because they are not aware of
CUDASTF’s state. Client code must call <code class="docutils literal notranslate"><span class="pre">ctx.finalize()</span></code> instead of
<code class="docutils literal notranslate"><span class="pre">cudaStreamSynchronize()</span></code> or <code class="docutils literal notranslate"><span class="pre">cudaDeviceSynchronize()</span></code>.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ctx.submit()</span></code> initiates the submission of all asynchronous tasks
within the sequence</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ctx.finalize()</span></code> awaits the conclusion of all outstanding
asynchronous operations in the context, automatically invoking
<code class="docutils literal notranslate"><span class="pre">ctx.submit()</span></code> if not previously called by user code</p></li>
</ul>
<p>Usually, creating the task and invoking <code class="docutils literal notranslate"><span class="pre">ctx.finalize()</span></code> is
sufficient. However, manually calling <code class="docutils literal notranslate"><span class="pre">ctx.submit()</span></code> can be beneficial
in at least two situations. First, it allows for executing additional
unrelated work on the CPU (or another GPU) between submission and
synchronization. Second, when it’s necessary for two contexts to run
concurrently, using the sequence
<code class="docutils literal notranslate"><span class="pre">ctx1.submit();</span> <span class="pre">ctx2.submit();</span> <span class="pre">ctx1.finalize();</span> <span class="pre">ctx2.finalize();</span></code>
achieves this goal (whereas calling
<code class="docutils literal notranslate"><span class="pre">ctx1.finalize();</span> <span class="pre">ctx2.finalize();</span></code> without the <code class="docutils literal notranslate"><span class="pre">submit()</span></code> calls
would wait for the completion of the first task before starting the
second).</p>
<p>To wait for the completion of all pending operations (tasks, transfers, …),
an asynchronous fence mechanism is available :</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">fence</span><span class="p">();</span>
<span class="n">cudaStreamSynchronize</span><span class="p">(</span><span class="n">stream</span><span class="p">);</span>
</pre></div>
</div>
<p>Another synchronization mechanism is the <code class="docutils literal notranslate"><span class="pre">wait</span></code> method of the
context object. It is typically used in combination with the <code class="docutils literal notranslate"><span class="pre">reduce()</span></code>
access mode for dynamic control flow. <code class="docutils literal notranslate"><span class="pre">auto</span> <span class="pre">val</span> <span class="pre">=</span> <span class="pre">ctx.wait(ld)</span></code> is a
blocking call that returns the content of the <code class="docutils literal notranslate"><span class="pre">ld</span></code> logical data. The type of
the returned value is defined by the <code class="docutils literal notranslate"><span class="pre">owning_container_of&lt;interface&gt;</span></code> trait
class where <code class="docutils literal notranslate"><span class="pre">interface</span></code> is the data interface of the logical data. The
<code class="docutils literal notranslate"><span class="pre">wait</span></code> method therefore cannot be called on a logical data with an
interface that does not overload this trait class.</p>
<p>This mechanism is illustrated in the dot product example of the
<a class="reference internal" href="#reduce-access-mode"><span class="std std-ref">Reduce access mode</span></a> section.</p>
</section>
<section id="places">
<h2><a class="toc-backref" href="#id22" role="doc-backlink">Places</a><a class="headerlink" href="#places" title="Link to this heading">#</a></h2>
<p>To assist users with managing data and execution affinity, CUDASTF
provides the notion of <em>place</em>. Places can represent either <em>execution
places</em>, which determine where code is executed, or <em>data places</em>,
specifying the location of data across the machine’s non-uniform memory.
One of CUDASTF’s goals is to ensure efficient data placement in line
with the execution place by default, while also providing users the
option to easily customize placement if necessary. Execution places
allow users to express where computation occurs without directly
engaging with the underlying CUDA APIs or dealing with the complex
synchronization that emerges from combining various execution places
asynchronously.</p>
<section id="execution-places">
<h3>Execution places<a class="headerlink" href="#execution-places" title="Link to this heading">#</a></h3>
<p>A task’s constructor allows choosing an execution place. The example
below creates a logical data variable that describes an integer as a
vector of one <code class="docutils literal notranslate"><span class="pre">int</span></code>. The logical data variable is then updated on
device <code class="docutils literal notranslate"><span class="pre">0</span></code> and on device <code class="docutils literal notranslate"><span class="pre">1</span></code> before being accessed again from the
host.</p>
<p>The first argument passed to <code class="docutils literal notranslate"><span class="pre">ctx.task</span></code> is called an <em>execution place</em>
and tells CUDASTF where the task is expected to execute.
<code class="docutils literal notranslate"><span class="pre">exec_place::device(id)</span></code> means that the task will run on device
<code class="docutils literal notranslate"><span class="pre">id</span></code>, and <code class="docutils literal notranslate"><span class="pre">exec_place::host()</span></code> specifies that the task will execute on
the host.</p>
<p>Regardless of the <em>execution place</em>, it is important to note that the
task’s body (i.e., the contents of the lambda function) corresponds to
CPU code that is expected to launch computation asynchronously. When
using <code class="docutils literal notranslate"><span class="pre">exec_place::device(id)</span></code>, CUDASTF will automatically set the
current CUDA device to <code class="docutils literal notranslate"><span class="pre">id</span></code> when the task is started, and restore the
previous current device when the task ends. <code class="docutils literal notranslate"><span class="pre">exec_place::host()</span></code> does
not affect the current CUDA device.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">context</span><span class="w"> </span><span class="n">ctx</span><span class="p">;</span>

<span class="kt">int</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">42</span><span class="p">;</span>

<span class="k">auto</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="p">}));</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="w"> </span><span class="n">lX</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sX</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">inc_kernel</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">sX</span><span class="p">);</span>
<span class="p">};</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="n">lX</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sX</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">inc_kernel</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">sX</span><span class="p">);</span>
<span class="p">};</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">exec_place</span><span class="o">::</span><span class="n">host</span><span class="p">(),</span><span class="w"> </span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sX</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">cudaStreamSynchronize</span><span class="p">(</span><span class="n">stream</span><span class="p">);</span>
<span class="w">    </span><span class="n">assert</span><span class="p">(</span><span class="n">sX</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">44</span><span class="p">);</span>
<span class="p">};</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">finalize</span><span class="p">();</span>
</pre></div>
</div>
<p>Tasks submitted on the host are also executed immediately upon task
creation and not when dependencies are ready. Asynchronous semantics are
observed in accordance to CUDA serialization on the <code class="docutils literal notranslate"><span class="pre">cudaStream_t</span></code>
lambda parameter. Therefore, the code shown synchronizes explicitly with
the CUDA stream by calling <code class="docutils literal notranslate"><span class="pre">cudaStreamSynchronize(stream)</span></code>. This
ensures the value <code class="docutils literal notranslate"><span class="pre">sX</span></code> is read only after data is guaranteed to be
valid, i.e., after the completion of prior operations in the stream.
This is disallowed in the <code class="docutils literal notranslate"><span class="pre">graph_ctx</span></code> backend.</p>
<p>An alternative solution which is compatible with all types of backend is
to use <code class="docutils literal notranslate"><span class="pre">ctx.host_launch</span></code>:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">host_launch</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="k">auto</span><span class="w"> </span><span class="n">sX</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">assert</span><span class="p">(</span><span class="n">sX</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">44</span><span class="p">);</span>
<span class="p">};</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">ctx.host_launch</span></code> member function circumvents synchronization of
the CPU thread with CUDA execution by invoking the lambda function as a
CUDA callback, thereby maintaining optimal asynchronous semantics for
the entire workload. Since no explicit synchronization with the
underlying CUDA stream is needed, <code class="docutils literal notranslate"><span class="pre">ctx.host_launch</span></code> is thus compatible
with the CUDA graph backend (i.e., a context of type <code class="docutils literal notranslate"><span class="pre">graph_ctx</span></code>).</p>
</section>
<section id="data-places">
<h3>Data places<a class="headerlink" href="#data-places" title="Link to this heading">#</a></h3>
<p>By default, logical data is associated with the device where it is
currently processed. A task launched on a device should therefore have
its data loaded into the global memory of that device, whereas a task
executed on the host would access data in host memory (RAM). These are
defined as the <em>affine</em> data places of an execution place.</p>
<p>In the example below, data places are not specified for the two tasks
created. Consequently, the affine data places will be chosen for the two
tasks: the memory of device <code class="docutils literal notranslate"><span class="pre">0</span></code> for the first task and the host RAM
for the second task.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="w"> </span><span class="n">lA</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">};</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">exec_place</span><span class="o">::</span><span class="n">host</span><span class="p">(),</span><span class="w"> </span><span class="n">lA</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">};</span>
</pre></div>
</div>
<p>The code above is equivalent with:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="w"> </span><span class="n">lA</span><span class="p">.</span><span class="n">rw</span><span class="p">(</span><span class="n">data_place</span><span class="o">::</span><span class="n">affine</span><span class="p">()))</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">};</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="w"> </span><span class="n">lA</span><span class="p">.</span><span class="n">rw</span><span class="p">(</span><span class="n">data_place</span><span class="o">::</span><span class="n">affine</span><span class="p">()))</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">};</span>
</pre></div>
</div>
<p>The affinity can also be made explicit:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="w"> </span><span class="n">lA</span><span class="p">.</span><span class="n">rw</span><span class="p">(</span><span class="n">data_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">};</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="w"> </span><span class="n">lA</span><span class="p">.</span><span class="n">rw</span><span class="p">(</span><span class="n">data_place</span><span class="o">::</span><span class="n">host</span><span class="p">()))</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">};</span>
</pre></div>
</div>
<p>CUDASTF also allows to localize data and execution on different places.
The example below ensures that an instance of logical data <code class="docutils literal notranslate"><span class="pre">A</span></code> located
in host memory is passed to the task so that it can be accessed from
device <code class="docutils literal notranslate"><span class="pre">0</span></code>:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="w"> </span><span class="n">lA</span><span class="p">.</span><span class="n">rw</span><span class="p">(</span><span class="n">data_place</span><span class="o">::</span><span class="n">host</span><span class="p">()))</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">};</span>
</pre></div>
</div>
<p>Overriding affinity can be advantageous when a task is known to make
only sparse accesses to a piece of logical data. By overriding affinity,
transferring large amounts of data is avoided; the paging system of CUDA
<a class="reference external" href="https://developer.nvidia.com/blog/unified-memory-cuda-beginners/">Unified
Memory</a>
will automatically fault in the portions of the data actually used.
Conversely, we can launch a task on the host that accesses data located
on a device:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">exec_place</span><span class="o">::</span><span class="n">host</span><span class="p">(),</span><span class="w"> </span><span class="n">lA</span><span class="p">.</span><span class="n">rw</span><span class="p">(</span><span class="n">data_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">};</span>
</pre></div>
</div>
<p>Alternatively, assuming there are at least two devices available, in
unified memory it is possible to access the memory of one device from
another:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="w"> </span><span class="n">lA</span><span class="p">.</span><span class="n">rw</span><span class="p">(</span><span class="n">data_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">};</span>
</pre></div>
</div>
<p>Non-affine data placement therefore provides flexibility and can be used
to improve performance or to address memory capacity issues when
accessing large data sets. They however assume that the system can
perform such accesses, which may depend on the hardware (NVLINK, UVM, …)
and the OS (WSL has limited support and lower performance when accessing
host memory from CUDA kernels, for example).</p>
</section>
<section id="grid-of-places">
<h3>Grid of places<a class="headerlink" href="#grid-of-places" title="Link to this heading">#</a></h3>
<p>CUDASTF also makes it possible to manipulate places which are a
collection of multiple places. In particular, it is possible an
execution place which corresponds to multiple device execution places.</p>
<section id="creating-grids-of-places">
<h4>Creating grids of places<a class="headerlink" href="#creating-grids-of-places" title="Link to this heading">#</a></h4>
<p>Grid of execution places are described with the <code class="docutils literal notranslate"><span class="pre">exec_place_grid</span></code>
class. This class is templated by two parameters : a scalar_view execution
place type which represents the type of each individual element, and a
partitioning class which defines how data and indexes are spread across
the different places of the grid.</p>
<p>The scalar_view execution place can be for example be <code class="docutils literal notranslate"><span class="pre">exec_place_device</span></code>
if all entries are devices, or it can be the base <code class="docutils literal notranslate"><span class="pre">exec_place</span></code> class
if the type of the places is not homogeneous in the grid, or if the type
is not known statically, for example.</p>
<p>It is possible to generate a 1D grid from a vector of places :</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">exec_place</span><span class="w"> </span><span class="nf">exec_place::grid</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">exec_place</span><span class="o">&gt;</span><span class="w"> </span><span class="n">places</span><span class="p">);</span>
</pre></div>
</div>
<p>For example, this is used to implement the <code class="docutils literal notranslate"><span class="pre">exec_place::all_devices()</span></code>
helper which creates a grid of all devices.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">partitioner_t</span><span class="o">&gt;</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">exec_place_grid</span><span class="o">&lt;</span><span class="n">exec_place_device</span><span class="p">,</span><span class="w"> </span><span class="n">partitioner_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">exec_place</span><span class="o">::</span><span class="n">all_devices</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">ndevs</span><span class="p">;</span>
<span class="w">    </span><span class="n">cuda_safe_call</span><span class="p">(</span><span class="n">cudaGetDeviceCount</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ndevs</span><span class="p">));</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">exec_place</span><span class="o">&gt;</span><span class="w"> </span><span class="n">devices</span><span class="p">;</span>
<span class="w">    </span><span class="n">devices</span><span class="p">.</span><span class="n">reserve</span><span class="p">(</span><span class="n">ndevs</span><span class="p">);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">ndevs</span><span class="p">;</span><span class="w"> </span><span class="n">d</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">devices</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="n">d</span><span class="p">));</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">exec_place</span><span class="o">::</span><span class="n">grid</span><span class="o">&lt;</span><span class="n">exec_place_device</span><span class="p">,</span><span class="w"> </span><span class="n">partitioner_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">devices</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The default partitioner class associated to
<code class="docutils literal notranslate"><span class="pre">exec_place::all_devices()</span></code> is <code class="docutils literal notranslate"><span class="pre">null_partition</span></code>, which means there
is no partitioning operator defined if none is provided.</p>
<p>It is possible to retrieve the total number of elements in a grid using
the <code class="docutils literal notranslate"><span class="pre">size_t</span> <span class="pre">size()</span></code> method. For <code class="docutils literal notranslate"><span class="pre">exec_place::all_devices()</span></code>, this
will correspond to the total number of devices.</p>
</section>
<section id="shaped-grids">
<h4>Shaped grids<a class="headerlink" href="#shaped-grids" title="Link to this heading">#</a></h4>
<p>To fit the needs of the applications, grid of places need not be 1D
arrays, and can be structured as a multi-dimensional grid described with
a <code class="docutils literal notranslate"><span class="pre">dim4</span></code> class. There is indeed another constructor which takes such a
<code class="docutils literal notranslate"><span class="pre">dim4</span></code> parameter :</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">exec_place</span><span class="o">::</span><span class="n">grid</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">exec_place</span><span class="o">&gt;</span><span class="w"> </span><span class="n">places</span><span class="p">,</span><span class="w"> </span><span class="n">dim4</span><span class="w"> </span><span class="n">dims</span><span class="p">);</span>
</pre></div>
</div>
<p>Note that the total size of <code class="docutils literal notranslate"><span class="pre">dims</span></code> must match the size of the vector
of places.</p>
<p>It is possible to query the <em>shape</em> of the grid using the following
methods : - <code class="docutils literal notranslate"><span class="pre">dim4</span> <span class="pre">get_dims()</span></code> returns the shape of the grid -
<code class="docutils literal notranslate"><span class="pre">int</span> <span class="pre">get_dim(int</span> <span class="pre">axis_id)</span></code> returns the number of elements along
direction <code class="docutils literal notranslate"><span class="pre">axis_id</span></code></p>
<p>Given an <code class="docutils literal notranslate"><span class="pre">exec_place_grid</span></code>, it is also possible to create a new grid
with a different shape using the reshape member of the
<code class="docutils literal notranslate"><span class="pre">exec_place_grid</span></code>. In this example, a grid of 8 devices is reshaped
into a cube of size 2.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// This assumes places.size() == 8</span>
<span class="k">auto</span><span class="w"> </span><span class="n">places</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exec_place</span><span class="o">::</span><span class="n">all_devices</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">places_reshaped</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">places</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">dim4</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">));</span>
</pre></div>
</div>
</section>
<section id="partitioning-policies">
<h4>Partitioning policies<a class="headerlink" href="#partitioning-policies" title="Link to this heading">#</a></h4>
<p>Partitioning policies makes it possible to express how data are
dispatched over the different places of a grid, or how the index space
of a <code class="docutils literal notranslate"><span class="pre">parallel_loop</span></code> will be scattered across places too.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MyPartition</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">partitioner_base</span><span class="w"> </span><span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">S_out</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">S_in</span><span class="o">&gt;</span>
<span class="w">    </span><span class="k">static</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">S_out</span><span class="w"> </span><span class="n">apply</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">S_in</span><span class="o">&amp;</span><span class="w"> </span><span class="n">in</span><span class="p">,</span><span class="w"> </span><span class="n">pos4</span><span class="w"> </span><span class="n">position</span><span class="p">,</span><span class="w"> </span><span class="n">dim4</span><span class="w"> </span><span class="n">grid_dims</span><span class="p">);</span>

<span class="w">    </span><span class="n">pos4</span><span class="w"> </span><span class="nf">get_executor</span><span class="p">(</span><span class="n">pos4</span><span class="w"> </span><span class="n">data_coords</span><span class="p">,</span><span class="w"> </span><span class="n">dim4</span><span class="w"> </span><span class="n">data_dims</span><span class="p">,</span><span class="w"> </span><span class="n">dim4</span><span class="w"> </span><span class="n">grid_dims</span><span class="p">);</span>
<span class="p">};</span>
</pre></div>
</div>
<p>A partitioning class must implement a <code class="docutils literal notranslate"><span class="pre">apply</span></code> method which takes :</p>
<ul class="simple">
<li><p>a reference to a shape of type <code class="docutils literal notranslate"><span class="pre">S_in</span></code> - a position within a grid of
execution places. This position is described using an object of type <code class="docutils literal notranslate"><span class="pre">pos4</span></code></p></li>
<li><p>the dimension of this grid express as a <code class="docutils literal notranslate"><span class="pre">dim4</span></code> object.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">apply</span></code> returns a shape which corresponds to the subset of the <code class="docutils literal notranslate"><span class="pre">in</span></code>
shape associated to this entry of the grid. Note that the output shape
type <code class="docutils literal notranslate"><span class="pre">S_out</span></code> may be different from the <code class="docutils literal notranslate"><span class="pre">S_in</span></code> type of the input
shape.</p>
<p>To support different types of shapes, appropriate overloads of the
<code class="docutils literal notranslate"><span class="pre">apply</span></code> method should be implemented.</p>
<p>This <code class="docutils literal notranslate"><span class="pre">apply</span></code> method is typically used by the <code class="docutils literal notranslate"><span class="pre">parallel_for</span></code>
construct in order to dispatch indices over the different places.</p>
<p>A partitioning class must also implement the <code class="docutils literal notranslate"><span class="pre">get_executor</span></code> virtual
method which allows CUDASTF to use localized data allocators. This
method indicates, for each entry of a shape, on which place this entry
should <em>preferably</em> be allocated.</p>
<p><code class="docutils literal notranslate"><span class="pre">get_executor</span></code> returns a <code class="docutils literal notranslate"><span class="pre">pos4</span></code> coordinate in the execution place
grid, and its arguments are :</p>
<ul class="simple">
<li><p>a coordinate within the shape described as a <code class="docutils literal notranslate"><span class="pre">pos4</span></code> object</p></li>
<li><p>the dimension of the shape expressed as a <code class="docutils literal notranslate"><span class="pre">dim4</span></code> object</p></li>
<li><p>the dimension of the execution place grid expressed as a <code class="docutils literal notranslate"><span class="pre">dim4</span></code> object</p></li>
</ul>
<p>Defining the <code class="docutils literal notranslate"><span class="pre">get_executor</span></code> makes it possible to map a piece of data
over a execution place grid. The <code class="docutils literal notranslate"><span class="pre">get_executor</span></code> method of partitioning
policy in an execution place grid therefore defines the <em>affine data
place</em> of a logical data accessed on that grid.</p>
</section>
<section id="predefined-partitioning-policies">
<h4>Predefined partitioning policies<a class="headerlink" href="#predefined-partitioning-policies" title="Link to this heading">#</a></h4>
<p>There are currently two policies readily available in CUDASTF : -
<code class="docutils literal notranslate"><span class="pre">tiled_partition&lt;TILE_SIZE&gt;</span></code> dispatches entries of a shape using a
<em>tiled</em> layout. For multi-dimensional shapes, the outermost dimension is
dispatched into contiguous tiles of size <code class="docutils literal notranslate"><span class="pre">TILE_SIZE</span></code>. -
<code class="docutils literal notranslate"><span class="pre">blocked_partition</span></code> dispatches entries of the shape using a <em>blocked</em>
layout, where each entry of the grid of places receive approximately
the same contiguous portion of the shape, dispatched along the outermost
dimension.</p>
<p>This illustrates how a 2D shape is dispatched over 3 places using the
blocked layout :</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span> __________________________________
|           |           |         |
|           |           |         |
|           |           |         |
|    P 0    |    P 1    |   P 2   |
|           |           |         |
|           |           |         |
|___________|___________|_________|
</pre></div>
</div>
<p>This illustrates how a 2D shape is dispatched over 3 places using a
tiled layout, where the dimension of the tiles is indicated by the
<code class="docutils literal notranslate"><span class="pre">TILE_SIZE</span></code> parameter :</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span> ________________________________
|     |     |     |     |     |  |
|     |     |     |     |     |  |
|     |     |     |     |     |  |
| P 0 | P 1 | P 2 | P 0 | P 1 |P2|
|     |     |     |     |     |  |
|     |     |     |     |     |  |
|_____|_____|_____|_____|_____|__|
</pre></div>
</div>
</section>
</section>
</section>
<section id="parallel-for-construct">
<span id="id1"></span><h2><a class="toc-backref" href="#id23" role="doc-backlink"><code class="docutils literal notranslate"><span class="pre">parallel_for</span></code> construct</a><a class="headerlink" href="#parallel-for-construct" title="Link to this heading">#</a></h2>
<p>CUDASTF provides a helper construct which creates CUDA kernels (or CPU
kernels) which execute an operation over an index space described as a
<em>shape</em>.</p>
<section id="example-with-a-1-dimensional-array">
<h3>Example with a 1-dimensional array<a class="headerlink" href="#example-with-a-1-dimensional-array" title="Link to this heading">#</a></h3>
<p>The example below illustrates processing a 1D array using
<code class="docutils literal notranslate"><span class="pre">parallel_for</span></code>:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="mi">128</span><span class="p">];</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lA</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">A</span><span class="p">);</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">parallel_for</span><span class="p">(</span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="n">lA</span><span class="p">.</span><span class="n">shape</span><span class="p">(),</span><span class="w"> </span><span class="n">lA</span><span class="p">.</span><span class="n">write</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[]</span><span class="w"> </span><span class="n">__device__</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sA</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">A</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">parallel_for</span></code> construct consists of 4 main elements:</p>
<ul class="simple">
<li><p>an execution place that indicates where the code will be executed;</p></li>
<li><p>a shape defining the index space of the generated kernel;</p></li>
<li><p>a set of data dependencies;</p></li>
<li><p>a body of code specified using the <code class="docutils literal notranslate"><span class="pre">-&gt;*</span></code> operator.</p></li>
</ul>
<p>In the example above, the kernel is launched on the CUDA device with
index 1, which corresponds to the second installed GPU. Each logical
data object has a corresponding <em>data shape</em>, which can be accessed
through the <code class="docutils literal notranslate"><span class="pre">shape()</span></code> member function of the <code class="docutils literal notranslate"><span class="pre">logical_data</span></code>
parametrized type. (The shape of logical data can be thought of as full
information about the layout, without the actual data.) In this example,
<code class="docutils literal notranslate"><span class="pre">lA</span></code> is the logical data associated with a 1D slice of size 128, which
naturally entails iteration over indices in a 1D dimension ranging from
0 to 127 (inclusive). The library associates the iteration strategy with
the data shape. The statement modifies <code class="docutils literal notranslate"><span class="pre">lA</span></code>, so the lambda function
will be executed only when the data is ready. The lambda function has
the <code class="docutils literal notranslate"><span class="pre">__device__</span></code> attribute because a device execution place was
specified. The first parameter corresponds to the index within the shape
(<code class="docutils literal notranslate"><span class="pre">size_t</span> <span class="pre">i</span></code> for a 1D shape). Subsequent parameters are the data
instances associated with the logical data arguments (e.g.,
<code class="docutils literal notranslate"><span class="pre">slice&lt;int&gt;</span> <span class="pre">sA</span></code>).</p>
</section>
<section id="example-with-multi-dimensional-arrays">
<h3>Example with multi-dimensional arrays<a class="headerlink" href="#example-with-multi-dimensional-arrays" title="Link to this heading">#</a></h3>
<p>For multidimensional data shapes, iteration (and consequently the lambda
function) requires additional parameters. Consider an example that uses
<code class="docutils literal notranslate"><span class="pre">parallel_for</span></code> to iterate over 2D arrays:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">const</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">16</span><span class="p">;</span>
<span class="kt">double</span><span class="w"> </span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="p">];</span>
<span class="kt">double</span><span class="w"> </span><span class="n">Y</span><span class="p">[</span><span class="n">N</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="p">];</span>

<span class="k">auto</span><span class="w"> </span><span class="n">lx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">make_slice</span><span class="p">(</span><span class="o">&amp;</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="p">{</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="p">},</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="p">));</span>
<span class="k">auto</span><span class="w"> </span><span class="n">ly</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">make_slice</span><span class="p">(</span><span class="o">&amp;</span><span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="p">{</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="p">},</span><span class="w"> </span><span class="n">N</span><span class="p">));</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">parallel_for</span><span class="p">(</span><span class="n">lx</span><span class="p">.</span><span class="n">shape</span><span class="p">(),</span><span class="w"> </span><span class="n">lx</span><span class="p">.</span><span class="n">write</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">=</span><span class="p">]</span><span class="w"> </span><span class="n">__device__</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">j</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sx</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">sx</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.1</span><span class="p">;</span><span class="w"> </span><span class="p">};</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">parallel_for</span><span class="p">(</span><span class="n">ly</span><span class="p">.</span><span class="n">shape</span><span class="p">(),</span><span class="w"> </span><span class="n">lx</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">ly</span><span class="p">.</span><span class="n">write</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">=</span><span class="p">]</span><span class="w"> </span><span class="n">__device__</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">j</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sx</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sy</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">sy</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y0</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w"> </span><span class="n">ii</span><span class="o">++</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w"> </span><span class="n">jj</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">sy</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">sx</span><span class="p">(</span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ii</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">jj</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="p">};</span>
</pre></div>
</div>
<p>Variables <code class="docutils literal notranslate"><span class="pre">lx</span></code> and <code class="docutils literal notranslate"><span class="pre">ly</span></code> are logical data objects that describe 2D
arrays, so their shapes are 2D index spaces as well. Consequently, a
<code class="docutils literal notranslate"><span class="pre">parallel_for</span></code> construct applied to <code class="docutils literal notranslate"><span class="pre">lx.shape()</span></code> is passed two
indices, <code class="docutils literal notranslate"><span class="pre">size_t</span> <span class="pre">i</span></code> and <code class="docutils literal notranslate"><span class="pre">size_t</span> <span class="pre">j</span></code>. In the second call of
<code class="docutils literal notranslate"><span class="pre">parallel_for</span></code>, two logical data objects with different shapes are
accessed within the same construct. Generally, <code class="docutils literal notranslate"><span class="pre">parallel_for</span></code> can
iterate any number of objects in lockstep, regardless of their
individual shapes.</p>
<p>Passing a lambda with a signature that starts with a number of
<code class="docutils literal notranslate"><span class="pre">size_t</span></code> parameters that does not match the dimensionality of the
shape will result in a compilation error.</p>
</section>
<section id="box-shape">
<h3>Box shape<a class="headerlink" href="#box-shape" title="Link to this heading">#</a></h3>
<p>There are situations where the desired index space does not correspond
to the shape of a logical data object. For those cases, CUDASTF also
provides the template class <code class="docutils literal notranslate"><span class="pre">box&lt;size_t</span> <span class="pre">dimensions</span> <span class="pre">=</span> <span class="pre">1&gt;</span></code> (located in
the header <code class="docutils literal notranslate"><span class="pre">cudastf/utility/dimensions.h</span></code>) that allows user code to
define multidimensional shapes with explicit bounds. The template
parameter represents the dimension of the shape.</p>
</section>
<section id="box-shapes-with-extents">
<h3>Box shapes with extents<a class="headerlink" href="#box-shapes-with-extents" title="Link to this heading">#</a></h3>
<p>Passing a shape object defined as <code class="docutils literal notranslate"><span class="pre">box&lt;2&gt;({2,</span> <span class="pre">3})</span></code> to <code class="docutils literal notranslate"><span class="pre">parallel_for</span></code>
will correspond to a 2-dimensional iteration where the first index
varies from 0 through 1 and the second from 0 through 2. Consider:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">parallel_for</span><span class="p">(</span><span class="n">box</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">}))</span><span class="o">-&gt;*</span><span class="p">[]</span><span class="w"> </span><span class="n">__device__</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;%ld, %ld</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">);</span>
<span class="p">};</span>
</pre></div>
</div>
<p>The code above will print (in an unspecified order):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="mi">1</span><span class="p">,</span> <span class="mi">0</span>
<span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
<span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
<span class="mi">0</span><span class="p">,</span> <span class="mi">2</span>
<span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>
</pre></div>
</div>
<p>Since the <code class="docutils literal notranslate"><span class="pre">box</span></code> default template parameter is 1, it is also possible
to write code to iterate over all values of <code class="docutils literal notranslate"><span class="pre">i</span></code> from 0 through 3:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">parallel_for</span><span class="p">(</span><span class="n">box</span><span class="p">({</span><span class="mi">4</span><span class="p">}))</span><span class="o">-&gt;*</span><span class="p">[]</span><span class="w"> </span><span class="n">__device__</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;%ld</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="p">);</span>
<span class="p">};</span>
</pre></div>
</div>
<section id="box-shapes-with-lower-and-upper-bounds">
<h4>Box shapes with lower and upper bounds<a class="headerlink" href="#box-shapes-with-lower-and-upper-bounds" title="Link to this heading">#</a></h4>
<p>Box shapes can be defined based on their lower and upper bounds. The
lower bounds are inclusive, while the upper bounds are exclusive.
Consider an example similar to the previous one:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">parallel_for</span><span class="p">(</span><span class="n">box</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">({{</span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">}}))</span><span class="o">-&gt;*</span><span class="p">[]</span><span class="w"> </span><span class="n">__device__</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;%ld, %ld</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">);</span>
<span class="p">};</span>
</pre></div>
</div>
<p>It will output (in an unspecified order):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span>
<span class="mi">6</span><span class="p">,</span> <span class="mi">2</span>
<span class="mi">7</span><span class="p">,</span> <span class="mi">2</span>
<span class="mi">5</span><span class="p">,</span> <span class="mi">3</span>
<span class="mi">6</span><span class="p">,</span> <span class="mi">3</span>
<span class="mi">7</span><span class="p">,</span> <span class="mi">3</span>
</pre></div>
</div>
</section>
<section id="defining-custom-shapes-advanced">
<h4>Defining custom shapes (advanced)<a class="headerlink" href="#defining-custom-shapes-advanced" title="Link to this heading">#</a></h4>
<p>Users typically map the <code class="docutils literal notranslate"><span class="pre">parallel_for</span></code> construct over the shape of a
logical data, or over a box shape describing a regular multidimensional
domain, but it is possible to define new types of shapes to describe an
index space.</p>
<p>To define a new type of shape <code class="docutils literal notranslate"><span class="pre">S</span></code> (where <code class="docutils literal notranslate"><span class="pre">S</span></code> typically has a form of
<code class="docutils literal notranslate"><span class="pre">shape_of&lt;I&gt;</span></code>) that can be used as an indexable shape for
<code class="docutils literal notranslate"><span class="pre">parallel_for</span></code>, <code class="docutils literal notranslate"><span class="pre">shape_of&lt;I&gt;</span></code> must define inner type <code class="docutils literal notranslate"><span class="pre">coords_t</span></code>
and member function <code class="docutils literal notranslate"><span class="pre">index_to_coords</span></code> as follows:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">I</span><span class="o">&gt;</span>
<span class="k">class</span><span class="w"> </span><span class="nc">shape_of</span><span class="o">&lt;</span><span class="n">I</span><span class="o">&gt;</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="k">using</span><span class="w"> </span><span class="n">coords_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">...;</span>

<span class="w">    </span><span class="c1">// This transforms a 1D index into a coordinate</span>
<span class="w">    </span><span class="n">__device__</span><span class="w"> </span><span class="n">__host__</span><span class="w"> </span><span class="n">coords_t</span><span class="w"> </span><span class="n">index_to_coords</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="p">...</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</pre></div>
</div>
<p>The dimensionality of this <code class="docutils literal notranslate"><span class="pre">coord_t</span></code> tuple type determines the number
of arguments passed to the lambda function in <code class="docutils literal notranslate"><span class="pre">parallel_for</span></code>.</p>
</section>
</section>
<section id="reduce-access-mode">
<span id="id2"></span><h3>Reduce access mode<a class="headerlink" href="#reduce-access-mode" title="Link to this heading">#</a></h3>
<p>The <cite>parallel_for</cite> construct supports the <code class="docutils literal notranslate"><span class="pre">reduce()</span></code> access mode. This mode
implements reductions within the compute kernel generated by the <cite>parallel_for</cite>
construct.</p>
<p><code class="docutils literal notranslate"><span class="pre">reduce()</span></code> accepts different arguments which define its behavior:
- The first argument must be the reduction operator which defines how multiple values are combined, and how to initialize a value (for example a sum   reduction will add two values, and initialize values to 0).
- An optional <code class="docutils literal notranslate"><span class="pre">no_init{}</span></code> value indicate that the result of the reduction should be accumulated to the existing value stored in the logical data, similarly to a <code class="docutils literal notranslate"><span class="pre">rw()</span></code> access mode. By default, if this value is not passed to <code class="docutils literal notranslate"><span class="pre">reduce()</span></code>, the content of the logical data would be overwritten as with a <code class="docutils literal notranslate"><span class="pre">write()</span></code> access mode. Using <code class="docutils literal notranslate"><span class="pre">no_init{}</span></code> with a logical data which has no valid instance is an error (for example when the logical data is just defined from its shape and that no previous write access was made).
- Other arguments are the same passed to other access modes such as the data place.</p>
<p>We can only apply the <code class="docutils literal notranslate"><span class="pre">reduce()</span></code> access mode on logical data which data
interface have defined the <code class="docutils literal notranslate"><span class="pre">owning_container_of</span></code> trait class. This is the
case of the <code class="docutils literal notranslate"><span class="pre">scalar_view&lt;T&gt;</span></code> data interface, which sets <code class="docutils literal notranslate"><span class="pre">owning_container_of</span></code> to
be <code class="docutils literal notranslate"><span class="pre">T</span></code>. The argument passed to the <code class="docutils literal notranslate"><span class="pre">parallel_for</span></code> construct is a reference
to object of this type. The following piece of code for example computes the
dot product of two vectors of double elements (<code class="docutils literal notranslate"><span class="pre">slice&lt;double&gt;</span></code>) using a
reduction, and a <code class="docutils literal notranslate"><span class="pre">scalar_view&lt;double&gt;</span></code>. Reductions are typically used in
combination with the <code class="docutils literal notranslate"><span class="pre">wait</span></code> mechanism which synchronously returns
the content of the logical data in a variable.</p>
<div class="literal-block-wrapper docutils container" id="id12">
<div class="code-block-caption"><span class="caption-text">dot product using a reduction operation</span><a class="headerlink" href="#id12" title="Link to this code">#</a></div>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">lsum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">shape_of</span><span class="o">&lt;</span><span class="n">scalar_view</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="p">());</span>

<span class="cm">/* Compute sum(x_i * y_i)*/</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">parallel_for</span><span class="p">(</span><span class="n">lY</span><span class="p">.</span><span class="n">shape</span><span class="p">(),</span><span class="w"> </span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lsum</span><span class="p">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">reducer</span><span class="o">::</span><span class="n">sum</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">{}))</span>
<span class="w">    </span><span class="o">-&gt;*</span><span class="p">[]</span><span class="w"> </span><span class="n">__device__</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dY</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="o">&amp;</span><span class="w"> </span><span class="n">sum</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">dX</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">dY</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
<span class="w">        </span><span class="p">};</span>

<span class="kt">double</span><span class="w"> </span><span class="n">res</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">wait</span><span class="p">(</span><span class="n">lsum</span><span class="p">);</span>
</pre></div>
</div>
</div>
<p>Note that if we had put a <code class="docutils literal notranslate"><span class="pre">no_init{}</span></code> argument after
<code class="docutils literal notranslate"><span class="pre">reducer::sum&lt;double&gt;{}</span></code> we would have an error because <code class="docutils literal notranslate"><span class="pre">lsum</span></code> was not
initialized.</p>
<p>Multiple reductions can be used with different operators in the same
<cite>parallel_for</cite> construct.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id13">
<caption><span class="caption-text">Predefined Reduction Operators and Neutral Elements</span><a class="headerlink" href="#id13" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Operator Name</p></th>
<th class="head"><p>Purpose</p></th>
<th class="head"><p>Neutral Element</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">sum</span></code></p></td>
<td><p>Computes the summation of elements.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">product</span></code></p></td>
<td><p>Computes the product of elements.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">maxval</span></code></p></td>
<td><p>Finds the maximum value.</p></td>
<td><p>Smallest representable value (e.g., <code class="docutils literal notranslate"><span class="pre">-inf</span></code> for floats).</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">minval</span></code></p></td>
<td><p>Finds the minimum value.</p></td>
<td><p>Largest representable value (e.g., <code class="docutils literal notranslate"><span class="pre">+inf</span></code> for floats).</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">logical_and</span></code></p></td>
<td><p>Performs logical AND reduction.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">logical_or</span></code></p></td>
<td><p>Performs logical OR reduction.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">bitwise_and</span></code></p></td>
<td><p>Performs bitwise AND reduction.</p></td>
<td><p>All bits set (e.g., <code class="docutils literal notranslate"><span class="pre">~0</span></code> or <code class="docutils literal notranslate"><span class="pre">-1</span></code> for integers).</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">bitwise_or</span></code></p></td>
<td><p>Performs bitwise OR reduction.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">bitwise_xor</span></code></p></td>
<td><p>Performs bitwise XOR reduction.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p>A set of predefined reduction operators are defined, but users may easily
define their own operators. These operators are defined in the
<code class="docutils literal notranslate"><span class="pre">cuda::experimental::stf::reducer</span></code> namespace. The following piece of code
for example illustrates how to implement the <code class="docutils literal notranslate"><span class="pre">sum</span></code> reduction operator.</p>
<div class="literal-block-wrapper docutils container" id="id14">
<div class="code-block-caption"><span class="caption-text">Defining the sum reduction operator</span><a class="headerlink" href="#id14" title="Link to this code">#</a></div>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="k">class</span><span class="w"> </span><span class="nc">sum</span>
<span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="n">__host__</span><span class="w"> </span><span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">init_op</span><span class="p">(</span><span class="n">T</span><span class="o">&amp;</span><span class="w"> </span><span class="n">dst</span><span class="p">)</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">    </span><span class="n">dst</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="n">__host__</span><span class="w"> </span><span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">apply_op</span><span class="p">(</span><span class="n">T</span><span class="o">&amp;</span><span class="w"> </span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">T</span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">)</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">    </span><span class="n">dst</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">src</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>
</pre></div>
</div>
</div>
<p>Every reduction operator should therefore define both the <code class="docutils literal notranslate"><span class="pre">ìnit_op</span></code> method
that sets the neutral element, and the <code class="docutils literal notranslate"><span class="pre">apply_op</span></code> method which combines two
elements. Appropriate <code class="docutils literal notranslate"><span class="pre">__host__</span></code> and/or <code class="docutils literal notranslate"><span class="pre">__device__</span></code> annotations are
required depending where the operation may occur. The type of the arguments are
references to <code class="docutils literal notranslate"><span class="pre">owning_container_of&lt;interface&gt;::type</span></code> where <code class="docutils literal notranslate"><span class="pre">interface</span></code> is
the data interface type of the logical data.</p>
</section>
</section>
<section id="launch-construct">
<span id="id3"></span><h2><a class="toc-backref" href="#id24" role="doc-backlink"><code class="docutils literal notranslate"><span class="pre">launch</span></code> construct</a><a class="headerlink" href="#launch-construct" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">ctx.launch</span></code> primitive in CUDASTF is a kernel-launch mechanism
that handles the mapping and launching of a single kernel onto execution
places implicitly.</p>
<p>Syntax:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">launch</span><span class="p">([</span><span class="kr">thread</span><span class="w"> </span><span class="n">hierarchy</span><span class="w"> </span><span class="n">spec</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="n">execution</span><span class="w"> </span><span class="n">place</span><span class="p">],</span><span class="w"> </span><span class="n">logicalData1</span><span class="p">.</span><span class="n">accessMode</span><span class="p">(),</span><span class="w"> </span><span class="n">logicalData2</span><span class="p">.</span><span class="n">accessMode</span><span class="p">())</span>
<span class="w">    </span><span class="o">-&gt;*</span><span class="p">[</span><span class="n">capture</span><span class="w"> </span><span class="n">list</span><span class="p">]</span><span class="w"> </span><span class="n">__device__</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">th_spec</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">data1</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">data2</span><span class="w"> </span><span class="p">...)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Kernel implementation</span>
<span class="w">    </span><span class="p">};</span>
</pre></div>
</div>
<section id="example-with-a-1-dimensional-array-1">
<span id="id4"></span><h3>Example with a 1-dimensional array<a class="headerlink" href="#example-with-a-1-dimensional-array-1" title="Link to this heading">#</a></h3>
<p>The example below illustrates processing a 1D array using <code class="docutils literal notranslate"><span class="pre">launch</span></code>:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">launch</span><span class="p">(</span><span class="n">par</span><span class="p">(</span><span class="mi">1024</span><span class="p">),</span><span class="w"> </span><span class="n">all_devs</span><span class="p">,</span><span class="w"> </span><span class="n">handle_X</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">cdp</span><span class="p">),</span><span class="w"> </span><span class="n">handle_Y</span><span class="p">.</span><span class="n">rw</span><span class="p">(</span><span class="n">cdp</span><span class="p">))</span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">=</span><span class="p">]</span><span class="w"> </span><span class="n">__device__</span><span class="p">(</span><span class="n">thread_info</span><span class="w"> </span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">tid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t</span><span class="p">.</span><span class="n">thread_id</span><span class="p">();</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">nthreads</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t</span><span class="p">.</span><span class="n">get_num_threads</span><span class="p">();</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tid</span><span class="p">;</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">nthreads</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">y</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">(</span><span class="n">ind</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">launch</span></code> construct consists of five main elements:</p>
<ul class="simple">
<li><p>an optional <code class="docutils literal notranslate"><span class="pre">execution_policy</span></code> that explicitly specifies the launch
shape. Here we specify that a group of 1024 independent threads should
execute the loop described in the body of the launch construct.</p></li>
<li><p>an execution place that indicates where the code will be executed;</p></li>
<li><p>a set of data dependencies;</p></li>
<li><p>a body of code specified using the <code class="docutils literal notranslate"><span class="pre">-&gt;*</span></code> operator.</p></li>
<li><p>a parameter to the kernel <code class="docutils literal notranslate"><span class="pre">thread_info</span> <span class="pre">t</span></code> for thread properties.</p></li>
</ul>
<p>In the example above, the kernel is launched on all of the available
CUDA devices. The lambda function has the <code class="docutils literal notranslate"><span class="pre">__device__</span></code> attribute
because a device execution place was specified. The first parameter
corresponds to the per thread information that the user can query. This
includes a global thread id and the total number of threads that will be
executing the kernel. Subsequent parameters are the data instances
associated with the logical data arguments (e.g., <code class="docutils literal notranslate"><span class="pre">slice&lt;double&gt;</span> <span class="pre">x</span></code>).</p>
</section>
<section id="describing-a-thread-hierarchy">
<h3>Describing a thread hierarchy<a class="headerlink" href="#describing-a-thread-hierarchy" title="Link to this heading">#</a></h3>
<p>The thread hierarchy specification describes the structure of the parallelism of this kernel. Level sizes can be computed automatically, be a dynamic value or be specified at compile-time.
Threads in a parallel group (<cite>par</cite>) are executed independently.
Threads in a concurrent group (<cite>con</cite>) can be synchronized using the <cite>sync()</cite> API which issues a group-level barrier.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">con</span><span class="p">()</span><span class="w"> </span><span class="c1">// A single level of threads which are allowed to synchronize</span>
<span class="n">par</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span><span class="w"> </span><span class="c1">// A single level of 128 threads which cannot synchronize</span>
<span class="n">par</span><span class="o">&lt;</span><span class="mi">128</span><span class="o">&gt;</span><span class="p">()</span><span class="w"> </span><span class="c1">// A single level with a statically defined size</span>
</pre></div>
</div>
<p>Thread are described in a hierarchical manner : we can nest multiple groups with different characteristics.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">par</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="n">con</span><span class="o">&lt;</span><span class="mi">256</span><span class="o">&gt;</span><span class="p">())</span><span class="w"> </span><span class="c1">// A two-level thread hierarchy with 128 independent groups of 256 synchronizable threads..</span>
</pre></div>
</div>
<p>Within each group, we can provide additional information such memory automatically shared among group members.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">con</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="n">mem</span><span class="p">(</span><span class="mi">64</span><span class="p">))</span><span class="w"> </span><span class="c1">// A group of 256 threads sharing 64 bytes of memory</span>
</pre></div>
</div>
<p>We can also provide some affinity information if we want to ensure that  group of threads is mapped on a specific level of the machine hierarchy.</p>
<p>The different scopes available are :</p>
<ul class="simple">
<li><p><cite>hw_scope::thread</cite> : CUDA threads</p></li>
<li><p><cite>hw_scope::block</cite> : CUDA blocks</p></li>
<li><p><cite>hw_scope::device</cite> : CUDA device</p></li>
<li><p><cite>hw_scope::all</cite> : all machine</p></li>
</ul>
<p>This for example describes a thread hierarchy where the inner-most level is limited to CUDA threads (i.e. it cannot span multiple CUDA blocks). And the overall kernel can be mapped at most on a single device, but not on multiple devices).</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">par</span><span class="p">(</span><span class="n">hw_scope</span><span class="o">::</span><span class="n">device</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">hw_scope</span><span class="o">::</span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="n">par</span><span class="o">&lt;</span><span class="mi">128</span><span class="o">&gt;</span><span class="p">(</span><span class="n">hw_scope</span><span class="o">::</span><span class="kr">thread</span><span class="p">));</span>
</pre></div>
</div>
</section>
<section id="manipulating-a-thread-hierarchy">
<h3>Manipulating a thread hierarchy<a class="headerlink" href="#manipulating-a-thread-hierarchy" title="Link to this heading">#</a></h3>
<p>When the <cite>ctx.launch</cite> construct is executed, a thread hierarchy object is passed to the function which implements the kernel. This object is available on the device(s) (assuming a device execution place), and makes it possible to query the structure of the hierarchy (e.g. based on the type of the thread hierarchy object), and allows the threads in the hierarchy to interact by the means of the <cite>sync()</cite> primitive for groups marked as concurrent (<cite>con</cite>).</p>
<p>As an example, let us consider that we have the <cite>par(128, con&lt;32&gt;())</cite> hierarchy.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">th</span><span class="p">.</span><span class="n">rank</span><span class="p">();</span><span class="w"> </span><span class="c1">// rank of the thread within the entire hierarchy (a number between 0 and 4096)</span>
<span class="n">th</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="c1">// size of the group within the entire hierarchy (4096)</span>
<span class="n">th</span><span class="p">.</span><span class="n">rank</span><span class="p">(</span><span class="n">i</span><span class="p">);</span><span class="w"> </span><span class="c1">// rank of the thread within the i-th level of the hierarchy (e.g. a number between 0 and 128 for th.rank(0))</span>
<span class="n">th</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="n">i</span><span class="p">);</span><span class="w"> </span><span class="c1">// size of the group at the i-th level of the hierarchy (128)</span>
<span class="n">th</span><span class="p">.</span><span class="n">static_with</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="c1">// size of the group at the i-th level if known at compile time (constexpr value)</span>
</pre></div>
</div>
<p>We can query if the i-th level is synchronizable or not using the following API:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">th</span><span class="p">.</span><span class="n">is_synchronizable</span><span class="p">(</span><span class="n">i</span><span class="p">);</span><span class="w"> </span><span class="c1">// if i is a dynamic value</span>
<span class="n">th</span><span class="p">.</span><span class="k">template</span><span class="w"> </span><span class="n">is_synchronizable</span><span class="o">&lt;</span><span class="n">i</span><span class="o">&gt;</span><span class="p">();</span><span class="w"> </span><span class="c1">// if i is known at compile time</span>
</pre></div>
</div>
<p>If the level is synchonizable, we can call</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">th</span><span class="p">.</span><span class="n">sync</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="c1">// synchronize all threads of the i-th level</span>
<span class="n">th</span><span class="p">.</span><span class="n">sync</span><span class="p">()</span><span class="w"> </span><span class="c1">// synchronize all threads of the top-most level (level 0)</span>
</pre></div>
</div>
<p>We can query the affinity of the i-th level with</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">th</span><span class="p">.</span><span class="n">get_scope</span><span class="p">(</span><span class="n">i</span><span class="p">);</span><span class="w"> </span><span class="c1">// returns the scope of the i-th level</span>
</pre></div>
</div>
<p>It is possible to get the amount of memory available for each level :</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">th</span><span class="p">.</span><span class="n">get_mem</span><span class="p">(</span><span class="n">i</span><span class="p">);</span><span class="w"> </span><span class="c1">// returns the amount of memory available at i-th level</span>
</pre></div>
</div>
<p>And we can retrieve the corresponding per-level buffer as a slice (which
CUDASTF will automatically allocate in the most appropriate level of the memory
hierarchy among shared memory, device memory or managed memory) :</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span><span class="w"> </span><span class="n">smem</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">th</span><span class="p">.</span><span class="k">template</span><span class="w"> </span><span class="n">storage</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
<p>The depth of the thread hierarchy corresponds to the number of nested levels in the hierarchy :</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">th</span><span class="p">.</span><span class="n">depth</span><span class="p">();</span><span class="w"> </span><span class="c1">// (constexpr) return the number of levels in the hierarchy</span>
</pre></div>
</div>
<p>To simplify how we navigate within hierarchies, applying the <cite>inner()</cite> method
returns a thread hierarchy where the top-most level was removed. The returned specification
will differ between the different threads which call <cite>inner()</cite>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">spec</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">con</span><span class="o">&lt;</span><span class="mi">128</span><span class="o">&gt;</span><span class="p">(</span><span class="n">con</span><span class="p">());</span>
<span class="p">...</span>
<span class="k">auto</span><span class="w"> </span><span class="n">ti</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">th</span><span class="p">.</span><span class="n">inner</span><span class="p">();</span>
<span class="n">ti</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="c1">// size within the par() hierarchy (automatically computed value)</span>
<span class="n">ti</span><span class="p">.</span><span class="n">rank</span><span class="p">();</span><span class="w"> </span><span class="c1">// rank within the par() hierarchy</span>
<span class="p">...</span>
<span class="n">th</span><span class="p">.</span><span class="n">inner</span><span class="p">().</span><span class="n">sync</span><span class="p">();</span><span class="w"> </span><span class="c1">// synchronize threads in the same block of the second level of the hierarchy</span>
</pre></div>
</div>
</section>
</section>
<section id="cuda-kernel-construct">
<h2><a class="toc-backref" href="#id25" role="doc-backlink"><code class="docutils literal notranslate"><span class="pre">cuda_kernel</span></code> construct</a><a class="headerlink" href="#cuda-kernel-construct" title="Link to this heading">#</a></h2>
<p>CUDASTF provides the <cite>cuda_kernel</cite> construct to implement tasks executing a
CUDA kernel. This construct is especially useful when we writing code that may
be executed using a CUDA graph backend, because its <cite>task</cite> construct relies on
a graph capture mechanism which has some overhead, while the <cite>cuda_kernel</cite>
construct is directly translated to CUDA kernel launch APIs, thus avoiding this
overhead.</p>
<p><cite>cuda_kernel</cite> accepts the same arguments as the task construct, including an
execution place and a list of data dependencies.  It implements a <cite>-&gt;*</cite>
operator that takes a lambda function as argument.  This lambda function must
return an object of type <cite>cuda_kernel_desc</cite>, describing the CUDA kernel to
execute.  The constructor of the <cite>cuda_kernel_desc</cite> class, shown below, takes
the CUDA kernel function pointer (ie. the <code class="docutils literal notranslate"><span class="pre">__global__</span></code> method defining the
kernel), a grid description, the amount of dynamically allocated shared memory,
and finally all the arguments that must be passed to the CUDA kernel.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">Fun</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="p">...</span><span class="w"> </span><span class="n">Args</span><span class="o">&gt;</span>
<span class="n">cuda_kernel_desc</span><span class="p">(</span><span class="n">Fun</span><span class="w"> </span><span class="n">func</span><span class="p">,</span><span class="w">           </span><span class="c1">// Pointer to the CUDA kernel function (__global__)</span>
<span class="w">                 </span><span class="n">dim3</span><span class="w"> </span><span class="n">gridDim_</span><span class="p">,</span><span class="w">      </span><span class="c1">// Dimensions of the grid (number of thread blocks)</span>
<span class="w">                 </span><span class="n">dim3</span><span class="w"> </span><span class="n">blockDim_</span><span class="p">,</span><span class="w">     </span><span class="c1">// Dimensions of each thread block</span>
<span class="w">                 </span><span class="kt">size_t</span><span class="w"> </span><span class="n">sharedMem_</span><span class="p">,</span><span class="w">  </span><span class="c1">// Amount of dynamically allocated shared memory</span>
<span class="w">                 </span><span class="n">Args</span><span class="p">...</span><span class="w"> </span><span class="n">args</span><span class="p">)</span><span class="w">       </span><span class="c1">// Arguments passed to the CUDA kernel</span>
</pre></div>
</div>
<p>For example, the following piece of code creates a task that launches a CUDA kernel that accesses two logical data.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">cuda_kernel</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="k">auto</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dY</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// calls __global__ void axpy(double a, slice&lt;const double&gt; x, slice&lt;double&gt; y);</span>
<span class="w">  </span><span class="c1">// similarly to axpy&lt;&lt;&lt;16, 128, 0, ...&gt;&gt;&gt;(alpha, dX, dY)</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">cuda_kernel_desc</span><span class="p">{</span><span class="n">axpy</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="n">dY</span><span class="p">};</span>
<span class="p">};</span>
</pre></div>
</div>
<p>Similar to the <cite>task</cite> construct, the <cite>cuda_kernel</cite> construct also supports
specifying dynamic dependencies using the <cite>add_deps</cite> method and retrieving data
instances using <cite>get</cite>. The previous code can therefore be rewritten as:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">cuda_kernel</span><span class="p">();</span>
<span class="n">t</span><span class="p">.</span><span class="n">add_deps</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">());</span>
<span class="n">t</span><span class="p">.</span><span class="n">add_deps</span><span class="p">(</span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">());</span>
<span class="n">t</span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">dX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t</span><span class="p">.</span><span class="k">template</span><span class="w"> </span><span class="n">get</span><span class="o">&lt;</span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">dY</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t</span><span class="p">.</span><span class="k">template</span><span class="w"> </span><span class="n">get</span><span class="o">&lt;</span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">cuda_kernel_desc</span><span class="p">{</span><span class="n">axpy</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="n">dY</span><span class="p">};</span>
<span class="p">};</span>
</pre></div>
</div>
</section>
<section id="cuda-kernel-chain-construct">
<h2><a class="toc-backref" href="#id26" role="doc-backlink"><code class="docutils literal notranslate"><span class="pre">cuda_kernel_chain</span></code> construct</a><a class="headerlink" href="#cuda-kernel-chain-construct" title="Link to this heading">#</a></h2>
<p>In addition to <cite>cuda_kernel</cite>, CUDASTF provides the <cite>cuda_kernel_chain</cite>
construct to execute sequences of CUDA kernels within a single task. Unlike
<cite>cuda_kernel</cite>, which expects a single kernel descriptor, the lambda passed to
the <cite>-&gt;*</cite> operator of <cite>cuda_kernel_chain</cite> should return a
<cite>::std::vector&lt;cuda_kernel_desc&gt;</cite> describing multiple kernel launches.
Kernels specified within the vector are executed sequentially in the order they appear.</p>
<p>The following two constructs are therefore equivalent, except that the
<cite>cuda_kernel_chain</cite> implementation directly translate to efficient, direct CUDA
kernel launch APIs, while the implementation of the <cite>task</cite> construct may rely
on graph capture when using a CUDA graph backend.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cm">/* Compute Y = Y + alpha X, Y = Y + beta X, then Y = Y + gamma X sequentially */</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">cuda_kernel_chain</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="k">auto</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dY</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="k">return</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">cuda_kernel_desc</span><span class="o">&gt;</span><span class="w"> </span><span class="p">{</span>
<span class="w">       </span><span class="p">{</span><span class="w"> </span><span class="n">axpy</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="n">dY</span><span class="w"> </span><span class="p">},</span>
<span class="w">       </span><span class="p">{</span><span class="w"> </span><span class="n">axpy</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w">  </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="n">dY</span><span class="w"> </span><span class="p">},</span>
<span class="w">       </span><span class="p">{</span><span class="w"> </span><span class="n">axpy</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">gamma</span><span class="p">,</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="n">dY</span><span class="w"> </span><span class="p">}</span>
<span class="w">   </span><span class="p">};</span>
<span class="p">};</span>

<span class="cm">/* Equivalent to the previous construct, but possibly less efficient */</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dY</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="n">axpy</span><span class="o">&lt;&lt;&lt;</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="n">dY</span><span class="p">);</span>
<span class="w">   </span><span class="n">axpy</span><span class="o">&lt;&lt;&lt;</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span><span class="w">  </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="n">dY</span><span class="p">);</span>
<span class="w">   </span><span class="n">axpy</span><span class="o">&lt;&lt;&lt;</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="n">dY</span><span class="p">);</span>
<span class="p">};</span>
</pre></div>
</div>
<p>Similarly to the <cite>cuda_kernel</cite> constructs, dependencies can be set dynamically:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cm">/* Compute Y = Y + alpha X, Y = Y + beta X, then Y = Y + gamma X sequentially */</span>
<span class="k">auto</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">cuda_kernel_chain</span><span class="p">();</span>
<span class="n">t</span><span class="p">.</span><span class="n">add_deps</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">());</span>
<span class="n">t</span><span class="p">.</span><span class="n">add_deps</span><span class="p">(</span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">());</span>
<span class="n">t</span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">dX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t</span><span class="p">.</span><span class="k">template</span><span class="w"> </span><span class="n">get</span><span class="o">&lt;</span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">dY</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t</span><span class="p">.</span><span class="k">template</span><span class="w"> </span><span class="n">get</span><span class="o">&lt;</span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">cuda_kernel_desc</span><span class="o">&gt;</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="p">{</span><span class="w"> </span><span class="n">axpy</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="n">dY</span><span class="w"> </span><span class="p">},</span>
<span class="w">      </span><span class="p">{</span><span class="w"> </span><span class="n">axpy</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="n">dY</span><span class="w"> </span><span class="p">},</span>
<span class="w">      </span><span class="p">{</span><span class="w"> </span><span class="n">axpy</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">gamma</span><span class="p">,</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="n">dY</span><span class="w"> </span><span class="p">}</span>
<span class="w">  </span><span class="p">};</span>
<span class="p">};</span>
</pre></div>
</div>
</section>
<section id="c-types-of-logical-data-and-tasks">
<h2><a class="toc-backref" href="#id27" role="doc-backlink">C++ Types of logical data and tasks</a><a class="headerlink" href="#c-types-of-logical-data-and-tasks" title="Link to this heading">#</a></h2>
<p>To prevent a common class of errors, CUDASTF strives to align its
processing semantics with C++ types as closely as possible. As shown in
the various examples, the use of the <code class="docutils literal notranslate"><span class="pre">auto</span></code> keyword is usually
recommended to create readable code while type safety is still enforced.</p>
<section id="logical-data-1">
<span id="id5"></span><h3>Logical data<a class="headerlink" href="#logical-data-1" title="Link to this heading">#</a></h3>
<p>The result of calling <code class="docutils literal notranslate"><span class="pre">ctx.logical_data()</span></code> is an object whose type
contains information about the underlying data interface used to
manipulate the logical data object. For example, a contiguous array of
<code class="docutils literal notranslate"><span class="pre">double</span></code> is internally represented as a <code class="docutils literal notranslate"><span class="pre">slice</span></code> (which is an alias
of <code class="docutils literal notranslate"><span class="pre">std::experimental::mdspan</span></code>) so that we can use the following type:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">X</span><span class="p">[</span><span class="mi">16</span><span class="p">];</span>
<span class="n">logical_data</span><span class="o">&lt;</span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
</pre></div>
</div>
<p>For simplicity and without losing any information, users can typically
rely on the <code class="docutils literal notranslate"><span class="pre">auto</span></code> keyword:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">X</span><span class="p">[</span><span class="mi">16</span><span class="p">];</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
</pre></div>
</div>
<p>One may for example store the logical data of a <code class="docutils literal notranslate"><span class="pre">slice&lt;int&gt;</span></code> in a C++
class or structure in such as way:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">foo</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="p">...</span>
<span class="w">   </span><span class="k">mutable</span><span class="w"> </span><span class="n">logical_data</span><span class="o">&lt;</span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">ldata</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
<p>Note the use of the <code class="docutils literal notranslate"><span class="pre">mutable</span></code> qualifier because a task accessing a
<code class="docutils literal notranslate"><span class="pre">const</span> <span class="pre">foo</span></code> object might want to read the <code class="docutils literal notranslate"><span class="pre">ldata</span></code> field. Submitting a
task that use this logical data in read only mode would modify the
internal data structures of the logical data, but should probably appear
as a <code class="docutils literal notranslate"><span class="pre">const</span></code> operation from user’s perspective. Without this <code class="docutils literal notranslate"><span class="pre">mutable</span></code>
qualifier, we could not have a <code class="docutils literal notranslate"><span class="pre">const</span></code> qualifier on the <code class="docutils literal notranslate"><span class="pre">f</span></code> variable
in the following code :</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">func</span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">foo</span><span class="w"> </span><span class="o">&amp;</span><span class="n">f</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">ldata</span><span class="p">.</span><span class="n">read</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="p">...</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="n">work</span><span class="w"> </span><span class="p">...</span>
<span class="w">    </span><span class="p">};</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id6">
<h3>Tasks<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>With a <code class="docutils literal notranslate"><span class="pre">stream_ctx</span></code> backend, <code class="docutils literal notranslate"><span class="pre">ctx.task(lX.read(),</span> <span class="pre">lY.rw())</span></code> returns
an object of type <code class="docutils literal notranslate"><span class="pre">stream_task&lt;TX,</span> <span class="pre">TY&gt;</span></code>, where the template arguments
<code class="docutils literal notranslate"><span class="pre">TX</span></code> and <code class="docutils literal notranslate"><span class="pre">TY</span></code> are the types associated to the data interfaces in
logical data <code class="docutils literal notranslate"><span class="pre">lX</span></code> and <code class="docutils literal notranslate"><span class="pre">lY</span></code>. Assuming two arrays of <code class="docutils literal notranslate"><span class="pre">double</span></code>, which
CUDASTF internally manages as <code class="docutils literal notranslate"><span class="pre">slice&lt;double&gt;</span></code> objects, the type of
this task will be:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">stream_task</span><span class="o">&lt;</span><span class="n">slice</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">double</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span>
</pre></div>
</div>
<p>The type of the task contains information about the element type and its
modifiability — read-only access is mapped to a slice of
<code class="docutils literal notranslate"><span class="pre">const</span> <span class="pre">double</span></code> as opposed to <code class="docutils literal notranslate"><span class="pre">double</span></code>. The type information is
propagated further from the task object to the lambda invoked by means
of <code class="docutils literal notranslate"><span class="pre">operator-&gt;*</span></code> in such a way that type errors are detected during
compilation.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">X</span><span class="p">[</span><span class="mi">16</span><span class="p">],</span><span class="w"> </span><span class="n">Y</span><span class="p">[</span><span class="mi">16</span><span class="p">];</span>
<span class="n">logical_data</span><span class="o">&lt;</span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
<span class="n">logical_data</span><span class="o">&lt;</span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">lY</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">Y</span><span class="p">);</span>

<span class="c1">// results in a compilation error due to the erroneous slice&lt;int&gt; type</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">};</span>
</pre></div>
</div>
<p>In most cases, it’s recommended to use the <code class="docutils literal notranslate"><span class="pre">auto</span></code> C++ keyword to
automatically obtain the correct data types:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">X</span><span class="p">[</span><span class="mi">16</span><span class="p">],</span><span class="w"> </span><span class="n">Y</span><span class="p">[</span><span class="mi">16</span><span class="p">];</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lY</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">Y</span><span class="p">);</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">};</span>
</pre></div>
</div>
<p>In the graph backend, the untyped task type equivalent to
<code class="docutils literal notranslate"><span class="pre">stream_task&lt;&gt;</span></code> is <code class="docutils literal notranslate"><span class="pre">graph_task</span></code>, and the equivalent to
<code class="docutils literal notranslate"><span class="pre">stream_task&lt;T1,</span> <span class="pre">T2&gt;</span></code> would be, for example, <code class="docutils literal notranslate"><span class="pre">graph_task&lt;T1,</span> <span class="pre">T2&gt;</span></code>.
When using the generic context type, CUDASTF would create a task of type
<code class="docutils literal notranslate"><span class="pre">unified_task&lt;T1,</span> <span class="pre">T2&gt;</span></code>.</p>
</section>
<section id="dynamically-typed-tasks">
<h3>Dynamically-typed tasks<a class="headerlink" href="#dynamically-typed-tasks" title="Link to this heading">#</a></h3>
<p>In certain circumstances, the exact data accessed by a task (and
consequently the type of a task as discussed above) may not be available
statically. For example, updating a part of the computation domain might
require accessing the closest neighbors of that part. The neighbors are
known only dynamically, meaning that it is not possible to directly pass
task dependencies as arguments to the <code class="docutils literal notranslate"><span class="pre">ctx.task()</span></code> call.</p>
<p>For such situations CUDASTF offers a dynamically-typed task, called
<code class="docutils literal notranslate"><span class="pre">stream_task&lt;&gt;</span></code> in the <code class="docutils literal notranslate"><span class="pre">stream_ctx</span></code> backend, whose member function
<code class="docutils literal notranslate"><span class="pre">add_deps</span></code> allows adding dependencies dynamically:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">X</span><span class="p">[</span><span class="mi">16</span><span class="p">],</span><span class="w"> </span><span class="n">Y</span><span class="p">[</span><span class="mi">16</span><span class="p">];</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lY</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">Y</span><span class="p">);</span>

<span class="n">stream_task</span><span class="o">&lt;&gt;</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">();</span>
<span class="n">t</span><span class="p">.</span><span class="n">add_deps</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">());</span>
</pre></div>
</div>
<p>This dynamic approach entails a loss of expressiveness. The API based on
the <code class="docutils literal notranslate"><span class="pre">-&gt;*</span></code> notation is only compatible with <em>statically-typed</em> tasks,
as the user-provided lambda function needs to be passed data instances
of the proper types (for example <code class="docutils literal notranslate"><span class="pre">slice&lt;double&gt;</span></code>) by CUDASTF. As a
consequence, the <code class="docutils literal notranslate"><span class="pre">stream_task&lt;&gt;</span></code> needs to be manipulated with the
<a class="reference internal" href="stf/lower_level_api.html"><span class="doc">low-level API</span></a>.</p>
</section>
<section id="combining-typed-and-untyped-tasks">
<h3>Combining typed and untyped tasks<a class="headerlink" href="#combining-typed-and-untyped-tasks" title="Link to this heading">#</a></h3>
<p>It is possible to dynamically add dependencies to a typed task, but the
type of the task will not reflect the dynamically added dependencies.
This allows for combining the low-level API with the <code class="docutils literal notranslate"><span class="pre">-&gt;*</span></code> notation in
the following way:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">X</span><span class="p">[</span><span class="mi">16</span><span class="p">],</span><span class="w"> </span><span class="n">Y</span><span class="p">[</span><span class="mi">16</span><span class="p">];</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lY</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">Y</span><span class="p">);</span>

<span class="k">auto</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">());</span>
<span class="n">t</span><span class="p">.</span><span class="n">add_deps</span><span class="p">(</span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">());</span>
<span class="n">t</span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t</span><span class="p">.</span><span class="k">template</span><span class="w"> </span><span class="n">get</span><span class="o">&lt;</span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="p">};</span>
</pre></div>
</div>
<p>The program remains safe because accesses are checked dynamically.
However, any errors will be caught at runtime instead of during
compilation.</p>
<p>Untyped tasks cannot be converted to typed tasks. On the other hand,
typed tasks can be converted implicitly to untyped tasks (thus losing
all the benefits of statically available types):</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">stream_task</span><span class="o">&lt;&gt;</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">());</span>
</pre></div>
</div>
</section>
</section>
<section id="modular-use-of-cudastf">
<h2><a class="toc-backref" href="#id28" role="doc-backlink">Modular use of CUDASTF</a><a class="headerlink" href="#modular-use-of-cudastf" title="Link to this heading">#</a></h2>
<p>CUDASTF maintains data consistency throughout a context and infers
concurrency opportunities based on data accesses. An existing application
may however already manage coherency or enforce dependencies by other means.
CUDASTF offers several facilities that facilitate incremental adoption
in existing code.</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">logical</span> <span class="pre">data</span> <span class="pre">freezing</span></code> mechanism ensures data availability while letting
the application take care of synchronization.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Tokens</span></code> make it possible to enforce concurrent execution while
letting the application manage data allocations and data transfers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Execution</span> <span class="pre">places</span></code> can be used without tasks for example to automate the management of CUDA streams, or set the current execution context.</p></li>
</ul>
<section id="freezing-logical-data">
<h3>Freezing logical data<a class="headerlink" href="#freezing-logical-data" title="Link to this heading">#</a></h3>
<p>When a piece of data is used very often, it can be beneficial to avoid enforcing
data dependencies every time it is accessed. A common example would be data that
is written once and then read many times.</p>
<p>CUDASTF provides a mechanism called <code class="docutils literal notranslate"><span class="pre">logical</span> <span class="pre">data</span> <span class="pre">freeze</span></code> that allows a
logical data to be accessed outside of tasks—or within tasks—without
enforcing data dependencies for every access, which minimizes overhead.</p>
<p>By default, calling the <code class="docutils literal notranslate"><span class="pre">freeze</span></code> method returns a frozen logical data object
that can be accessed in read-only mode without additional synchronization. The
<code class="docutils literal notranslate"><span class="pre">get</span></code> method of the frozen logical data returns a view of the underlying data
on the specified data place. This view can be used asynchronously with respect
to the stream passed to <code class="docutils literal notranslate"><span class="pre">get</span></code> until calling the non-blocking <code class="docutils literal notranslate"><span class="pre">unfreeze</span></code>
method on the frozen logical data. It is possible to call <code class="docutils literal notranslate"><span class="pre">get</span></code> multiple times.
Modifying these frozen read-only views results in undefined behavior.
If necessary, implicit data transfers or allocations are performed asynchronously
when calling <code class="docutils literal notranslate"><span class="pre">get</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">frozen_ld</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">ld</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">dX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">frozen_ld</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">data_place</span><span class="o">::</span><span class="n">current_device</span><span class="p">(),</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="p">...,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">dX</span><span class="p">);</span>

<span class="c1">// Get a read-only copy of the frozen data on other data places</span>
<span class="k">auto</span><span class="w"> </span><span class="n">dX1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">frozen_ld</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">data_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">hX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">frozen_ld</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">data_place</span><span class="o">::</span><span class="n">host</span><span class="p">(),</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>

<span class="n">frozen_ld</span><span class="p">.</span><span class="n">unfreeze</span><span class="p">(</span><span class="n">stream</span><span class="p">);</span>
</pre></div>
</div>
<p>While data are frozen, it is still possible to launch tasks which access
them. CUDASTF will allow tasks with a read access modes to run
concurrently before <code class="docutils literal notranslate"><span class="pre">unfreeze</span></code> is called, but it will defer write accesses
until data is made is made modifiable again, after <code class="docutils literal notranslate"><span class="pre">unfreeze</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">frozen_ld</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">ld</span><span class="p">,</span><span class="w"> </span><span class="n">access_mode</span><span class="o">::</span><span class="n">rw</span><span class="p">,</span><span class="w"> </span><span class="n">data_place</span><span class="o">::</span><span class="n">current_device</span><span class="p">());</span>
<span class="k">auto</span><span class="w"> </span><span class="n">dX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">frozen_ld</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">data_place</span><span class="o">::</span><span class="n">current_device</span><span class="p">(),</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>
<span class="c1">// kernel can modify dX</span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="p">...,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">dX</span><span class="p">);</span>
<span class="n">frozen_ld</span><span class="p">.</span><span class="n">unfreeze</span><span class="p">(</span><span class="n">stream</span><span class="p">);</span>
</pre></div>
</div>
<p>As shown above, it is also possible to create a modifiable frozen logical data,
allowing an application to temporarily transfer ownership of the logical data
to code that does not use tasks.  Because no further synchronization is
performed to ensure the consistency of this logical data once it is frozen,
users need to specify where the view of the data is needed.  Any tasks that
access this modifiable frozen logical data will be deferred until <code class="docutils literal notranslate"><span class="pre">unfreeze</span></code>
is called.</p>
<p>It is not possible to freeze the same logical data concurrently. Therefore, we
need to call <code class="docutils literal notranslate"><span class="pre">unfreeze</span></code> before calling <code class="docutils literal notranslate"><span class="pre">freeze</span></code> again, and it is the
programmer’s responsibility to ensure that the stream passed to <code class="docutils literal notranslate"><span class="pre">freeze</span></code>
depends on the completions of all operations in the stream previously passed to
<code class="docutils literal notranslate"><span class="pre">unfreeze</span></code>.</p>
<p>It is possible to use different streams in the <code class="docutils literal notranslate"><span class="pre">freeze</span></code>, <code class="docutils literal notranslate"><span class="pre">get</span></code> and
<code class="docutils literal notranslate"><span class="pre">unfreeze</span></code> methods. However it is also programmer’s responsibility to ensure
that the stream passed to <code class="docutils literal notranslate"><span class="pre">get</span></code> depends on the completion of the work in the
stream passed to <code class="docutils literal notranslate"><span class="pre">freeze</span></code> (for example, by using a blocking call such as
<code class="docutils literal notranslate"><span class="pre">cudaStreamSynchronize</span></code>). Similarly, the stream passed to <code class="docutils literal notranslate"><span class="pre">unfreeze</span></code> must
depend on the completion of the work in the streams used for any preceding
<code class="docutils literal notranslate"><span class="pre">freeze</span></code> and <code class="docutils literal notranslate"><span class="pre">get</span></code> calls.</p>
<p>It is possible to retrieve the access mode used to freeze a logical data with
the <code class="docutils literal notranslate"><span class="pre">get_access_mode()</span></code> method of the <code class="docutils literal notranslate"><span class="pre">frozen_logical_data</span></code> object.</p>
</section>
<section id="tokens">
<h3>Tokens<a class="headerlink" href="#tokens" title="Link to this heading">#</a></h3>
<p>A token is a specific type of logical data whose only purpose is to
automate synchronization, while letting the application manage the actual data.
This can, for example, be useful with user-provided buffers on a single device,
where no allocations or transfers are required, but where concurrent accesses
may occur.</p>
<p>A token internally relies on the <code class="docutils literal notranslate"><span class="pre">void_interface</span></code> data interface,
which is specifically optimized to skip unnecessary stages in the cache
coherency protocol (e.g., data allocations or copying data). When appropriate,
using a token rather than a logical data with a full-fledged data
interface therefore minimizes runtime overhead.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">token</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">token</span><span class="p">();</span>

<span class="c1">// A and B are assumed to be two other valid logical data</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">token</span><span class="p">.</span><span class="n">rw</span><span class="p">(),</span><span class="w"> </span><span class="n">A</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">B</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">};</span>
</pre></div>
</div>
<p>The example above shows how to create a token and how to use it in a
task.</p>
<p>Since the token is only used for synchronization purposes, the
corresponding argument may be omitted in the lambda function passed as the
task’s implementation. Thus, the above task is equivalent to this code:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">token</span><span class="p">.</span><span class="n">rw</span><span class="p">(),</span><span class="w"> </span><span class="n">A</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">B</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="n">void_interface</span><span class="w"> </span><span class="n">dummy</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
<p>To avoid ambiguities, you must either consistently ignore every
<code class="docutils literal notranslate"><span class="pre">void_interface</span></code> data instance or include them all, even if they remain
unused. Eliding these token arguments is possible in the <code class="docutils literal notranslate"><span class="pre">ctx.task</span></code> and
<code class="docutils literal notranslate"><span class="pre">ctx.host_launch</span></code> constructs.</p>
<p>Note that the token created by the <code class="docutils literal notranslate"><span class="pre">token</span></code> method of the context
object is already valid, which means the first access can be either a <code class="docutils literal notranslate"><span class="pre">read()</span></code>
or an <code class="docutils literal notranslate"><span class="pre">rw()</span></code> access. There is no need to set any content in the token
(unlike a logical data object created from a shape).</p>
<p>A token corresponds to a <code class="docutils literal notranslate"><span class="pre">logical_data&lt;void_interface&gt;</span></code> object, so that the
<code class="docutils literal notranslate"><span class="pre">token</span></code> type serves as a short-hand for this type. <code class="docutils literal notranslate"><span class="pre">ctx.token()</span></code> thus
returns an object with a <code class="docutils literal notranslate"><span class="pre">token</span></code> type.</p>
</section>
<section id="stream-management-with-execution-places">
<h3>Stream management with execution places<a class="headerlink" href="#stream-management-with-execution-places" title="Link to this heading">#</a></h3>
<p>CUDASTF’s execution places can be used independently of the task system to
manage CUDA streams in a structured way. This is useful when you want to use
CUDASTF’s place abstractions (devices, green contexts) for stream management
without the full task-based programming model.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">exec_place::pick_stream</span></code> method returns a CUDA stream from the stream
pool associated with a specific execution place. To use this facility, you need
an <code class="docutils literal notranslate"><span class="pre">async_resources_handle</span></code> object, which manages the underlying stream pools.</p>
<p>The method accepts an optional <code class="docutils literal notranslate"><span class="pre">for_computation</span></code> hint (defaults to <code class="docutils literal notranslate"><span class="pre">true</span></code>)
that may select between computation and data transfer stream pools to improve
overlapping. This is purely a performance hint, and it does not affect correctness. Not all execution places enforce
it.</p>
<p><strong>Using execution places without a context:</strong></p>
<p>When using execution places without a CUDASTF context, create a standalone
<code class="docutils literal notranslate"><span class="pre">async_resources_handle</span></code>:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cuda/experimental/stf.cuh&gt;</span>
<span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">cuda</span><span class="o">::</span><span class="nn">experimental</span><span class="o">::</span><span class="nn">stf</span><span class="p">;</span>

<span class="c1">// Create an async_resources_handle (manages stream pools)</span>
<span class="n">async_resources_handle</span><span class="w"> </span><span class="n">resources</span><span class="p">;</span>

<span class="c1">// Get a stream from the current device</span>
<span class="n">exec_place</span><span class="w"> </span><span class="n">place</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exec_place</span><span class="o">::</span><span class="n">current_device</span><span class="p">();</span>
<span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">place</span><span class="p">.</span><span class="n">pick_stream</span><span class="p">(</span><span class="n">resources</span><span class="p">);</span>

<span class="c1">// Use the stream for CUDA operations</span>
<span class="n">myKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_data</span><span class="p">);</span>

<span class="c1">// Get streams from specific devices</span>
<span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream_dev0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">pick_stream</span><span class="p">(</span><span class="n">resources</span><span class="p">);</span>
<span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream_dev1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">pick_stream</span><span class="p">(</span><span class="n">resources</span><span class="p">);</span>
</pre></div>
</div>
<p>Stream pools are populated lazily—CUDA streams are only created when first
requested via <code class="docutils literal notranslate"><span class="pre">pick_stream()</span></code>.</p>
<p><strong>Using execution places alongside a context:</strong></p>
<p>When working alongside a CUDASTF context, use <code class="docutils literal notranslate"><span class="pre">ctx.async_resources()</span></code> to
ensure the same stream pools are shared between your code and the context’s
internal operations:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">stream_ctx</span><span class="w"> </span><span class="n">ctx</span><span class="p">;</span>

<span class="c1">// Get a stream using the context&#39;s async_resources</span>
<span class="n">exec_place</span><span class="w"> </span><span class="n">place</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">place</span><span class="p">.</span><span class="n">pick_stream</span><span class="p">(</span><span class="n">ctx</span><span class="p">.</span><span class="n">async_resources</span><span class="p">());</span>

<span class="c1">// This stream comes from the same pool used by the context internally.</span>
<span class="c1">// ctx.pick_stream() is a shorthand that uses the default execution place</span>
<span class="c1">// for the calling thread.</span>
<span class="n">cudaStream_t</span><span class="w"> </span><span class="n">ctx_stream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">pick_stream</span><span class="p">();</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">finalize</span><span class="p">();</span>
</pre></div>
</div>
<p><strong>Getting multiple streams:</strong></p>
<p>You can query the pool size and retrieve all streams as a vector:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">async_resources_handle</span><span class="w"> </span><span class="n">resources</span><span class="p">;</span>
<span class="n">exec_place</span><span class="w"> </span><span class="n">place</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exec_place</span><span class="o">::</span><span class="n">current_device</span><span class="p">();</span>

<span class="c1">// Query the number of streams in the pool</span>
<span class="kt">size_t</span><span class="w"> </span><span class="n">pool_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">place</span><span class="p">.</span><span class="n">stream_pool_size</span><span class="p">(</span><span class="n">resources</span><span class="p">);</span>

<span class="c1">// Get all streams from the pool as a vector</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">cudaStream_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">streams</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">place</span><span class="p">.</span><span class="n">pick_all_streams</span><span class="p">(</span><span class="n">resources</span><span class="p">);</span>

<span class="c1">// Use the streams for concurrent operations</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">streams</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">myKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">streams</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_data</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="setting-the-current-device-or-context">
<h3>Setting the current device or context<a class="headerlink" href="#setting-the-current-device-or-context" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">exec_place::activate()</span></code> method provides a generic alternative to
<code class="docutils literal notranslate"><span class="pre">cudaSetDevice()</span></code> that works uniformly across different execution place types.
This is useful when you want to set the current CUDA device or context without
using CUDASTF tasks.</p>
<p>The method returns an <code class="docutils literal notranslate"><span class="pre">exec_place</span></code> representing the previous state, which can
be used to restore the original device or context.</p>
<p><strong>Behavior by execution place type:</strong></p>
<ul class="simple">
<li><p><strong>Device places</strong> (<code class="docutils literal notranslate"><span class="pre">exec_place::device(id)</span></code>): Calls <code class="docutils literal notranslate"><span class="pre">cudaSetDevice(id)</span></code></p></li>
<li><p><strong>Green context places</strong>: Sets the current CUDA driver context via <code class="docutils literal notranslate"><span class="pre">cuCtxSetCurrent()</span></code></p></li>
<li><p><strong>Host places</strong>: No-op</p></li>
</ul>
<p><strong>Basic usage with devices:</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">exec_place</span><span class="w"> </span><span class="n">place</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="n">exec_place</span><span class="w"> </span><span class="n">prev</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">place</span><span class="p">.</span><span class="n">activate</span><span class="p">();</span><span class="w">  </span><span class="c1">// Switch to device 1</span>

<span class="c1">// ... perform operations on device 1 ...</span>

<span class="n">place</span><span class="p">.</span><span class="n">deactivate</span><span class="p">(</span><span class="n">prev</span><span class="p">);</span><span class="w">  </span><span class="c1">// Restore previous device</span>
</pre></div>
</div>
<p><strong>Alternative restoration pattern:</strong></p>
<p>You can also restore by calling <code class="docutils literal notranslate"><span class="pre">activate()</span></code> on the returned place:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">exec_place</span><span class="w"> </span><span class="n">place</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="n">exec_place</span><span class="w"> </span><span class="n">prev</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">place</span><span class="p">.</span><span class="n">activate</span><span class="p">();</span>

<span class="c1">// ... work on device 1 ...</span>

<span class="n">prev</span><span class="p">.</span><span class="n">activate</span><span class="p">();</span><span class="w">  </span><span class="c1">// Equivalent to place.deactivate(prev)</span>
</pre></div>
</div>
<p><strong>Usage with green contexts (CUDA 12.4+):</strong></p>
<p>Green contexts provide SM-level partitioning of GPU resources. The
<code class="docutils literal notranslate"><span class="pre">activate()</span></code>/<code class="docutils literal notranslate"><span class="pre">deactivate()</span></code> methods handle the underlying driver context
management:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create green contexts with 8 SMs each</span>
<span class="n">green_context_helper</span><span class="w"> </span><span class="nf">gc</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="n">device_id</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">view</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gc</span><span class="p">.</span><span class="n">get_view</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>

<span class="n">exec_place</span><span class="w"> </span><span class="n">gc_place</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exec_place</span><span class="o">::</span><span class="n">green_ctx</span><span class="p">(</span><span class="n">view</span><span class="p">);</span>
<span class="n">exec_place</span><span class="w"> </span><span class="n">prev</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gc_place</span><span class="p">.</span><span class="n">activate</span><span class="p">();</span><span class="w">  </span><span class="c1">// Sets green context as current</span>

<span class="c1">// ... GPU work runs with SM affinity ...</span>

<span class="n">gc_place</span><span class="p">.</span><span class="n">deactivate</span><span class="p">(</span><span class="n">prev</span><span class="p">);</span><span class="w">  </span><span class="c1">// Restore original context</span>
</pre></div>
</div>
<p><strong>RAII guard for scoped activation:</strong></p>
<p>For exception-safe code or when you want automatic restoration, use the
<code class="docutils literal notranslate"><span class="pre">exec_place_guard</span></code> RAII helper:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="n">exec_place_guard</span><span class="w"> </span><span class="nf">guard</span><span class="p">(</span><span class="n">exec_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>
<span class="w">    </span><span class="c1">// Device 1 is now active</span>
<span class="w">    </span><span class="c1">// ... perform operations on device 1 ...</span>
<span class="p">}</span>
<span class="c1">// Previous device is automatically restored when guard goes out of scope</span>
</pre></div>
</div>
<p>The guard automatically restores the previous execution place when it goes out
of scope, making it useful for exception-safe code.</p>
</section>
<section id="memory-allocation-with-data-places">
<h3>Memory allocation with data places<a class="headerlink" href="#memory-allocation-with-data-places" title="Link to this heading">#</a></h3>
<p>Data places provide a unified interface for memory allocation that works across
different memory types (host, device, managed) and place extensions (green
contexts, user-defined places). This allows you to allocate memory without using
CUDASTF tasks, while benefiting from the place abstraction.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">data_place::allocate()</span></code> and <code class="docutils literal notranslate"><span class="pre">data_place::deallocate()</span></code> methods provide
raw memory allocation. The stream parameter defaults to <code class="docutils literal notranslate"><span class="pre">nullptr</span></code>, which is
convenient for non-stream-ordered allocations (host, managed) where the stream
is ignored:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cuda/experimental/stf.cuh&gt;</span>
<span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">cuda</span><span class="o">::</span><span class="nn">experimental</span><span class="o">::</span><span class="nn">stf</span><span class="p">;</span>

<span class="c1">// Allocate on host (pinned memory) - stream defaults to nullptr</span>
<span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">host_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_place</span><span class="o">::</span><span class="n">host</span><span class="p">().</span><span class="n">allocate</span><span class="p">(</span><span class="mi">1024</span><span class="p">);</span>
<span class="c1">// ... use host_ptr ...</span>
<span class="n">data_place</span><span class="o">::</span><span class="n">host</span><span class="p">().</span><span class="n">deallocate</span><span class="p">(</span><span class="n">host_ptr</span><span class="p">,</span><span class="w"> </span><span class="mi">1024</span><span class="p">);</span>

<span class="c1">// Allocate on a specific device (stream-ordered)</span>
<span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">;</span>
<span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">);</span>
<span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">dev_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">allocate</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>
<span class="c1">// ... use dev_ptr with stream ...</span>
<span class="n">data_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">deallocate</span><span class="p">(</span><span class="n">dev_ptr</span><span class="p">,</span><span class="w"> </span><span class="mi">1024</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>
<span class="n">cudaStreamDestroy</span><span class="p">(</span><span class="n">stream</span><span class="p">);</span>

<span class="c1">// Allocate managed memory - stream defaults to nullptr</span>
<span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">managed_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_place</span><span class="o">::</span><span class="n">managed</span><span class="p">().</span><span class="n">allocate</span><span class="p">(</span><span class="mi">1024</span><span class="p">);</span>
<span class="c1">// ... use managed_ptr from host or device ...</span>
<span class="n">data_place</span><span class="o">::</span><span class="n">managed</span><span class="p">().</span><span class="n">deallocate</span><span class="p">(</span><span class="n">managed_ptr</span><span class="p">,</span><span class="w"> </span><span class="mi">1024</span><span class="p">);</span>
</pre></div>
</div>
<p><strong>Stream-ordered vs immediate allocations:</strong></p>
<p>Different data places have different allocation behaviors:</p>
<ul class="simple">
<li><p><strong>Host</strong> (<code class="docutils literal notranslate"><span class="pre">data_place::host()</span></code>): Uses <code class="docutils literal notranslate"><span class="pre">cudaMallocHost()</span></code> / <code class="docutils literal notranslate"><span class="pre">cudaFreeHost()</span></code> - immediate, stream parameter is ignored</p></li>
<li><p><strong>Managed</strong> (<code class="docutils literal notranslate"><span class="pre">data_place::managed()</span></code>): Uses <code class="docutils literal notranslate"><span class="pre">cudaMallocManaged()</span></code> / <code class="docutils literal notranslate"><span class="pre">cudaFree()</span></code> - immediate, stream parameter is ignored (note: <code class="docutils literal notranslate"><span class="pre">cudaFree</span></code> may introduce implicit synchronization)</p></li>
<li><p><strong>Device</strong> (<code class="docutils literal notranslate"><span class="pre">data_place::device(id)</span></code>): Uses <code class="docutils literal notranslate"><span class="pre">cudaMallocAsync()</span></code> / <code class="docutils literal notranslate"><span class="pre">cudaFreeAsync()</span></code> - stream-ordered</p></li>
<li><p><strong>Extensions</strong> (green contexts, etc.): Behavior depends on the extension implementation</p></li>
</ul>
<p>You can query whether a place uses stream-ordered allocation with
<code class="docutils literal notranslate"><span class="pre">allocation_is_stream_ordered()</span></code>:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">data_place</span><span class="w"> </span><span class="n">place</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">place</span><span class="p">.</span><span class="n">allocation_is_stream_ordered</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Allocation is stream-ordered - synchronize via the stream</span>
<span class="w">    </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">place</span><span class="p">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>
<span class="w">    </span><span class="n">myKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">ptr</span><span class="p">);</span>
<span class="w">    </span><span class="n">place</span><span class="p">.</span><span class="n">deallocate</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaStreamSynchronize</span><span class="p">(</span><span class="n">stream</span><span class="p">);</span>
<span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Allocation is immediate - stream is ignored, safe to use right away</span>
<span class="w">    </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">place</span><span class="p">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">size</span><span class="p">);</span>
<span class="w">    </span><span class="c1">// ... use ptr ...</span>
<span class="w">    </span><span class="n">place</span><span class="p">.</span><span class="n">deallocate</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This abstraction is particularly useful when writing generic code that needs to
work with different types of places, including custom place extensions.</p>
</section>
<section id="vmm-based-allocation-with-mem-create">
<h3>VMM-based allocation with mem_create<a class="headerlink" href="#vmm-based-allocation-with-mem-create" title="Link to this heading">#</a></h3>
<p>For advanced use cases involving CUDA’s Virtual Memory Management (VMM) API,
<code class="docutils literal notranslate"><span class="pre">data_place</span></code> also provides the <code class="docutils literal notranslate"><span class="pre">mem_create()</span></code> method. This is a lower-level
interface used internally by localized arrays (<code class="docutils literal notranslate"><span class="pre">composite_slice</span></code>) to create
physical memory segments that are then mapped into a contiguous virtual address
space.</p>
<p>Unlike <code class="docutils literal notranslate"><span class="pre">allocate()</span></code>, which returns a usable pointer directly, <code class="docutils literal notranslate"><span class="pre">mem_create()</span></code>
returns a <code class="docutils literal notranslate"><span class="pre">CUmemGenericAllocationHandle</span></code> that must be subsequently mapped with
<code class="docutils literal notranslate"><span class="pre">cuMemMap()</span></code> before use:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cuda/experimental/stf.cuh&gt;</span>
<span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">cuda</span><span class="o">::</span><span class="nn">experimental</span><span class="o">::</span><span class="nn">stf</span><span class="p">;</span>

<span class="c1">// Create a physical memory handle for device 0</span>
<span class="n">CUmemGenericAllocationHandle</span><span class="w"> </span><span class="n">handle</span><span class="p">;</span>
<span class="n">data_place</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">mem_create</span><span class="p">(</span><span class="o">&amp;</span><span class="n">handle</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>

<span class="c1">// The handle must be mapped to a virtual address before use</span>
<span class="c1">// (see CUDA VMM documentation for cuMemMap, cuMemSetAccess, etc.)</span>
</pre></div>
</div>
<p><strong>When to use each method:</strong></p>
<ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">allocate()</span></code> for most cases - it provides ready-to-use memory with
stream-ordered semantics where applicable.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">mem_create()</span></code> only when you need explicit control over virtual memory
mapping, such as creating localized arrays that span multiple devices with a
unified virtual address space.</p></li>
</ul>
<p><strong>Limitations of mem_create:</strong></p>
<ul class="simple">
<li><p>Only supports device memory and host memory (pinned)</p></li>
<li><p>Managed memory is <strong>not supported</strong> by the VMM API</p></li>
<li><p>The returned handle requires additional VMM API calls to be usable</p></li>
</ul>
<p>Custom place extensions can override <code class="docutils literal notranslate"><span class="pre">mem_create()</span></code> to provide specialized
VMM allocation behavior (e.g., memory localization for hardware partitions).</p>
</section>
</section>
<section id="debugging">
<h2><a class="toc-backref" href="#id29" role="doc-backlink">Debugging</a><a class="headerlink" href="#debugging" title="Link to this heading">#</a></h2>
<section id="enabling-internal-checks">
<h3>Enabling internal checks<a class="headerlink" href="#enabling-internal-checks" title="Link to this heading">#</a></h3>
<p>CUDASTF includes internal assertions (<code class="docutils literal notranslate"><span class="pre">_CCCL_ASSERT</span></code>) that help detect
programming errors and invalid usage patterns during development. These checks
are disabled by default for performance but can be enabled to aid debugging.</p>
<p><strong>With CMake:</strong></p>
<p>When building in Debug mode, assertions are enabled automatically:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cmake<span class="w"> </span>-DCMAKE_BUILD_TYPE<span class="o">=</span>Debug<span class="w"> </span>..
</pre></div>
</div>
<p>To explicitly enable assertions for any build type, add the compile definition
to your target:</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">target_compile_definitions</span><span class="p">(</span><span class="s">your_target</span><span class="w"> </span><span class="s">PRIVATE</span><span class="w"> </span><span class="s">CCCL_ENABLE_ASSERTIONS</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>With Makefile or manual compilation:</strong></p>
<p>Add the <code class="docutils literal notranslate"><span class="pre">-DCCCL_ENABLE_ASSERTIONS</span></code> flag to your compiler invocation:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># For nvcc</span>
nvcc<span class="w"> </span>-DCCCL_ENABLE_ASSERTIONS<span class="w"> </span>...

<span class="c1"># For host compiler</span>
g++<span class="w"> </span>-DCCCL_ENABLE_ASSERTIONS<span class="w"> </span>...
</pre></div>
</div>
<p>Note that this flag enables the assertion checks themselves. For full debugging
support (setting breakpoints, inspecting variables), you may also want to add
debug symbol flags (<code class="docutils literal notranslate"><span class="pre">-g</span></code> for host code, <code class="docutils literal notranslate"><span class="pre">-G</span></code> for device code).</p>
</section>
</section>
<section id="tools">
<h2><a class="toc-backref" href="#id30" role="doc-backlink">Tools</a><a class="headerlink" href="#tools" title="Link to this heading">#</a></h2>
<section id="visualizing-task-graphs">
<h3>Visualizing task graphs<a class="headerlink" href="#visualizing-task-graphs" title="Link to this heading">#</a></h3>
<p>In order to visualize the task graph generated by CUDASTF, it is
possible to generate a file in the Graphviz format. This visualization
helps to better understand the application, and can be helpful to
optimize the algorithms as it sometimes allow to identify inefficient
patterns.</p>
<section id="generating-visualizations-of-task-graphs">
<h4>Generating visualizations of task graphs<a class="headerlink" href="#generating-visualizations-of-task-graphs" title="Link to this heading">#</a></h4>
<p>Let us consider the <code class="docutils literal notranslate"><span class="pre">examples/01-axpy.cu</span></code> example which we compile as
usual with <code class="docutils literal notranslate"><span class="pre">make</span> <span class="pre">build/examples/01-axpy</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run the application with CUDASTF_DOT_FILE set to the filename</span>
<span class="nv">CUDASTF_DOT_FILE</span><span class="o">=</span>axpy.dot<span class="w"> </span>build/examples/01-axpy

<span class="c1"># Generate the visualization from this dot file</span>
<span class="c1">## PDF format</span>
dot<span class="w"> </span>-Tpdf<span class="w"> </span>axpy.dot<span class="w"> </span>-o<span class="w"> </span>axpy.pdf
<span class="c1">## PNG format</span>
dot<span class="w"> </span>-Tpng<span class="w"> </span>axpy.dot<span class="w"> </span>-o<span class="w"> </span>axpy.png
</pre></div>
</div>
<p>We obtain a visualization like this, where we only see a single task
with little :</p>
<img alt="../_images/dot-output-axpy.png" src="../_images/dot-output-axpy.png" />
<p>To have more information, we can enhance the application with some extra
debugging information. For example, we can specify what is the name of a
logical data using the <code class="docutils literal notranslate"><span class="pre">set_symbol</span></code> method of the <code class="docutils literal notranslate"><span class="pre">logical_data</span></code>
class. As illustrated here :</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">X</span><span class="p">).</span><span class="n">set_symbol</span><span class="p">(</span><span class="s">&quot;X&quot;</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lY</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">Y</span><span class="p">);</span>
<span class="n">lY</span><span class="p">.</span><span class="n">set_symbol</span><span class="p">(</span><span class="s">&quot;Y&quot;</span><span class="p">);</span>
</pre></div>
</div>
<p>We can also annotate tasks with symbols. Instead of writing this :</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dY</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">axpy</span><span class="o">&lt;&lt;&lt;</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="n">dY</span><span class="p">);</span><span class="w"> </span><span class="p">};</span>
</pre></div>
</div>
<p>We can write code like this :</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Inlined notation</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">()).</span><span class="n">set_symbol</span><span class="p">(</span><span class="s">&quot;axpy&quot;</span><span class="p">)</span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dY</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">axpy</span><span class="o">&lt;&lt;&lt;</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="n">dY</span><span class="p">);</span><span class="w"> </span><span class="p">};</span>

<span class="c1">// Explicit manipulation of the task class</span>
<span class="k">auto</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">());</span>
<span class="n">t</span><span class="p">.</span><span class="n">set_symbol</span><span class="p">(</span><span class="s">&quot;axpy&quot;</span><span class="p">);</span>
<span class="n">t</span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dY</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">axpy</span><span class="o">&lt;&lt;&lt;</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="n">dY</span><span class="p">);</span><span class="w"> </span><span class="p">};</span>
</pre></div>
</div>
<p>We then obtain the following output, which contains more interesting
annotations :</p>
<img alt="../_images/dot-output-axpy-annotated.png" src="../_images/dot-output-axpy-annotated.png" />
<p>On a more elaborated application, such as the <code class="docutils literal notranslate"><span class="pre">examples/heat_mgpu.cu</span></code>
example, we can easily understand the overall workflow thanks to this
visualization.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDASTF_DOT_FILE</span><span class="o">=</span>heat.dot<span class="w"> </span>build/examples/heat_mgpu<span class="w"> </span><span class="m">1000</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">4</span>
dot<span class="w"> </span>-Tpng<span class="w"> </span>heat.dot<span class="w"> </span>-o<span class="w"> </span>heat.png
</pre></div>
</div>
<img alt="../_images/dot-output-heat.png" src="../_images/dot-output-heat.png" />
<p>For advanced users, it is also possible to display internally generated
asynchronous operations by setting the <code class="docutils literal notranslate"><span class="pre">CUDASTF_DOT_IGNORE_PREREQS</span></code>
environment variable to 0.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">CUDASTF_DOT_IGNORE_PREREQS</span><span class="o">=</span><span class="mi">0</span><span class="w"> </span><span class="n">CUDASTF_DOT_FILE</span><span class="o">=</span><span class="n">axpy</span><span class="o">-</span><span class="n">with</span><span class="o">-</span><span class="n">events</span><span class="p">.</span><span class="n">dot</span><span class="w"> </span><span class="n">build</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="mo">01</span><span class="o">-</span><span class="n">axpy</span>
<span class="n">dot</span><span class="w"> </span><span class="o">-</span><span class="n">Tpng</span><span class="w"> </span><span class="n">axpy</span><span class="o">-</span><span class="n">with</span><span class="o">-</span><span class="n">events</span><span class="p">.</span><span class="n">dot</span><span class="w"> </span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">axpy</span><span class="o">-</span><span class="n">with</span><span class="o">-</span><span class="n">events</span><span class="p">.</span><span class="n">png</span>
</pre></div>
</div>
<img alt="../_images/dot-output-axpy-events.png" src="../_images/dot-output-axpy-events.png" />
<p>It is possible to color the different tasks accordingly to the device
executing it by setting the <code class="docutils literal notranslate"><span class="pre">CUDASTF_DOT_COLOR_BY_DEVICE</span></code> environment
variable.</p>
<p>To reduce the amount of information displayed in the graph, we can
remove the list of data associated to each task by setting the
<code class="docutils literal notranslate"><span class="pre">CUDASTF_DOT_REMOVE_DATA_DEPS</span></code> environment variable.</p>
<p>It is also possible to include timing information in this graph by setting the
<code class="docutils literal notranslate"><span class="pre">CUDASTF_DOT_TIMING</span></code> environment variable to a non-null value. This will
color the graph nodes according to their relative duration, and the measured
duration will be included in task labels.</p>
</section>
<section id="condensed-and-structured-graphs-visualization">
<h4>Condensed and structured graphs visualization<a class="headerlink" href="#condensed-and-structured-graphs-visualization" title="Link to this heading">#</a></h4>
<p>Realistic workloads are typically made of thousands or millions of tasks which
cannot be easily visualized using graphviz (dot). To simplify the generated
graphs we can further
annotate the application using dot sections.
Dot sections can also be nested to better structure the visualization.</p>
<p>This is achieved by creating <cite>dot_section</cite> objects in the application. <cite>ctx.dot_section</cite> returns
an object whose lifetime defines a dot section valid until it is destroyed, or
when calling the <cite>end()</cite> method on this object. The following example
illustrates how to add nested sections:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">context</span><span class="w"> </span><span class="n">ctx</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lA</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">token</span><span class="p">().</span><span class="n">set_symbol</span><span class="p">(</span><span class="s">&quot;A&quot;</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lB</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">token</span><span class="p">().</span><span class="n">set_symbol</span><span class="p">(</span><span class="s">&quot;B&quot;</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lC</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">token</span><span class="p">().</span><span class="n">set_symbol</span><span class="p">(</span><span class="s">&quot;C&quot;</span><span class="p">);</span>

<span class="c1">// Begin a top-level section named &quot;foo&quot;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">s_foo</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">dot_section</span><span class="p">(</span><span class="s">&quot;foo&quot;</span><span class="p">);</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">  </span><span class="c1">// Section named &quot;bar&quot; using RAII</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">s_bar</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">dot_section</span><span class="p">(</span><span class="s">&quot;bar&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lA</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lB</span><span class="p">.</span><span class="n">rw</span><span class="p">()).</span><span class="n">set_symbol</span><span class="p">(</span><span class="s">&quot;t1&quot;</span><span class="p">)</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="p">)</span><span class="w"> </span><span class="p">{};</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">     </span><span class="c1">// Section named &quot;baz&quot; using RAII</span>
<span class="w">     </span><span class="k">auto</span><span class="w"> </span><span class="n">s_bar</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">dot_section</span><span class="p">(</span><span class="s">&quot;baz&quot;</span><span class="p">);</span>
<span class="w">     </span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lA</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lC</span><span class="p">.</span><span class="n">rw</span><span class="p">()).</span><span class="n">set_symbol</span><span class="p">(</span><span class="s">&quot;t2&quot;</span><span class="p">)</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="p">)</span><span class="w"> </span><span class="p">{};</span>
<span class="w">     </span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lB</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lC</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lA</span><span class="p">.</span><span class="n">rw</span><span class="p">()).</span><span class="n">set_symbol</span><span class="p">(</span><span class="s">&quot;t3&quot;</span><span class="p">)</span><span class="o">-&gt;*</span><span class="p">[](</span><span class="n">cudaStream_t</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="p">)</span><span class="w"> </span><span class="p">{};</span>
<span class="w">     </span><span class="c1">// Implicit end of section &quot;baz&quot;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="c1">// Implicit end of section &quot;bar&quot;</span>
<span class="p">}</span>
<span class="n">s_foo</span><span class="p">.</span><span class="n">end</span><span class="p">();</span><span class="w"> </span><span class="c1">// Explicit end of section &quot;foo&quot;</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">finalize</span><span class="p">();</span>
</pre></div>
</div>
<p>When running this with the <cite>CUDASTF_DOT_FILE</cite> environment variable for example
set to <cite>dag.dot</cite>, we observe that the graph produced by <cite>dot -Tpdf dag.dot -o
dag.pdf</cite> depicts these sections as dashed boxes.</p>
<img alt="../_images/dag-sections.png" src="../_images/dag-sections.png" />
<p>Adding sections also makes it possible to define a maximum depth for the
generated graphs by setting the <cite>CUDASTF_DOT_MAX_DEPTH</cite> environment variable.
When it is undefined, CUDASTF will display all tasks. Otherwise, if
<cite>CUDASTF_DOT_MAX_DEPTH</cite> is an integer value of <cite>i</cite> any sections and tasks which
nesting level is deeper than <cite>i</cite> will be collapsed.</p>
<p>When setting <cite>CUDASTF_DOT_MAX_DEPTH=2</cite>, the previous graph becomes:</p>
<img alt="../_images/dag-sections-2.png" src="../_images/dag-sections-2.png" />
<p>When setting <cite>CUDASTF_DOT_MAX_DEPTH=1</cite>, one additional level is collapsed:</p>
<img alt="../_images/dag-sections-1.png" src="../_images/dag-sections-1.png" />
<p>With <cite>CUDASTF_DOT_MAX_DEPTH=0</cite>, only the top-most tasks and sections are displayed:</p>
<img alt="../_images/dag-sections-0.png" src="../_images/dag-sections-0.png" />
<p>Note that <cite>CUDASTF_DOT_MAX_DEPTH</cite> and <cite>CUDASTF_DOT_TIMING</cite> can be used in
combination, and that the duration of a section corresponds to the duration of
all tasks in this sections.</p>
</section>
</section>
<section id="kernel-tuning-with-ncu">
<h3>Kernel tuning with ncu<a class="headerlink" href="#kernel-tuning-with-ncu" title="Link to this heading">#</a></h3>
<p>Users can analyze the performance of kernels generated using
<code class="docutils literal notranslate"><span class="pre">ctx.parallel_for</span></code> and <code class="docutils literal notranslate"><span class="pre">ctx.launch</span></code> using the <code class="docutils literal notranslate"><span class="pre">ncu</span></code> tool.</p>
<section id="naming-kernels">
<h4>Naming kernels<a class="headerlink" href="#naming-kernels" title="Link to this heading">#</a></h4>
<p>However, displayed kernel names would be hardly exploitable as they
would all have the same name. One possible work-around is to let <code class="docutils literal notranslate"><span class="pre">ncu</span></code>
rename kernels accordingly to <code class="docutils literal notranslate"><span class="pre">NVTX</span></code> annotations. To do so, a symbol
must be associated to the <code class="docutils literal notranslate"><span class="pre">ctx.parallel_for</span></code> and <code class="docutils literal notranslate"><span class="pre">ctx.launch</span></code>
constructs using the <code class="docutils literal notranslate"><span class="pre">set_symbol</span></code> method. In the following example, we
name the generated kernel “updateA” :</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="mi">128</span><span class="p">];</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lA</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">A</span><span class="p">);</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">parallel_for</span><span class="p">(</span><span class="n">lA</span><span class="p">.</span><span class="n">shape</span><span class="p">(),</span><span class="w"> </span><span class="n">lA</span><span class="p">.</span><span class="n">write</span><span class="p">()).</span><span class="n">set_symbol</span><span class="p">(</span><span class="s">&quot;updateA&quot;</span><span class="p">)</span><span class="o">-&gt;*</span><span class="p">[]</span><span class="w"> </span><span class="n">__device__</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">sA</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">A</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
</section>
<section id="example-with-miniweather">
<h4>Example with miniWeather<a class="headerlink" href="#example-with-miniweather" title="Link to this heading">#</a></h4>
<p>Kernel tuning should always be performed on optimized code :</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>make<span class="w"> </span>build/examples/miniweather
</pre></div>
</div>
<p>The following command will analyse the performance of kernels :</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ncu<span class="w"> </span>--section<span class="o">=</span>ComputeWorkloadAnalysis<span class="w"> </span>--print-nvtx-rename<span class="o">=</span>kernel<span class="w"> </span>--nvtx<span class="w"> </span>-o<span class="w"> </span>output<span class="w"> </span>build/examples/miniWeather
</pre></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">--print-nvtx-rename=kernel</span> <span class="pre">--nvtx</span></code> is used to name kernels
accordingly to <code class="docutils literal notranslate"><span class="pre">NVTX</span></code> traces (which are enabled by the <code class="docutils literal notranslate"><span class="pre">set_symbol</span></code>
API). Failing to do so would results in all kernel names being
<code class="docutils literal notranslate"><span class="pre">thrust::cuda_cub::core::_kernel_agent</span></code>. To properly display renamed
kernel names, users may have to set option
<code class="docutils literal notranslate"><span class="pre">Options-&gt;Report</span> <span class="pre">UI-&gt;NVTX</span> <span class="pre">Rename</span> <span class="pre">Mode</span></code> to a value equal to <code class="docutils literal notranslate"><span class="pre">Kernel</span></code>
or <code class="docutils literal notranslate"><span class="pre">All</span></code>.</p>
<p>Depending on machine configuration, users may also have to execute the
<code class="docutils literal notranslate"><span class="pre">ncu</span></code> command as root or to setup their machine accordingly. This is
for example required when the following message is appears during
<code class="docutils literal notranslate"><span class="pre">ncu</span></code> trace collection :</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">==</span><span class="n">ERROR</span><span class="o">==</span> <span class="n">ERR_NVGPUCTRPERM</span> <span class="o">-</span> <span class="n">The</span> <span class="n">user</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">have</span> <span class="n">permission</span> <span class="n">to</span> <span class="n">access</span> <span class="n">NVIDIA</span> <span class="n">GPU</span> <span class="n">Performance</span> <span class="n">Counters</span> <span class="n">on</span> <span class="n">the</span> <span class="n">target</span> <span class="n">device</span> <span class="mf">0.</span> <span class="n">For</span> <span class="n">instructions</span> <span class="n">on</span> <span class="n">enabling</span> <span class="n">permissions</span> <span class="ow">and</span> <span class="n">to</span> <span class="n">get</span> <span class="n">more</span> <span class="n">information</span> <span class="n">see</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">developer</span><span class="o">.</span><span class="n">nvidia</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">ERR_NVGPUCTRPERM</span>
</pre></div>
</div>
<p>The file generated by <code class="docutils literal notranslate"><span class="pre">ncu</span></code> can be opened using <code class="docutils literal notranslate"><span class="pre">ncu-ui</span></code> :</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ncu-ui<span class="w"> </span>output.ncu-rep
</pre></div>
</div>
<p>In this case, we can see that the kernel are named accordingly to the
symbols set in the tasks of the miniWeather examples : <img alt="image1" src="../_images/ncu-ui.png" /></p>
</section>
</section>
</section>
<section id="cudastf-reference-card">
<h2><a class="toc-backref" href="#id31" role="doc-backlink">CUDASTF Reference Card</a><a class="headerlink" href="#cudastf-reference-card" title="Link to this heading">#</a></h2>
<p>This section gives a brief overview of the CUDASTF API.</p>
<section id="id7">
<h3>Using CUDASTF<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>STF is a C++ header-only library which API is defined in the <cite>cuda::experimental::stf</cite> namespace.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cudastf/cudastf.h&gt;</span>

<span class="k">using</span><span class="w"> </span><span class="n">cuda</span><span class="o">::</span><span class="n">experimental</span><span class="o">::</span><span class="n">stf</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="creating-a-context">
<h3>Creating a Context<a class="headerlink" href="#creating-a-context" title="Link to this heading">#</a></h3>
<p>Contexts store the state of the CUDASTF library and are used as an entry point for all API calls. The <cite>finalize()</cite> method must be called upon completion.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">context</span><span class="w"> </span><span class="n">ctx</span><span class="p">;</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">finalize</span><span class="p">();</span>
</pre></div>
</div>
<p>By default, this relies on the <code class="docutils literal notranslate"><span class="pre">stream_ctx</span></code> back-end, but it is also possible to select at runtime a <code class="docutils literal notranslate"><span class="pre">graph_ctx</span></code> back-end :</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">context</span><span class="w"> </span><span class="n">ctx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">graph_ctx</span><span class="p">();</span>
</pre></div>
</div>
<p>Or one may statically select either backends :</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">stream_ctx</span><span class="w"> </span><span class="n">ctx</span><span class="p">;</span>
<span class="n">graph_ctx</span><span class="w"> </span><span class="n">ctx</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="id8">
<h3>Logical Data<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<section id="creating-a-logical-data">
<h4>Creating a Logical Data<a class="headerlink" href="#creating-a-logical-data" title="Link to this heading">#</a></h4>
<p>Purpose: Encapsulates data structures (e.g. arrays, slices) to be shared and accessed by tasks. Logical data represents the abstraction of data in the model.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create a logical data from an existing piece of data</span>
<span class="k">auto</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">data</span><span class="w"> </span><span class="n">view</span><span class="w"> </span><span class="p">[</span><span class="n">data_place</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_place</span><span class="o">::</span><span class="n">current_device</span><span class="p">()]);</span>
</pre></div>
</div>
<p>Examples:</p>
<ul class="simple">
<li><p>Describing an array</p></li>
</ul>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="n">X</span><span class="p">[</span><span class="mi">1024</span><span class="p">];</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Describing a vector of n elements of type T located at address <cite>addr</cite></p></li>
</ul>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">data_handle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">slice</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">addr</span><span class="w"> </span><span class="p">{</span><span class="n">n</span><span class="p">}));</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Describing a contiguous matrix of size (m, n)</p></li>
</ul>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">data_handle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">slice</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(</span><span class="n">addr</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="n">m</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">}));</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Describing a matrix of size (m, n) with a stride of ld elements</p></li>
</ul>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">data_handle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">slice</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(</span><span class="n">addr</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="n">m</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="n">ld</span><span class="p">}));</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Create a logical data from a shape</p></li>
</ul>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">shape</span><span class="p">);</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">shape_of</span><span class="o">&lt;</span><span class="n">slice</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="mi">1024</span><span class="p">));</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lY</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">shape</span><span class="p">());</span>
</pre></div>
</div>
</section>
</section>
<section id="id9">
<h3>Tasks<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<section id="data-dependency">
<h4>Data Dependency<a class="headerlink" href="#data-dependency" title="Link to this heading">#</a></h4>
<p>Purpose: Define how a logical data should be used in a task construct (and derivated constructs such as <cite>parallel_for</cite>, <cite>launch</cite>, <cite>host_launch</cite>).</p>
<p>Syntax:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">logicalData</span><span class="p">.</span><span class="n">accessMode</span><span class="p">([</span><span class="n">data</span><span class="w"> </span><span class="n">place</span><span class="p">])</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Data Places</strong>: Specify where a logical data in the data dependencies should be located:</p>
<ul>
<li><p><cite>data_place::affine()</cite> (default): Locate data on the data place affine to the execution place (e.g., device memory when running on a CUDA device).</p></li>
<li><p><cite>data_place::managed()</cite>: Use managed memory.</p></li>
<li><p><cite>data_place::device(i)</cite>: Put data in the memory of the i-th CUDA device (which may be different from the current device or the device of the execution place).</p></li>
</ul>
</li>
<li><p><strong>Access Modes</strong>:</p>
<ul>
<li><p><cite>.read()</cite>: Read-only access.</p></li>
<li><p><cite>.write()</cite>: Write-only access.</p></li>
<li><p><cite>.rw()</cite>: Read and write access.</p></li>
</ul>
</li>
</ul>
</section>
<section id="task-creation">
<h4>Task Creation<a class="headerlink" href="#task-creation" title="Link to this heading">#</a></h4>
<p>Purpose: Define computational tasks that operate on logical data. Tasks can specify data dependencies and access modes.</p>
<p>Syntax:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">([</span><span class="n">execution</span><span class="w"> </span><span class="n">place</span><span class="p">]</span><span class="w"> </span><span class="n">dependency1</span><span class="w"> </span><span class="n">dependency2</span><span class="w"> </span><span class="p">...)</span>
<span class="w">    </span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">data1</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">data2</span><span class="w"> </span><span class="p">...)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Task implementation using stream</span>
<span class="w">    </span><span class="p">};</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Execution Place</strong>: Specify where the task should be executed:</p>
<ul>
<li><p><cite>exec_place::current_device()</cite> (default): Run on current CUDA device.</p></li>
<li><p><cite>exec_place::device(ID)</cite>: Run on CUDA device identified by its index.</p></li>
<li><p><cite>exec_place::host()</cite>: Run on the host (Note: this is providing a CUDA stream which should be used to submit CUDA callbacks. For example, users should typically use the <cite>host_launch</cite> API instead).</p></li>
</ul>
</li>
</ul>
<p>Examples:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">task</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span>
<span class="w">    </span><span class="o">-&gt;*</span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dY</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">axpy</span><span class="o">&lt;&lt;&lt;</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="n">dY</span><span class="p">);</span>
<span class="w">    </span><span class="p">};</span>
</pre></div>
</div>
</section>
<section id="host-side-task-execution-with-host-launch">
<h4>Host-Side Task Execution with <cite>host_launch</cite><a class="headerlink" href="#host-side-task-execution-with-host-launch" title="Link to this heading">#</a></h4>
<p>Purpose: Execute tasks on the host (CPU) while still utilizing the task and data management system.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">host_launch</span><span class="p">(</span><span class="n">logicalData1</span><span class="p">.</span><span class="n">accessMode</span><span class="p">(),</span><span class="w"> </span><span class="n">logicalData2</span><span class="p">.</span><span class="n">accessMode</span><span class="p">()</span><span class="w"> </span><span class="p">...)</span>
<span class="w">    </span><span class="o">-&gt;*</span><span class="p">[</span><span class="n">capture</span><span class="w"> </span><span class="n">list</span><span class="p">](</span><span class="k">auto</span><span class="w"> </span><span class="n">data1</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">data2</span><span class="w"> </span><span class="p">...)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Host-based task implementation here</span>
<span class="w">    </span><span class="p">};</span>
</pre></div>
</div>
</section>
</section>
<section id="kernel-authoring">
<h3>Kernel authoring<a class="headerlink" href="#kernel-authoring" title="Link to this heading">#</a></h3>
<p>In addition to user-provided CUDA kernels or CUDA libraries, CUDASTF makes it possible to author compute kernels directly on top of logical data using the <cite>parallel_for</cite> and <cite>launch</cite> mechanisms.</p>
<section id="id10">
<h4><cite>parallel_for</cite> construct<a class="headerlink" href="#id10" title="Link to this heading">#</a></h4>
<p>Purpose: Apply a kernel on each coordinate of a shape.</p>
<p>Syntax:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">parallel_for</span><span class="p">([</span><span class="n">execution</span><span class="w"> </span><span class="n">place</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="n">partitioner</span><span class="p">],</span><span class="w"> </span><span class="n">shape</span><span class="p">,</span><span class="w"> </span><span class="n">logicalData1</span><span class="p">.</span><span class="n">accessMode</span><span class="p">(),</span><span class="w"> </span><span class="n">logicalData2</span><span class="p">.</span><span class="n">accessMode</span><span class="p">(),</span><span class="w"> </span><span class="p">...)</span>
<span class="w">    </span><span class="o">-&gt;*</span><span class="p">[</span><span class="n">capture</span><span class="w"> </span><span class="n">list</span><span class="p">]</span><span class="w"> </span><span class="n">__device__</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">index1</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">index2</span><span class="p">,</span><span class="w"> </span><span class="p">...</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">data1</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">data2</span><span class="w"> </span><span class="p">...)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Kernel implementation</span>
<span class="w">    </span><span class="p">};</span>
</pre></div>
</div>
<p>Examples:</p>
<ul class="simple">
<li><p>Applying a kernel on a 1D array:</p></li>
</ul>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">X</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
<span class="kt">double</span><span class="w"> </span><span class="n">Y</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>

<span class="k">auto</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lY</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">Y</span><span class="p">);</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">parallel_for</span><span class="p">(</span><span class="n">lY</span><span class="p">.</span><span class="n">shape</span><span class="p">(),</span><span class="w"> </span><span class="n">lX</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">rw</span><span class="p">())</span>
<span class="w">    </span><span class="o">-&gt;*</span><span class="p">[</span><span class="n">alpha</span><span class="p">]</span><span class="w"> </span><span class="n">__device__</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dY</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">dY</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">dX</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
<span class="w">    </span><span class="p">};</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Applying a kernel on a 2D matrix:</p></li>
</ul>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">X</span><span class="p">[</span><span class="n">N</span><span class="o">*</span><span class="n">N</span><span class="p">];</span>
<span class="kt">double</span><span class="w"> </span><span class="n">Y</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>

<span class="k">auto</span><span class="w"> </span><span class="n">lX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">make_slice</span><span class="p">(</span><span class="o">&amp;</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="p">{</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">}));</span>
<span class="k">auto</span><span class="w"> </span><span class="n">lY</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="p">.</span><span class="n">logical_data</span><span class="p">(</span><span class="n">Y</span><span class="p">);</span>

<span class="n">ctx</span><span class="p">.</span><span class="n">parallel_for</span><span class="p">(</span><span class="n">lX</span><span class="p">.</span><span class="n">shape</span><span class="p">(),</span><span class="w"> </span><span class="n">lX</span><span class="p">.</span><span class="n">rw</span><span class="p">(),</span><span class="w"> </span><span class="n">lY</span><span class="p">.</span><span class="n">read</span><span class="p">())</span>
<span class="w">    </span><span class="o">-&gt;*</span><span class="p">[](</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">j</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dX</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">dY</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">dX</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">dY</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">dY</span><span class="p">(</span><span class="n">j</span><span class="p">);</span>
<span class="w">    </span><span class="p">};</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#parallel-for-construct"><span class="std std-ref">this section</span></a> for more details.</p>
</section>
<section id="id11">
<h4><cite>launch</cite> construct<a class="headerlink" href="#id11" title="Link to this heading">#</a></h4>
<p>The <cite>launch</cite> construct makes it possible to launch structured compute kernels. Contrary to <cite>parallel_for</cite> which applies the same operation on every member of a shape, <cite>launch</cite> executes a kernel over a thread hierarchy.</p>
<p>Syntax:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">thread_hierarchy_spec_t</span><span class="o">&gt;</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">launch</span><span class="p">([</span><span class="kr">thread</span><span class="w"> </span><span class="n">hierarchy</span><span class="w"> </span><span class="n">spec</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="n">execution</span><span class="w"> </span><span class="n">place</span><span class="p">],</span><span class="w"> </span><span class="n">logicalData1</span><span class="p">.</span><span class="n">accessMode</span><span class="p">(),</span><span class="w"> </span><span class="n">logicalData2</span><span class="p">.</span><span class="n">accessMode</span><span class="p">(),</span><span class="w"> </span><span class="p">...)</span>
<span class="w">    </span><span class="o">-&gt;*</span><span class="p">[</span><span class="n">capture</span><span class="w"> </span><span class="n">list</span><span class="p">]</span><span class="w"> </span><span class="n">__device__</span><span class="w"> </span><span class="p">(</span><span class="n">thread_hierarchy_spec_t</span><span class="w"> </span><span class="n">th</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">data1</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">data2</span><span class="w"> </span><span class="p">...)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Kernel implementation</span>
<span class="w">    </span><span class="p">};</span>
</pre></div>
</div>
<p>The thread hierarchy passed to the construct is defined as a nested object of the form</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="n">par</span><span class="o">|</span><span class="n">con</span><span class="p">}</span><span class="o">&lt;</span><span class="p">[</span><span class="n">static_width</span><span class="p">]</span><span class="o">&gt;</span><span class="p">([</span><span class="n">width</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="n">scope</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="n">mem</span><span class="p">(</span><span class="n">size</span><span class="p">)]</span><span class="w"> </span><span class="p">[</span><span class="n">nested</span><span class="w"> </span><span class="n">hierarchy</span><span class="w"> </span><span class="n">specification</span><span class="p">])</span>
</pre></div>
</div>
<p>When passed to the user-provided kernel implementation, this object provides different mechanisms to let threads interact or to query the structure of the parallelism</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">th</span><span class="p">.</span><span class="n">rank</span><span class="p">();</span><span class="w"> </span><span class="c1">// get thread rank within the entire hierarchy</span>
<span class="n">th</span><span class="p">.</span><span class="n">rank</span><span class="p">(</span><span class="n">i</span><span class="p">);</span><span class="w"> </span><span class="c1">// get thread rank at i-th level</span>
<span class="n">th</span><span class="p">.</span><span class="k">template</span><span class="w"> </span><span class="n">rank</span><span class="o">&lt;</span><span class="n">i</span><span class="o">&gt;</span><span class="p">();</span><span class="w"> </span><span class="c1">// (constexpr) get thread rank at the i-th level at compile time</span>

<span class="n">th</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="c1">// get the total number of threads</span>
<span class="n">th</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="n">i</span><span class="p">);</span><span class="w"> </span><span class="c1">// get the number of threads at the i-th level</span>
<span class="n">th</span><span class="p">.</span><span class="k">template</span><span class="w"> </span><span class="n">size</span><span class="o">&lt;</span><span class="n">i</span><span class="o">&gt;</span><span class="p">();</span><span class="w"> </span><span class="c1">// (constexpr) get the number of threads at the i-th level at compile time</span>

<span class="n">th</span><span class="p">.</span><span class="n">get_scope</span><span class="p">(</span><span class="n">i</span><span class="p">);</span><span class="w"> </span><span class="c1">// get the affinity of the i-th level</span>
<span class="n">th</span><span class="p">.</span><span class="k">template</span><span class="w"> </span><span class="n">storage</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">i</span><span class="p">);</span><span class="w"> </span><span class="c1">// get the local storage associated to the i-th level as a slice&lt;T&gt;</span>

<span class="n">th</span><span class="p">.</span><span class="n">sync</span><span class="p">();</span><span class="w"> </span><span class="c1">// issue a barrier among all threads</span>
<span class="n">th</span><span class="p">.</span><span class="n">sync</span><span class="p">(</span><span class="n">i</span><span class="p">);</span><span class="w"> </span><span class="c1">// issue a barrier among threads of the i-th level</span>
<span class="n">th</span><span class="p">.</span><span class="n">is_synchronizable</span><span class="p">(</span><span class="n">i</span><span class="p">);</span><span class="w"> </span><span class="c1">// check if we can call sync(i)</span>
<span class="n">th</span><span class="p">.</span><span class="k">template</span><span class="w"> </span><span class="n">is_synchronizable</span><span class="o">&lt;</span><span class="n">i</span><span class="o">&gt;</span><span class="p">();</span><span class="w"> </span><span class="c1">// (constexpr) check if we can call sync(i) at compile time</span>

<span class="n">th</span><span class="p">.</span><span class="n">depth</span><span class="p">();</span><span class="w"> </span><span class="c1">// (constexpr) get the depth of the thread hierarchy</span>
<span class="n">th</span><span class="p">.</span><span class="n">inner</span><span class="p">();</span><span class="w"> </span><span class="c1">// get the thread hierarchy subset obtained by removing the top-most level of the hierarchy</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">con</span><span class="p">(</span><span class="mi">128</span><span class="p">);</span>
<span class="n">par</span><span class="p">();</span>
<span class="n">con</span><span class="o">&lt;</span><span class="mi">128</span><span class="o">&gt;</span><span class="p">();</span>
<span class="n">con</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="n">par</span><span class="p">(</span><span class="mi">32</span><span class="p">));</span>
<span class="n">con</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="n">mem</span><span class="p">(</span><span class="mi">64</span><span class="p">),</span><span class="w"> </span><span class="n">con</span><span class="p">(</span><span class="mi">32</span><span class="p">));</span>
<span class="n">par</span><span class="p">(</span><span class="n">hw_scope</span><span class="o">::</span><span class="n">device</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">hw_scope</span><span class="o">::</span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="n">par</span><span class="o">&lt;</span><span class="mi">128</span><span class="o">&gt;</span><span class="p">(</span><span class="n">hw_scope</span><span class="o">::</span><span class="kr">thread</span><span class="p">));</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#launch-construct"><span class="std std-ref">this section</span></a> for more details.</p>
</section>
</section>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="api/structcuda_1_1experimental_1_1stf_1_1graphed__interface__of_3_01void__interface_01_4.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">cuda::experimental::stf::graphed_interface_of&lt; void_interface &gt;</p>
      </div>
    </a>
    <a class="right-next"
       href="stf/custom_data_interface.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Implementation of the <code class="docutils literal notranslate"><span class="pre">matrix</span></code> class</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            


              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sequential-task-flow-stf-programming-model">The Sequential Task Flow (STF) programming model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started-with-cudastf">Getting started with CUDASTF</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-cudastf">Getting CUDASTF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-cudastf">Using CUDASTF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compiling">Compiling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-cudastf-within-a-cmake-project">Using CUDASTF within a CMake project</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-simple-example">A simple example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compiling-examples">Compiling examples</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backends-and-contexts">Backends and contexts</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tasks-in-the-stream-backend">Tasks in the Stream backend</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tasks-in-the-graph-backend">Tasks in the Graph backend</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logical-data">Logical data</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-interfaces">Data interfaces</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-custom-data-interfaces-advanced">Defining custom data interfaces (advanced)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#write-back-policy">Write-back policy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#slices">Slices</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-multidimensional-slices">Defining multidimensional slices</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-logical-data-from-a-shape">Defining logical data from a shape</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tasks">Tasks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-creating-and-using-multiple-tasks">Example of creating and using multiple tasks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lower-level-task-api">Lower-level task API</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#synchronization">Synchronization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#places">Places</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#execution-places">Execution places</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-places">Data places</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-of-places">Grid of places</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-grids-of-places">Creating grids of places</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#shaped-grids">Shaped grids</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#partitioning-policies">Partitioning policies</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#predefined-partitioning-policies">Predefined partitioning policies</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parallel-for-construct"><code class="docutils literal notranslate"><span class="pre">parallel_for</span></code> construct</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-with-a-1-dimensional-array">Example with a 1-dimensional array</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-with-multi-dimensional-arrays">Example with multi-dimensional arrays</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#box-shape">Box shape</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#box-shapes-with-extents">Box shapes with extents</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#box-shapes-with-lower-and-upper-bounds">Box shapes with lower and upper bounds</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-custom-shapes-advanced">Defining custom shapes (advanced)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduce-access-mode">Reduce access mode</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#launch-construct"><code class="docutils literal notranslate"><span class="pre">launch</span></code> construct</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-with-a-1-dimensional-array-1">Example with a 1-dimensional array</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#describing-a-thread-hierarchy">Describing a thread hierarchy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manipulating-a-thread-hierarchy">Manipulating a thread hierarchy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cuda-kernel-construct"><code class="docutils literal notranslate"><span class="pre">cuda_kernel</span></code> construct</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cuda-kernel-chain-construct"><code class="docutils literal notranslate"><span class="pre">cuda_kernel_chain</span></code> construct</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-types-of-logical-data-and-tasks">C++ Types of logical data and tasks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logical-data-1">Logical data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Tasks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamically-typed-tasks">Dynamically-typed tasks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-typed-and-untyped-tasks">Combining typed and untyped tasks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modular-use-of-cudastf">Modular use of CUDASTF</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#freezing-logical-data">Freezing logical data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokens">Tokens</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stream-management-with-execution-places">Stream management with execution places</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-the-current-device-or-context">Setting the current device or context</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-allocation-with-data-places">Memory allocation with data places</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vmm-based-allocation-with-mem-create">VMM-based allocation with mem_create</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#debugging">Debugging</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enabling-internal-checks">Enabling internal checks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tools">Tools</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-task-graphs">Visualizing task graphs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-visualizations-of-task-graphs">Generating visualizations of task graphs</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#condensed-and-structured-graphs-visualization">Condensed and structured graphs visualization</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-tuning-with-ncu">Kernel tuning with ncu</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#naming-kernels">Naming kernels</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-with-miniweather">Example with miniWeather</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cudastf-reference-card">CUDASTF Reference Card</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Using CUDASTF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-context">Creating a Context</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Logical Data</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-logical-data">Creating a Logical Data</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Tasks</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-dependency">Data Dependency</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#task-creation">Task Creation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#host-side-task-execution-with-host-launch">Host-Side Task Execution with <cite>host_launch</cite></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-authoring">Kernel authoring</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10"><cite>parallel_for</cite> construct</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11"><cite>launch</cite> construct</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            

          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  

  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">




  <p class="copyright">
    
      Copyright © 2026, NVIDIA Corporation.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 9.1.0.
    <br/>
  </p>
</div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>